\section{Introduction}
% task for experts, large datasources by hand 
%   -> partly automatic would be useful (ranking)
% risk analysis -> relations important
% data required, unstructured i.e. in 10k filings
% where do you get info? newspaper, b√∂rse, bankberichte
% what changed? more data. + availability

Evaluating the credibility of a company is an important and complex task for financial experts.
When estimating the risk associated with a potential asset, analysts rely on large amounts of data from a variety of different sources, such as newspapers, stock market trends, and bank statements.
Finding relevant information in mostly unstructured data is a tedious task and examining all sources by hand quickly becomes infeasible.

An important aspect of risk management is the relations of a company of interest to other financial entities.
Automatically extracting such relationships from unstructured text files, such as 10-K filings, significantly reduces the amount of manual work.
Such structured knowledge enables experts to quickly gain insight into a company's relationship network.
However, not all extracted relationships may be important in a given context.
In this paper, we propose an approach to rank extracted relationships based on a text snippets, such that important information can be displayed more prominently.
%With our approach we are able to achieve an overall normalized discounted cumulative gain (NDCG) score of 0.98.

\section{Dataset}
The dataset used for this work was provided in the context of the FEIII Challenge 2017\footnote{\url{https://ir.nist.gov/feiii/}}, which contains almost 1000 \emph{triples} extracted from 25 10-K and 10-Q filings, which describe a relationship (\emph{role}) between the \emph{filing company} and a \emph{mentioned financial entity}.
Relationships are limited to ten predefined roles (see \fref{tab:roleresults}).
Triples were labelled by experts according to their relevance from a business perspective as \emph{irrelevant}, \emph{neutral}, \emph{relevant}, or \emph{highly relevant}.
%Depending on the context, the relevance can be understood as an indicator for the potential impact a relationship might have on the business \textbf{business what? operations?} of the filing company. 

{\setlength{\parindent}{0cm}
\paragraph{\textbf{Inter Annotator Agreement}}
The quality of the annotations is estimated using Cohen's Kappa ($\kappa\in [0,1]$), which quantifies the inter annotator agreement (IAA) between two experts.%\cite{ir}.
Around 40\% of the triples were rated by more than one expert with a weighted average of $\overline{\kappa}=0.45$, which indicates a high level of disagreement.
For our training and evaluation purposes we map the ratings to numerical values (1--4) and consider the average rating for each triple labelled by multiple experts.
}

%{\setlength{\parindent}{0cm}
%\paragraph{\textbf{Additional Data}}
%The triples were extracted from financial documents.
%The surrounding sentences were used by experts to label them.
%Besides the triples, additional information about the word context a triple was extracted from is also given.
%We complement this by the full original filings, which are available online\footnote{\url{https://www.sec.gov/edgar/aboutedgar.htm}}.
%}

\section{Our Approach}
We use logistic regression to generate a ranking for the sentences a particular triple was found in.
%This ranking reflects the relevance of a sentence with respect to the imminence of business implications.
The ranking is obtained by classifying the sentences as relevant or irrelevant and rank them based on the output probability for the relevant class.
We trained three models with different features.
Then we combined the three models to form an ensemble to classify the snippets.
The logit models in the ensemble performs a one-vs-rest classification for each respective category.
The predictions of the model return a softmax confidence score for each category, from which the weighted sum is taken as the ranking score.

%For some roles, the number of samples is to small to train a reasonable model.
%It was found, that training on the entire set, disregarding the roles, results in similar or even better performance.

The different models of the ensemble are based on different feature sets, namely \emph{bag-of-words} (BOW), \emph{embeddings} (EMB), and \emph{syntax features} (SYN).

% classification model to produce score
% try to learn from labelled data (whats rel/irrel)
% therefore logit
% for features, three approaches
% 3 subsections describing each

\subsection{Bag-of-Words}
The bag-of-words representation uses a predefined dictionary.
A sentence is represented by a vector of the dimensionality of the dictionary.
The presence of a word is indicated by a non-zero value corresponding to its index in the dictionary.
To emphasise words most meaningful for category distinction, the inverse document frequency (IDF) is calculated by virtually putting all texts with the same rating in one ``document''. 
Each sentence then is encoded as a bag-of-words vector where the respective positions contain the IDF weighted by the corresponding term frequency in the sentence.

In order to reduce the feature space, a filter removes the most and least frequent terms.
This also limits the chance of over-fitting the model due to sparse terms.

This encoding does not represent phrases, which may loose their meaning when only considering their terms individually.
Thus, the dictionary is created using n-grams of length one to three from the lemmatised tokens in the training set.
\begin{table*}
	\caption{Averaged experimental results for each role using BOW}
	\label{tab:roleresults}
	\begin{tabular}{lcccccccccc}
		\toprule
		& affiliate & agent & counterpart & guarantor & insurer & issuer & seller & servicer & trustee & underwriter \\
		\midrule
               \# samples        & 185  & 61   & 64   & 34   & 19   & 129  & 20   & 21   & 420  & 21   \\
               NDCG (5fold-CV)   & 0.97 & 0.97 & 0.93 & 0.97 & 0.99 & 0.92 & 1.0  & 0.98 & 0.99 & 1.0  \\
               Baseline (random) & 0.86 & 0.81 & 0.84 & 0.96 & 0.97 & 0.75 & 0.96 & 0.93 & 0.88 & 0.96 \\
		\bottomrule
	\end{tabular}
\end{table*}

\subsection{Embedding}
Difficulties with previously unseen examples might arise from the limited size of the dictionary for the BOW.
Previous research has shown, that doc2vec embeddings manage to outperform BOW approaches.
Therefore, we created a embedding from 25 of the original full text filings on a sentence basis using the Gensim\footnote{\url{https://radimrehurek.com/gensim/models/doc2vec.html}} implementation.

Such an embedding uses unsupervised deep learning. 
Internally, the neural network is trained to predict the following word to a sequence of words. 
The length of the sequence has a fixed length, which is compensated by an additional paragraph matrix, implicitly acting as a memory of the context.
After multiple iterations of training, this matrix can be used to induce fixed size, dense vectors for text of variable length by representing its context.

We use a window size of 10 and a paragraph vector of size 50, which is trained for 30 epochs over the entire set containing 60k sentences (2M words).
The representation for our classifier is construced by inducing a vector for each of the three sentences in a sample are concatenating them.
If necessary, the resulting vector is padded or truncated, since in some cases, there are more or less than three sentences.

\subsection{Syntax Features}
Lastly, to provide a language independent approach, we created a selection of syntax-based features.
Following the gini impurity metric, features like the ratio of upper-case words and numbers or the number of dollar signs and word repetitions appear to be most meaningful for classification.
In total we derived 25 features describing the number or presence of different syntactical characteristics.

While a logit model is a good choice for BOW and EMB, it has shown unsatisfactory performances on syntax features.
Random forests, on the other hand, have shown promising results in this case.
They are not applied to BOW, as they potentially are more prone to favouring a small number of n-grams and thus won't generalise well.

\subsection{Ensemble}
Each of the numerical representations and their resulting model have their strengths and weaknesses.
For example, the language independence of SYN can tolerate a changing vocabulary to an extent, but misses the advantage to identify key phrases which may prove useful for classification.
Thus, combining them through concatenation provides more information to a classifier.
However, more features lead to noise in the data, which hinders efficient training or even prevents its convergence.

Better results were achieved by keeping the input spaces separate and fitting individual models.
Predictions of each model are summed together to a soft vote.

\section{Evaluation}

The system's performance is measured by the Normalised Discounted Cumulative Gain (NDCG).%\cite{ir}.
\begin{table}[H]
	\caption{Experimental results for bag-of-words (BOW), embedding (EMB), syntax (SYN) features, and ensemble}
	\label{tab:results}
	\begin{tabular}{lcccc}
		\toprule
		Approach & NDCG & $\sigma ($NDCG$)$ & F1-Score &  $\sigma ($F1$)$\\
		\midrule
		Baseline (random) & 0.87 & 0.07 & - & - \\
		Baseline (worst)  & 0.73 & 0.13 & - & - \\
		\midrule
		BOW & \textbf{0.98} & 0.03 & 0.73 & 0.27\\
		EMB & 0.92 & 0.07 & 0.41 & 0.16\\
		SYN & 0.94 & 0.06 & 0.43 & 0.26 \\
		BOW+EMB+SYN& 0.93 & 0.06 & 0.46 & 0.22\\
		\bottomrule
	\end{tabular}
\end{table}
For the cross-evaluation, the labelled samples are split into a training and test set.
Triples in those sets do not share the filings they were extracted from and the distribution of ratings is roughly stratified.
For each the five cross evaluation (5fold-CV) passes, samples are reassigned to different sets.
Models are trained on the training set as described in the sections above.
The performance is measured for samples of each role respectively by calculating the NDCG that results from the calculated ranking score and F1-Score, which indicates the quality of underlying classifiers.

\Fref{tab:results} lists the mean NDCG scores and the standard deviation ($\sigma$).
For comparison, we consider a baseline of the worst possible ranking (inverse order of the ideal ranking) and the average of multiple random rankings.

The BOW model performed best in our experiments, the EMB and SYN models show similar results. 
Training a model on text usually requires a reasonably large corpus.
With the data at hand we had to pay close attention to the feature selection, since seemingly very specific terms are likely to negatively affect the model's ability to classify unseen samples.

Looking at the performance of the classification task itself, measured by the F1-Score, the EMB model has the smallest standard deviation across multiple runs of the evaluation.

Combining different text represenations did not produce an improved model.
The soft-voted ensemble of all classifiers shows similar results as EMB and SYN alone.

\section{Conclusion}
Overall, we managed to achieve very good NDCG scores of around 0.98 using a BOW model.
However, with the limited amount of data at hand, concerns about the ability to generalise to unseen samples that diviate from the vocabulary can not be fully eliminated.
We assume BOW may be more robust to such changes, since the underlying word embedding is trained on a significantly larger set of text and is able to reflect phrase similarities.
A comparison of the standard deviation of the F1-Scores of 0.27 and 0.17, respectively, supports this assumption.

The impact of a relationship on business perspectives of a company may also be judged by comparing revenues.
For example, the (new) relationship of a small company to a high-valued company may indicate future growth.
Thus, in future work, triples could be enriched by adding the (historical) revenue of two involved financial entities. 



%\begin{figure}
%	\begin{subfigure}[t]{0.5\linewidth}
%		\centering
%		\includegraphics[width=\linewidth]{conf_full}
%		\caption{Trained on all samples}
%	\end{subfigure}%
%	~ 
%	\begin{subfigure}[t]{0.5\linewidth}
%		\includegraphics[width=\linewidth]{conf_role}
%		\caption{Trained on role samples}
%	\end{subfigure}
%	\caption{Normalised aggregated confusion matrices for the model with different training sets}
%	\label{fig:confmatrix}
%\end{figure}
%


%\begin{table}
%	\caption{Experimental results for bag-of-words (BOW), embedding (EMB), syntax (SYN) features}, and ensemble (ENS)
%	\label{tab:results}
%	\begin{tabular}{lcc}
%		\toprule
%		Approach & NDCG & $\sigma ($NDCG$)$\\
%		\midrule
%		Baseline (random) & 0.87 & 0.07\\
%		Baseline (worst) & 0.73 & 0.13\\
%		\midrule
%		%F1 full 0.73 std=0.27
%		%F1 role 0.49 std=0.17
%		BOW, full set, categorical & 0.97 & 0.04 \\
%		BOW, role based, categorical & 0.92 & 0.07  \\
%		BOW, full set, continuous & \textbf{0.98} & 0.03 \\
%		BOW, role based, continuous & 0.93 & 0.07 \\
%		\midrule
%		% F1 full 0.41 std=0.16
%		% F1 role 0.38 std=0.21
%		EMB, full set, categorical & 0.91 & 0.07 \\
%		EMB, role based, categorical & 0.89 & 0.07  \\
%		EMB, full set, continuous & 0.92 & 0.07 \\
%		EMB, role based, continuous & 0.90 & 0.08 \\
%		\midrule
%		% F1 full 0.43 std=0.26
%		% F1 role 0.39 std=0.26
%		SYN, full set, categorical & 0.93 & 0.07 \\
%		SYN, role based, categorical & 0.92 & 0.07  \\
%		SYN, full set, continuous & 0.94 & 0.06 \\
%		SYN, role based, continuous & 0.92 & 0.08 \\
%		\midrule
%		% F1 full 0.46 std=0.22
%		% F1 role 0.39 std=0.20
%		ENS, full set, categorical & 0.93 & 0.06 \\
%		ENS, role based, categorical & 0.91 & 0.07  \\
%		ENS, full set, continuous & 0.93 & 0.06 \\
%		ENS, role based, continuous & 0.92 & 0.07 \\
%		\bottomrule
%	\end{tabular}
%\end{table}
