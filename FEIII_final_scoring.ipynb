{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pprint\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained embedding. See readme for training your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from code.feiii_transformers import _EmbeddingHolder\n",
    "embedding = _EmbeddingHolder()\n",
    "embedding.load('embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "reading file STATE-STREET_2016.csv with 23 entries.\n",
      "reading file STATE-STREET_2014.csv with 26 entries.\n",
      "reading file PNC_2016.csv with 33 entries.\n",
      "reading file PNC_2014.csv with 38 entries.\n",
      "reading file JPM_2016.csv with 52 entries.\n",
      "reading file COMERICA_2016.csv with 11 entries.\n",
      "reading file FIFTH-THIRD_2014.csv with 36 entries.\n",
      "reading file CITIGROUP_2014.csv with 52 entries.\n",
      "reading file AMERICAN-EXPRESS_2015.csv with 11 entries.\n",
      "reading file BANK-OF-AMERICA_2015.csv with 74 entries.\n",
      "reading file ALLY_2016.csv with 44 entries.\n",
      "reading file CITIGROUP_2016.csv with 50 entries.\n",
      "reading file ALLY_2014.csv with 40 entries.\n",
      "reading file SUNTRUST_2013.csv with 35 entries.\n",
      "reading file DISCOVER_2014.csv with 41 entries.\n",
      "reading file MORGAN-STANLEY_2015.csv with 128 entries.\n",
      "reading file SUNTRUST_2016.csv with 27 entries.\n",
      "reading file BBT_2014.csv with 14 entries.\n",
      "reading file GENERAL-ELECTRIC_2013.csv with 21 entries.\n",
      "reading file FIFTH-THIRD_2015.csv with 46 entries.\n",
      "reading file CAPITAL-ONE_2013.csv with 25 entries.\n",
      "reading file BANK-OF-AMERICA_2013.csv with 88 entries.\n",
      "reading file NORTHERN-TRUST_2013.csv with 18 entries.\n",
      "reading file MT_2015.csv with 21 entries.\n",
      "reading file MT_2013.csv with 21 entries.\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th></th><th>TRAINING</th><th>TESTING SET</th></tr><tr><td>#ROWS</td><td>975</td><td>900</td></tr><tr><td>ROLES</td><td>{'affiliate': 186,<br /> 'agent': 61,<br /> 'counterpart': 64,<br /> 'guarantor': 34,<br /> 'insurer': 19,<br /> 'issuer': 129,<br /> 'seller': 20,<br /> 'servicer': 21,<br /> 'trustee': 420,<br /> 'underwriter': 21}</td><td>{'affiliate': 129,<br /> 'agent': 40,<br /> 'counterpart': 108,<br /> 'guarantor': 28,<br /> 'insurer': 47,<br /> 'issuer': 98,<br /> 'seller': 49,<br /> 'servicer': 57,<br /> 'trustee': 304,<br /> 'underwriter': 40}</td></tr><tr><td>DOCUMENTS</td><td>{'1393612-2013-FY': 41,<br /> '19617-2015-FY': 52,<br /> '28412-2015-FY': 11,<br /> '35527-2013-FY': 36,<br /> '35527-2014-FY': 46,<br /> '36270-m&t-2012': 21,<br /> '36270-m&t-2014': 21,<br /> '40545-2012-FY': 21,<br /> '40729-2013-FY': 40,<br /> '40729-2015-FY': 44,<br /> '4962-2014-FY': 11,<br /> '70858-2012-FY': 88,<br /> '70858-2014-FY': 74,<br /> '713676-2013-FY': 38,<br /> '713676-2015-FY': 33,<br /> '73124-2012-FY': 18,<br /> '750556-2012-FY': 35,<br /> '750556-2015-FY': 27,<br /> '831001-2013-FY': 52,<br /> '831001-2015-FY': 50,<br /> '895421-2014-FY': 128,<br /> '92230-2013-FY': 14,<br /> '927628-2012-FY': 25,<br /> '93751-2013-FY': 26,<br /> '93751-2015-FY': 23}</td><td>{'1026214-2011-Q2': 64,<br /> '1026214-2013-FY': 99,<br /> '109380-2014-FY': 13,<br /> '1390777-2015-Q1': 26,<br /> '1393612-2010-FY': 11,<br /> '19617-2011-FY': 67,<br /> '28412-2012-FY': 11,<br /> '310522-2012-FY': 59,<br /> '310522-2013-Q2': 21,<br /> '316709-2015-FY': 14,<br /> '36104-_2015-FY': 8,<br /> '36270-m&t-2010': 18,<br /> '40545-2015-FY': 27,<br /> '40729-2012-Q3': 23,<br /> '4962-2015-FY': 19,<br /> '70858-2013-FY': 81,<br /> '713676-2014-FY': 33,<br /> '73124-2015-FY': 28,<br /> '831001-2011-FY': 37,<br /> '886982-2013-FY': 36,<br /> '895421-2015-FY': 134,<br /> '91576-2012-FY': 10,<br /> '92230-2010-FY': 21,<br /> '927628-2010-FY': 20,<br /> '93751-2010-FY': 20}</td></tr><tr><td>COMPANIES</td><td>{'AMERICAN EXPRESS CO': 11,<br /> 'Ally Financial Inc': 84,<br /> 'BANK OF AMERICA CORP': 162,<br /> 'BB&T CORP': 14,<br /> 'CAPITAL ONE FINANCIAL CORP': 25,<br /> 'CITIGROUP INC': 102,<br /> 'COMERICA INC': 11,<br /> 'Discover Financial Services': 41,<br /> 'FIFTH THIRD BANCORP': 82,<br /> 'GENERAL ELECTRIC CO': 21,<br /> 'JPMORGAN CHASE & CO': 52,<br /> 'M&T BANK CORP': 42,<br /> 'MORGAN STANLEY': 128,<br /> 'NORTHERN TRUST CORP': 18,<br /> 'PNC FINANCIAL SERVICES GROUP INC': 71,<br /> 'STATE STREET CORP': 49,<br /> 'SUNTRUST BANKS INC': 62}</td><td>{'AMERICAN EXPRESS CO': 19,<br /> 'Ally Financial Inc': 23,<br /> 'BANK OF AMERICA CORP': 81,<br /> 'BB&T CORP': 21,<br /> 'Bank of New York Mellon Corp': 26,<br /> 'CAPITAL ONE FINANCIAL CORP': 20,<br /> 'CITIGROUP INC': 37,<br /> 'COMERICA INC': 11,<br /> 'Discover Financial Services': 11,<br /> 'FEDERAL_HOME_LOAN_MORTGAGE_CORP': 163,<br /> 'FEDERAL_NATIONAL_MORTGAGE_ASSOCIATION_FANNIE_MAE': 80,<br /> 'GENERAL ELECTRIC CO': 27,<br /> 'GOLDMAN SACHS GROUP INC': 36,<br /> 'JPMORGAN CHASE & CO': 67,<br /> 'KEYCORP': 10,<br /> 'M&T BANK CORP': 18,<br /> 'MORGAN STANLEY': 134,<br /> 'NORTHERN TRUST CORP': 28,<br /> 'PNC FINANCIAL SERVICES GROUP INC': 33,<br /> 'SCHWAB_CHARLES_CORP ': 14,<br /> 'STATE STREET CORP': 20,<br /> 'US_BANCORP': 8,<br /> 'ZIONS BANCORPORATION': 13}</td></tr><tr><td>Expert ratings</td><td>ALL: [H]_295 [R]_283 [N]_307 [I]__90<br/>[H]149 [R]_93 [N]139 [I]_28 [x]566 (RATING_EXPERT_1)<br/>[H]101 [R]_35 [N]_83 [I]__7 [x]749 (RATING_EXPERT_1.1)<br/>[H]__0 [R]_20 [N]__0 [I]__0 [x]955 (RATING_EXPERT_10)<br/>[H]_58 [R]163 [N]131 [I]_14 [x]609 (RATING_EXPERT_2)<br/>[H]__8 [R]__8 [N]_15 [I]_29 [x]915 (RATING_EXPERT_3)<br/>[H]__5 [R]_23 [N]__5 [I]__2 [x]940 (RATING_EXPERT_4)<br/>[H]__7 [R]_29 [N]_41 [I]__3 [x]895 (RATING_EXPERT_5)<br/>[H]_10 [R]_34 [N]__6 [I]_10 [x]915 (RATING_EXPERT_6)<br/>[H]_10 [R]_27 [N]_36 [I]__2 [x]900 (RATING_EXPERT_7)<br/>[H]_62 [R]__1 [N]__1 [I]_10 [x]901 (RATING_EXPERT_9)<br/></td><td>ALL: [H]_309 [R]_369 [N]_142 [I]__80<br/></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from code.feiii_experiment import evaluate, kfold\n",
    "from code.feiii_data import DataHolder\n",
    "from code.feiii_pipeline import FeiiiPipeline\n",
    "\n",
    "\n",
    "def pp(obj):\n",
    "    return pprint.pformat(obj).replace('\\n','<br />')\n",
    "\n",
    "data = DataHolder(eval_docs=2)\n",
    "print('done')\n",
    "\n",
    "out = '<table><tr><th></th><th>TRAINING</th><th>TESTING SET</th></tr>'\n",
    "out+= '<tr><td>#ROWS</td><td>'+str(len(data.train_full))+'</td><td>'+str(len(data.test))+'</td></tr>'\n",
    "out+= '<tr><td>ROLES</td><td>'+pp(dict(Counter(data.train_full['grp'])))+'</td><td>'+pp(dict(Counter(data.test['grp'])))+'</td></tr>'\n",
    "out+= '<tr><td>DOCUMENTS</td><td>'+pp(dict(Counter(data.train_full['SOURCE'])))+'</td><td>'+pp(dict(Counter(data.test['SOURCE'])))+'</td></tr>'\n",
    "out+= '<tr><td>COMPANIES</td><td>'+pp(dict(Counter(data.train_full['FILER_NAME'])))+'</td><td>'+pp(dict(Counter(data.test['FILER_NAME'])))+'</td></tr>'\n",
    "out+= '<tr><td>Expert ratings</td><td>'\n",
    "tmp = Counter(data.train_full['rating'])\n",
    "out+= \"ALL: [H]{:_>4d} [R]{:_>4d} [N]{:_>4d} [I]{:_>4d}<br/>\".format(\n",
    "    tmp.get('highly',0),tmp.get('relevant',0),tmp.get('neutral',0),tmp.get('irrelevant',0))\n",
    "for c in data.train_full.filter(regex=(\"RATING\")):\n",
    "    tmp = Counter(data.train_full[c])\n",
    "    out+=\"[H]{:_>3d} [R]{:_>3d} [N]{:_>3d} [I]{:_>3d} [x]{:_>3d} ({})<br/>\".format(\n",
    "        tmp.get(\"Highly relevant\", 0), tmp.get(\"Relevant\", 0),\n",
    "        tmp.get(\"Neutral\", 0),tmp.get(\"Irrelevant\", 0),\n",
    "        tmp.get(np.nan, 0), c)\n",
    "out+= '</td><td>'\n",
    "tmp = Counter(data.test['rating'])\n",
    "out+= \"ALL: [H]{:_>4d} [R]{:_>4d} [N]{:_>4d} [I]{:_>4d}<br/>\".format(\n",
    "    tmp.get('highly',0),tmp.get('relevant',0),tmp.get('neutral',0),tmp.get('irrelevant',0))\n",
    "out+= '</td></tr>'\n",
    "out+= '</table>'\n",
    "\n",
    "HTML(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "import code.feiii_transformers as ft\n",
    "\n",
    "params_ = {\n",
    "    'cv': {\n",
    "        'ngram_range': (1, 3),\n",
    "        'min_df': 0.4,\n",
    "        'max_df': 0.6,\n",
    "        'stop_words': 'english'\n",
    "    },\n",
    "    'tt': {\n",
    "        'use_idf': True,\n",
    "        'sublinear_tf': True,\n",
    "    },\n",
    "    'emb': {\n",
    "        'num_files': 30,\n",
    "        'num_epoch': 20\n",
    "    },\n",
    "    'logit': {\n",
    "        'loss': 'log',  # ['hinge', 'log', 'perceptron','huber'] # for pred_proba: log or modified_huber\n",
    "        'penalty': 'l2',\n",
    "        'shuffle': True,\n",
    "        'alpha': 1e-4,\n",
    "        'n_iter': 15,\n",
    "        'random_state': 42,\n",
    "        'class_weight': 'balanced'\n",
    "    },\n",
    "    'rf': {\n",
    "        'n_estimators': 20,\n",
    "        'criterion': 'gini',  # gini or entropy\n",
    "        'max_features': 'auto',  # int, float, auto, sqrt, log2, None\n",
    "        'random_state': 42,\n",
    "        'class_weight': 'balanced'\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': 1.0,\n",
    "        'kernel': 'sigmoid',  # linear’, ‘poly’, ‘rbf’, ‘sigmoid’\n",
    "        'probability': True,\n",
    "        'class_weight': 'balanced',\n",
    "        'decision_function_shape': 'ovr',  # ovo, ovr\n",
    "        'random_state': 42\n",
    "    }\n",
    "}\n",
    "\n",
    "def pipeline():\n",
    "    pipln = 'all_vote'\n",
    "    if pipln == 'all_union':\n",
    "        line = [\n",
    "            ('union', FeatureUnion(\n",
    "                transformer_list=[\n",
    "                    ('emb', Pipeline([\n",
    "                        ('emb', ft.Embedder(embedding))\n",
    "                    ])),\n",
    "                    ('syntax', Pipeline([\n",
    "                        ('feats', ft.SyntaxFeatures()),\n",
    "                    ])),\n",
    "                    ('bow', Pipeline([\n",
    "                        ('lem', ft.Lemmatiser()),\n",
    "                        ('vect', CountVectorizer(**params_['cv'])),\n",
    "                        ('tfidf', TfidfTransformer(**params_['tt']))\n",
    "                    ]))\n",
    "                ],\n",
    "                transformer_weights={\n",
    "                    'syntax': 1,\n",
    "                    'bow': 1\n",
    "                },\n",
    "            )),\n",
    "            # ('clf', SGDClassifier(**params_['svm']))\n",
    "            ('clf', SGDClassifier(**params_['logit']))\n",
    "            # ('clf', RandomForestClassifier(**params_['rf']))\n",
    "        ]\n",
    "    elif pipln == 'all_vote':\n",
    "        line = [\n",
    "            ('clf', VotingClassifier(\n",
    "                voting='soft',  # hard, soft\n",
    "                # weights=[2,1,2],\n",
    "                estimators=[\n",
    "                    ('syn', Pipeline([\n",
    "                        ('feats', ft.SyntaxFeatures()),\n",
    "                        ('rf', RandomForestClassifier(**params_['rf'])),\n",
    "                        # ('svc', SVC(**params_['svm']))\n",
    "                    ])),\n",
    "                    ('emb', Pipeline([\n",
    "                        ('emb', ft.Embedder(embedding)),\n",
    "                        ('svc', SVC(**params_['svm']))\n",
    "                    ])),\n",
    "                    ('bow', Pipeline([\n",
    "                        ('lem', ft.Lemmatiser()),\n",
    "                        ('vect', CountVectorizer(**params_['cv'])),\n",
    "                        ('tfidf', TfidfTransformer(**params_['tt'])),\n",
    "                        ('svc', SVC(**params_['svm']))\n",
    "                        # ('bclf', SGDClassifier(**params_['logit']))\n",
    "                    ]))\n",
    "                ]))]\n",
    "    elif pipln == 'syn':\n",
    "        line = [\n",
    "            ('feats', ft.SyntaxFeatures()),\n",
    "            ('clf', RandomForestClassifier(**params_['rf']))\n",
    "        ]\n",
    "    elif pipln == 'emb':\n",
    "        line = [\n",
    "            ('emb', ft.Embedder(embedding)),\n",
    "            ('clf', SVC(**params_['svm']))\n",
    "           # ('bclf', SGDClassifier(**params_['logit']))\n",
    "        ]\n",
    "    else:  # pipln == 'bow'\n",
    "        line = [\n",
    "            ('lem', ft.Lemmatiser()),\n",
    "            ('vect', CountVectorizer(**params_['cv'])),\n",
    "            ('tfidf', TfidfTransformer(**params_['tt'])),\n",
    "            ('svc', SVC(**params_['svm']))\n",
    "            #('clf', SGDClassifier(**params_['logit']))\n",
    "        ]\n",
    "    return FeiiiPipeline(line=line, embedding=embedding)\n",
    "\n",
    "def score_func(x):\n",
    "    kind = 'mulmax'\n",
    "    if kind=='last':\n",
    "        xx = x[-1]\n",
    "    elif kind=='mulmax':\n",
    "        xx = (x.argmax(axis=1)+1)*x.max(axis=1)\n",
    "    elif kind=='maxadjust':\n",
    "        xx = np.array([ai-xi[ai:ai+1].sum()+xi[ai+1:].sum() for xi,ai in zip(x,x.argmax(axis=1))])\n",
    "    elif kind == '1234':\n",
    "        xx = np.sum(x * np.array([1, 2, 3, 4]), axis=1)\n",
    "    elif kind == '1246':\n",
    "        xx = np.sum(x * np.array([1, 2, 4, 6]), axis=1)\n",
    "    elif kind=='1245':\n",
    "        xx = np.sum(x * np.array([1, 2, 4, 5]), axis=1)\n",
    "    else:\n",
    "        raise AttributeError('meh.')\n",
    "        \n",
    "    return (xx-xx.min())/(xx-xx.min()).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following line in case of errors during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.shuffle_train_eval(n_docs_eval=0, max_tries=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in training set: 849 (87.08%)\n",
      "Items in eval set: 126\n",
      "Items in test set: 900\n",
      " = 1875\n",
      "Number of source documents: 50 total, 22 train, 3 eval 25 test\n",
      "Absolute (training): IR 87.00, N 240.00, R 241.00, HR 281.00\n",
      "Relative (training): IR 0.10, N 0.28, R 0.28, HR 0.33\n",
      "Absolute (eval): IR 3.00, N 67.00, R 42.00, HR 14.00\n",
      "Relative (eval): IR 0.02, N 0.53, R 0.33, HR 0.11\n",
      "Role samples for TRUSTEE in train: 373, eval: 47, test: 304\n",
      "Role samples for COUNTERPART in train: 58, eval: 6, test: 108\n",
      "Role samples for SELLER in train: 18, eval: 2, test: 49\n",
      "Role samples for GUARANTOR in train: 19, eval: 15, test: 28\n",
      "Role samples for ISSUER in train: 116, eval: 13, test: 98\n",
      "Role samples for AGENT in train: 53, eval: 8, test: 40\n",
      "Role samples for UNDERWRITER in train: 21, eval: 0, test: 40\n",
      "Role samples for SERVICER in train: 18, eval: 3, test: 57\n",
      "Role samples for AFFILIATE in train: 154, eval: 32, test: 129\n",
      "Role samples for INSURER in train: 19, eval: 0, test: 47\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f62a7922bbfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmacro_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_matrix_role\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf_matrix_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/tim/Uni/HPI/workspace/FEII/code/feiii_experiment.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(data, pipeline_generator, score_func, aggregate_baseline, predict_on)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0mfullmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mfullmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# run evaluation for each role\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tim/Uni/HPI/workspace/FEII/code/feiii_pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, frm, target)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mratingmap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/sklearn/ensemble/voting_classifier.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n\u001b[1;32m    164\u001b[0m                     sample_weight)\n\u001b[0;32m--> 165\u001b[0;31m                     for _, clf in self.estimators)\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 758\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/sklearn/ensemble/voting_classifier.py\u001b[0m in \u001b[0;36m_parallel_fit_estimator\u001b[0;34m(estimator, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \"\"\"\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    232\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/tim/Uni/HPI/workspace/FEII/code/feiii_transformers.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, frm)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             vecs = [self.embedding.infer(se) for s in nlp(row['THREE_SENTENCES']).sents for se in s][\n\u001b[0m\u001b[1;32m    140\u001b[0m                    :self.num_sents]\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvecs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_sents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, tag, parse, entity)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/site-packages/spacy/syntax/parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.parser.Parser.__call__ (spacy/syntax/parser.cpp:6160)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fm, m, res, macro_res, conf_matrix_role, conf_matrix_full = evaluate(data, pipeline, score_func=score_func,predict_on='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaving 3 docs out per fold\n",
      "\n",
      "\n",
      "==========================================================================\n",
      "===                      CROSSEVAL ITERATION 1/5                     =====\n",
      "==========================================================================\n",
      "\n",
      "\n",
      "Items in training set: 774 (79.38%)\n",
      "Items in eval set: 201\n",
      "Items in test set: 900\n",
      " = 1875\n",
      "Number of source documents: 50 total, 22 train, 3 eval 25 test\n",
      "Absolute (training): IR 84.00, N 245.00, R 248.00, HR 197.00\n",
      "Relative (training): IR 0.11, N 0.32, R 0.32, HR 0.25\n",
      "Absolute (eval): IR 6.00, N 62.00, R 35.00, HR 98.00\n",
      "Relative (eval): IR 0.03, N 0.31, R 0.17, HR 0.49\n",
      "Role samples for TRUSTEE in train: 291, eval: 129, test: 304\n",
      "Role samples for COUNTERPART in train: 63, eval: 1, test: 108\n",
      "Role samples for SELLER in train: 17, eval: 3, test: 49\n",
      "Role samples for GUARANTOR in train: 34, eval: 0, test: 28\n",
      "Role samples for ISSUER in train: 109, eval: 20, test: 98\n",
      "Role samples for AGENT in train: 49, eval: 12, test: 40\n",
      "Role samples for UNDERWRITER in train: 19, eval: 2, test: 40\n",
      "Role samples for SERVICER in train: 19, eval: 2, test: 57\n",
      "Role samples for AFFILIATE in train: 155, eval: 31, test: 129\n",
      "Role samples for INSURER in train: 18, eval: 1, test: 47\n",
      "=== GUARANTOR ======\n",
      "Items in training set: 34 (100.00%)\n",
      "Items in eval set: 0\n",
      "Items in test set: 28\n",
      " = 62\n",
      "Number of source documents: 15 total, 6 train, 0 eval 9 test\n",
      "Absolute (training): IR 1.00, N 3.00, R 22.00, HR 8.00\n",
      "Relative (training): IR 0.03, N 0.09, R 0.65, HR 0.24\n",
      "Absolute (eval): IR 0.00, N 0.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR nan, N nan, R nan, HR nan\n",
      "Role samples for GUARANTOR in train: 34, eval: 0, test: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/Uni/HPI/workspace/FEII/code/feiii_data.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  rating_agg.sum()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.904693093232 | std = 0.0224933107974\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.810440241115\n",
      "Accuracy | role : 0.785714285714\n",
      "Accuracy | full : 0.535714285714\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         0\n",
      "    neutral       0.17      0.33      0.22         3\n",
      "   relevant       0.85      0.50      0.63        22\n",
      "     highly       0.33      1.00      0.50         3\n",
      "\n",
      "avg / total       0.72      0.54      0.57        28\n",
      "\n",
      "[[ 0  0  0  0]\n",
      " [ 0  1  2  0]\n",
      " [ 0  5 11  6]\n",
      " [ 0  0  0  3]]\n",
      "> NDCG Score | role | categ  | 0.84067\n",
      "> NDCG Score | role | proba* | 0.90207\n",
      "> NDCG Score | full | categ  | 0.95140\n",
      "> NDCG Score | full | proba* | 0.95381\n",
      "=== ISSUER ======\n",
      "Items in training set: 109 (84.50%)\n",
      "Items in eval set: 20\n",
      "Items in test set: 98\n",
      " = 227\n",
      "Number of source documents: 46 total, 21 train, 3 eval 22 test\n",
      "Absolute (training): IR 29.00, N 43.00, R 23.00, HR 14.00\n",
      "Relative (training): IR 0.27, N 0.39, R 0.21, HR 0.13\n",
      "Absolute (eval): IR 3.00, N 2.00, R 7.00, HR 8.00\n",
      "Relative (eval): IR 0.15, N 0.10, R 0.35, HR 0.40\n",
      "Role samples for ISSUER in train: 109, eval: 20, test: 98\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.817504328601 | std = 0.0327401170005\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.621677771913\n",
      "Accuracy | role : 0.30612244898\n",
      "Accuracy | full : 0.316326530612\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.71      0.48      0.57        21\n",
      "    neutral       0.33      0.15      0.21        27\n",
      "   relevant       0.22      0.48      0.30        27\n",
      "     highly       0.31      0.17      0.22        23\n",
      "\n",
      "avg / total       0.38      0.32      0.31        98\n",
      "\n",
      "[[10  0 11  0]\n",
      " [ 2  4 18  3]\n",
      " [ 2  6 13  6]\n",
      " [ 0  2 17  4]]\n",
      "> NDCG Score | role | categ  | 0.84903\n",
      "> NDCG Score | role | proba* | 0.83336\n",
      "> NDCG Score | full | categ  | 0.85368\n",
      "> NDCG Score | full | proba* | 0.87541\n",
      "=== UNDERWRITER ======\n",
      "Items in training set: 19 (90.48%)\n",
      "Items in eval set: 2\n",
      "Items in test set: 40\n",
      " = 61\n",
      "Number of source documents: 16 total, 6 train, 1 eval 9 test\n",
      "Absolute (training): IR 0.00, N 0.00, R 12.00, HR 7.00\n",
      "Relative (training): IR 0.00, N 0.00, R 0.63, HR 0.37\n",
      "Absolute (eval): IR 0.00, N 0.00, R 2.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for UNDERWRITER in train: 19, eval: 2, test: 40\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.908006997794 | std = 0.0377869477764\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.721180267881\n",
      "Accuracy | role : 0.375\n",
      "Accuracy | full : 0.525\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.17      1.00      0.29         2\n",
      "   relevant       0.53      0.67      0.59        15\n",
      "     highly       1.00      0.45      0.62        20\n",
      "\n",
      "avg / total       0.71      0.53      0.55        40\n",
      "\n",
      "[[ 0  1  2  0]\n",
      " [ 0  2  0  0]\n",
      " [ 0  5 10  0]\n",
      " [ 0  4  7  9]]\n",
      "> NDCG Score | role | categ  | 0.86908\n",
      "> NDCG Score | role | proba* | 0.84526\n",
      "> NDCG Score | full | categ  | 0.97421\n",
      "> NDCG Score | full | proba* | 0.98289\n",
      "=== SERVICER ======\n",
      "Items in training set: 19 (90.48%)\n",
      "Items in eval set: 2\n",
      "Items in test set: 57\n",
      " = 78\n",
      "Number of source documents: 18 total, 7 train, 1 eval 10 test\n",
      "Absolute (training): IR 0.00, N 4.00, R 6.00, HR 9.00\n",
      "Relative (training): IR 0.00, N 0.21, R 0.32, HR 0.47\n",
      "Absolute (eval): IR 0.00, N 2.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 1.00, R 0.00, HR 0.00\n",
      "Role samples for SERVICER in train: 19, eval: 2, test: 57\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.882676121833 | std = 0.0261045537872\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.717524169319\n",
      "Accuracy | role : 0.350877192982\n",
      "Accuracy | full : 0.491228070175\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.20      0.11      0.14         9\n",
      "   relevant       0.62      0.66      0.64        38\n",
      "     highly       0.17      0.29      0.21         7\n",
      "\n",
      "avg / total       0.47      0.49      0.48        57\n",
      "\n",
      "[[ 0  0  3  0]\n",
      " [ 0  1  8  0]\n",
      " [ 0  3 25 10]\n",
      " [ 0  1  4  2]]\n",
      "> NDCG Score | role | categ  | 0.88610\n",
      "> NDCG Score | role | proba* | 0.92632\n",
      "> NDCG Score | full | categ  | 0.89569\n",
      "> NDCG Score | full | proba* | 0.88326\n",
      "=== TRUSTEE ======\n",
      "Items in training set: 291 (69.29%)\n",
      "Items in eval set: 129\n",
      "Items in test set: 304\n",
      " = 724\n",
      "Number of source documents: 40 total, 18 train, 3 eval 19 test\n",
      "Absolute (training): IR 21.00, N 115.00, R 70.00, HR 85.00\n",
      "Relative (training): IR 0.07, N 0.40, R 0.24, HR 0.29\n",
      "Absolute (eval): IR 0.00, N 49.00, R 9.00, HR 71.00\n",
      "Relative (eval): IR 0.00, N 0.38, R 0.07, HR 0.55\n",
      "Role samples for TRUSTEE in train: 291, eval: 129, test: 304\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.931202213822 | std = 0.0111950710907\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.814144820697\n",
      "Accuracy | role : 0.394736842105\n",
      "Accuracy | full : 0.516447368421\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         9\n",
      "    neutral       0.16      0.36      0.23        47\n",
      "   relevant       0.45      0.23      0.31       124\n",
      "     highly       0.83      0.90      0.86       124\n",
      "\n",
      "avg / total       0.55      0.52      0.51       304\n",
      "\n",
      "[[  0   1   5   3]\n",
      " [  2  17  26   2]\n",
      " [  0  77  29  18]\n",
      " [  0   9   4 111]]\n",
      "> NDCG Score | role | categ  | 0.95338\n",
      "> NDCG Score | role | proba* | 0.95031\n",
      "> NDCG Score | full | categ  | 0.98597\n",
      "> NDCG Score | full | proba* | 0.97990\n",
      "=== COUNTERPART ======\n",
      "Items in training set: 63 (98.44%)\n",
      "Items in eval set: 1\n",
      "Items in test set: 108\n",
      " = 172\n",
      "Number of source documents: 31 total, 16 train, 1 eval 14 test\n",
      "Absolute (training): IR 6.00, N 12.00, R 29.00, HR 16.00\n",
      "Relative (training): IR 0.10, N 0.19, R 0.46, HR 0.25\n",
      "Absolute (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for COUNTERPART in train: 63, eval: 1, test: 108\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.860436553498 | std = 0.0292564668944\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.677182649456\n",
      "Accuracy | role : 0.388888888889\n",
      "Accuracy | full : 0.398148148148\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       1.00      0.06      0.11        18\n",
      "    neutral       0.10      0.06      0.07        17\n",
      "   relevant       0.33      0.58      0.42        38\n",
      "     highly       0.61      0.54      0.58        35\n",
      "\n",
      "avg / total       0.50      0.40      0.36       108\n",
      "\n",
      "[[ 1  1 16  0]\n",
      " [ 0  1 15  1]\n",
      " [ 0  5 22 11]\n",
      " [ 0  3 13 19]]\n",
      "> NDCG Score | role | categ  | 0.94220\n",
      "> NDCG Score | role | proba* | 0.85121\n",
      "> NDCG Score | full | categ  | 0.93072\n",
      "> NDCG Score | full | proba* | 0.93995\n",
      "=== SELLER ======\n",
      "Items in training set: 17 (85.00%)\n",
      "Items in eval set: 3\n",
      "Items in test set: 49\n",
      " = 69\n",
      "Number of source documents: 19 total, 9 train, 1 eval 9 test\n",
      "Absolute (training): IR 0.00, N 1.00, R 11.00, HR 5.00\n",
      "Relative (training): IR 0.00, N 0.06, R 0.65, HR 0.29\n",
      "Absolute (eval): IR 0.00, N 0.00, R 3.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for SELLER in train: 17, eval: 3, test: 49\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.866089351788 | std = 0.0397533310384\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.674252161736\n",
      "Accuracy | role : 0.34693877551\n",
      "Accuracy | full : 0.510204081633\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.00      0.00      0.00        10\n",
      "   relevant       0.47      0.88      0.61        17\n",
      "     highly       0.83      0.56      0.67        18\n",
      "\n",
      "avg / total       0.47      0.51      0.46        49\n",
      "\n",
      "[[ 0  0  4  0]\n",
      " [ 0  0  9  1]\n",
      " [ 0  1 15  1]\n",
      " [ 0  4  4 10]]\n",
      "> NDCG Score | role | categ  | 0.80424\n",
      "> NDCG Score | role | proba* | 0.87101\n",
      "> NDCG Score | full | categ  | 0.95394\n",
      "> NDCG Score | full | proba* | 0.91708\n",
      "=== AGENT ======\n",
      "Items in training set: 49 (80.33%)\n",
      "Items in eval set: 12\n",
      "Items in test set: 40\n",
      " = 101\n",
      "Number of source documents: 25 total, 13 train, 3 eval 9 test\n",
      "Absolute (training): IR 3.00, N 29.00, R 14.00, HR 3.00\n",
      "Relative (training): IR 0.06, N 0.59, R 0.29, HR 0.06\n",
      "Absolute (eval): IR 0.00, N 6.00, R 4.00, HR 2.00\n",
      "Relative (eval): IR 0.00, N 0.50, R 0.33, HR 0.17\n",
      "Role samples for AGENT in train: 49, eval: 12, test: 40\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.854804552428 | std = 0.0373773782604\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.650212572736\n",
      "Accuracy | role : 0.2\n",
      "Accuracy | full : 0.425\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.18      0.25      0.21         8\n",
      "   relevant       0.46      0.73      0.56        15\n",
      "     highly       0.80      0.31      0.44        13\n",
      "\n",
      "avg / total       0.47      0.42      0.40        40\n",
      "\n",
      "[[ 0  3  0  1]\n",
      " [ 0  2  6  0]\n",
      " [ 0  4 11  0]\n",
      " [ 0  2  7  4]]\n",
      "> NDCG Score | role | categ  | 0.83777\n",
      "> NDCG Score | role | proba* | 0.81160\n",
      "> NDCG Score | full | categ  | 0.88626\n",
      "> NDCG Score | full | proba* | 0.93328\n",
      "=== AFFILIATE ======\n",
      "Items in training set: 155 (83.33%)\n",
      "Items in eval set: 31\n",
      "Items in test set: 129\n",
      " = 315\n",
      "Number of source documents: 41 total, 19 train, 3 eval 19 test\n",
      "Absolute (training): IR 23.00, N 37.00, R 54.00, HR 41.00\n",
      "Relative (training): IR 0.15, N 0.24, R 0.35, HR 0.26\n",
      "Absolute (eval): IR 3.00, N 3.00, R 8.00, HR 17.00\n",
      "Relative (eval): IR 0.10, N 0.10, R 0.26, HR 0.55\n",
      "Role samples for AFFILIATE in train: 155, eval: 31, test: 129\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.892312480085 | std = 0.0231252432616\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.720304562167\n",
      "Accuracy | role : 0.488372093023\n",
      "Accuracy | full : 0.496124031008\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.20      0.06      0.09        17\n",
      "    neutral       0.07      0.08      0.08        12\n",
      "   relevant       0.49      0.69      0.57        54\n",
      "     highly       0.71      0.54      0.62        46\n",
      "\n",
      "avg / total       0.49      0.50      0.48       129\n",
      "\n",
      "[[ 1  1 15  0]\n",
      " [ 1  1  9  1]\n",
      " [ 3  5 37  9]\n",
      " [ 0  7 14 25]]\n",
      "> NDCG Score | role | categ  | 0.95093\n",
      "> NDCG Score | role | proba* | 0.97226\n",
      "> NDCG Score | full | categ  | 0.93319\n",
      "> NDCG Score | full | proba* | 0.96299\n",
      "=== INSURER ======\n",
      "Items in training set: 18 (94.74%)\n",
      "Items in eval set: 1\n",
      "Items in test set: 47\n",
      " = 66\n",
      "Number of source documents: 15 total, 7 train, 1 eval 7 test\n",
      "Absolute (training): IR 1.00, N 1.00, R 7.00, HR 9.00\n",
      "Relative (training): IR 0.06, N 0.06, R 0.39, HR 0.50\n",
      "Absolute (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for INSURER in train: 18, eval: 1, test: 47\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.906263959845 | std = 0.0259821026274\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.755505248883\n",
      "Accuracy | role : 0.425531914894\n",
      "Accuracy | full : 0.68085106383\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         1\n",
      "    neutral       1.00      0.14      0.25         7\n",
      "   relevant       0.62      0.68      0.65        19\n",
      "     highly       0.72      0.90      0.80        20\n",
      "\n",
      "avg / total       0.71      0.68      0.64        47\n",
      "\n",
      "[[ 0  0  1  0]\n",
      " [ 0  1  5  1]\n",
      " [ 0  0 13  6]\n",
      " [ 0  0  2 18]]\n",
      "> NDCG Score | role | categ  | 0.89620\n",
      "> NDCG Score | role | proba* | 0.91170\n",
      "> NDCG Score | full | categ  | 0.93389\n",
      "> NDCG Score | full | proba* | 0.92517\n",
      "TOTAL NDCG | role | categ  | 0.96185\n",
      "TOTAL NDCG | role | proba* | 0.93688\n",
      "TOTAL NDCG | full | categ  | 0.97525\n",
      "TOTAL NDCG | full | proba* | 0.96588\n",
      "baseline_rand 10\n",
      "baseline_worst 10\n",
      "ndcg_role 10\n",
      "ndcg_full 10\n",
      "ndcg_role_proba 10\n",
      "ndcg_full_proba 10\n",
      "acc_role 10\n",
      "acc_full 10\n",
      "f1_role 10\n",
      "f1_full 10\n",
      "\n",
      "\n",
      "==========================================================================\n",
      "===                      CROSSEVAL ITERATION 2/5                     =====\n",
      "==========================================================================\n",
      "\n",
      "\n",
      "Items in training set: 787 (80.72%)\n",
      "Items in eval set: 188\n",
      "Items in test set: 900\n",
      " = 1875\n",
      "Number of source documents: 50 total, 22 train, 3 eval 25 test\n",
      "Absolute (training): IR 77.00, N 295.00, R 257.00, HR 158.00\n",
      "Relative (training): IR 0.10, N 0.37, R 0.33, HR 0.20\n",
      "Absolute (eval): IR 13.00, N 12.00, R 26.00, HR 137.00\n",
      "Relative (eval): IR 0.07, N 0.06, R 0.14, HR 0.73\n",
      "Role samples for TRUSTEE in train: 328, eval: 92, test: 304\n",
      "Role samples for COUNTERPART in train: 43, eval: 21, test: 108\n",
      "Role samples for SELLER in train: 18, eval: 2, test: 49\n",
      "Role samples for GUARANTOR in train: 34, eval: 0, test: 28\n",
      "Role samples for ISSUER in train: 119, eval: 10, test: 98\n",
      "Role samples for AGENT in train: 57, eval: 4, test: 40\n",
      "Role samples for UNDERWRITER in train: 17, eval: 4, test: 40\n",
      "Role samples for SERVICER in train: 14, eval: 7, test: 57\n",
      "Role samples for AFFILIATE in train: 149, eval: 37, test: 129\n",
      "Role samples for INSURER in train: 8, eval: 11, test: 47\n",
      "=== GUARANTOR ======\n",
      "Items in training set: 34 (100.00%)\n",
      "Items in eval set: 0\n",
      "Items in test set: 28\n",
      " = 62\n",
      "Number of source documents: 15 total, 6 train, 0 eval 9 test\n",
      "Absolute (training): IR 1.00, N 3.00, R 22.00, HR 8.00\n",
      "Relative (training): IR 0.03, N 0.09, R 0.65, HR 0.24\n",
      "Absolute (eval): IR 0.00, N 0.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR nan, N nan, R nan, HR nan\n",
      "Role samples for GUARANTOR in train: 34, eval: 0, test: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/Uni/HPI/workspace/FEII/code/feiii_data.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  rating_agg.sum()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.905324891457 | std = 0.0255478279368\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.810440241115\n",
      "Accuracy | role : 0.785714285714\n",
      "Accuracy | full : 0.5\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         0\n",
      "    neutral       0.17      0.33      0.22         3\n",
      "   relevant       0.79      0.50      0.61        22\n",
      "     highly       0.25      0.67      0.36         3\n",
      "\n",
      "avg / total       0.66      0.50      0.54        28\n",
      "\n",
      "[[ 0  0  0  0]\n",
      " [ 0  1  2  0]\n",
      " [ 0  5 11  6]\n",
      " [ 0  0  1  2]]\n",
      "> NDCG Score | role | categ  | 0.84067\n",
      "> NDCG Score | role | proba* | 0.90207\n",
      "> NDCG Score | full | categ  | 0.91938\n",
      "> NDCG Score | full | proba* | 0.94848\n",
      "=== ISSUER ======\n",
      "Items in training set: 119 (92.25%)\n",
      "Items in eval set: 10\n",
      "Items in test set: 98\n",
      " = 227\n",
      "Number of source documents: 46 total, 21 train, 3 eval 22 test\n",
      "Absolute (training): IR 27.00, N 43.00, R 28.00, HR 21.00\n",
      "Relative (training): IR 0.23, N 0.36, R 0.24, HR 0.18\n",
      "Absolute (eval): IR 5.00, N 2.00, R 2.00, HR 1.00\n",
      "Relative (eval): IR 0.50, N 0.20, R 0.20, HR 0.10\n",
      "Role samples for ISSUER in train: 119, eval: 10, test: 98\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.807876625219 | std = 0.0341858477199\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.621677771913\n",
      "Accuracy | role : 0.34693877551\n",
      "Accuracy | full : 0.336734693878\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.80      0.57      0.67        21\n",
      "    neutral       0.24      0.15      0.18        27\n",
      "   relevant       0.25      0.56      0.34        27\n",
      "     highly       0.33      0.09      0.14        23\n",
      "\n",
      "avg / total       0.38      0.34      0.32        98\n",
      "\n",
      "[[12  2  7  0]\n",
      " [ 2  4 18  3]\n",
      " [ 1 10 15  1]\n",
      " [ 0  1 20  2]]\n",
      "> NDCG Score | role | categ  | 0.85727\n",
      "> NDCG Score | role | proba* | 0.88242\n",
      "> NDCG Score | full | categ  | 0.86582\n",
      "> NDCG Score | full | proba* | 0.85733\n",
      "=== UNDERWRITER ======\n",
      "Items in training set: 17 (80.95%)\n",
      "Items in eval set: 4\n",
      "Items in test set: 40\n",
      " = 61\n",
      "Number of source documents: 16 total, 5 train, 2 eval 9 test\n",
      "Absolute (training): IR 0.00, N 0.00, R 14.00, HR 3.00\n",
      "Relative (training): IR 0.00, N 0.00, R 0.82, HR 0.18\n",
      "Absolute (eval): IR 0.00, N 0.00, R 0.00, HR 4.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 0.00, HR 1.00\n",
      "Role samples for UNDERWRITER in train: 17, eval: 4, test: 40\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.906009045562 | std = 0.0353011538561\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.721180267881\n",
      "Accuracy | role : 0.375\n",
      "Accuracy | full : 0.675\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.00      0.00      0.00         2\n",
      "   relevant       0.56      1.00      0.71        15\n",
      "     highly       1.00      0.60      0.75        20\n",
      "\n",
      "avg / total       0.71      0.68      0.64        40\n",
      "\n",
      "[[ 0  1  2  0]\n",
      " [ 0  0  2  0]\n",
      " [ 0  0 15  0]\n",
      " [ 0  0  8 12]]\n",
      "> NDCG Score | role | categ  | 0.86908\n",
      "> NDCG Score | role | proba* | 0.87247\n",
      "> NDCG Score | full | categ  | 0.99169\n",
      "> NDCG Score | full | proba* | 0.98345\n",
      "=== SERVICER ======\n",
      "Items in training set: 14 (66.67%)\n",
      "Items in eval set: 7\n",
      "Items in test set: 57\n",
      " = 78\n",
      "Number of source documents: 18 total, 6 train, 2 eval 10 test\n",
      "Absolute (training): IR 0.00, N 6.00, R 6.00, HR 2.00\n",
      "Relative (training): IR 0.00, N 0.43, R 0.43, HR 0.14\n",
      "Absolute (eval): IR 0.00, N 0.00, R 0.00, HR 7.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 0.00, HR 1.00\n",
      "Role samples for SERVICER in train: 14, eval: 7, test: 57\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.883668324941 | std = 0.0244836575862\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.717524169319\n",
      "Accuracy | role : 0.175438596491\n",
      "Accuracy | full : 0.473684210526\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.50      0.22      0.31         9\n",
      "   relevant       0.60      0.63      0.62        38\n",
      "     highly       0.08      0.14      0.10         7\n",
      "\n",
      "avg / total       0.49      0.47      0.47        57\n",
      "\n",
      "[[ 0  0  3  0]\n",
      " [ 0  2  7  0]\n",
      " [ 0  2 24 12]\n",
      " [ 0  0  6  1]]\n",
      "> NDCG Score | role | categ  | 0.85663\n",
      "> NDCG Score | role | proba* | 0.91809\n",
      "> NDCG Score | full | categ  | 0.89858\n",
      "> NDCG Score | full | proba* | 0.84893\n",
      "=== TRUSTEE ======\n",
      "Items in training set: 328 (78.10%)\n",
      "Items in eval set: 92\n",
      "Items in test set: 304\n",
      " = 724\n",
      "Number of source documents: 40 total, 19 train, 2 eval 19 test\n",
      "Absolute (training): IR 17.00, N 160.00, R 79.00, HR 72.00\n",
      "Relative (training): IR 0.05, N 0.49, R 0.24, HR 0.22\n",
      "Absolute (eval): IR 4.00, N 4.00, R 0.00, HR 84.00\n",
      "Relative (eval): IR 0.04, N 0.04, R 0.00, HR 0.91\n",
      "Role samples for TRUSTEE in train: 328, eval: 92, test: 304\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.931369592619 | std = 0.00998041432334\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.814144820697\n",
      "Accuracy | role : 0.473684210526\n",
      "Accuracy | full : 0.486842105263\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         9\n",
      "    neutral       0.15      0.32      0.20        47\n",
      "   relevant       0.38      0.30      0.33       124\n",
      "     highly       0.92      0.77      0.84       124\n",
      "\n",
      "avg / total       0.55      0.49      0.51       304\n",
      "\n",
      "[[ 0  3  6  0]\n",
      " [ 0 15 29  3]\n",
      " [ 0 82 37  5]\n",
      " [ 0  3 25 96]]\n",
      "> NDCG Score | role | categ  | 0.97811\n",
      "> NDCG Score | role | proba* | 0.98735\n",
      "> NDCG Score | full | categ  | 0.98735\n",
      "> NDCG Score | full | proba* | 0.98927\n",
      "=== COUNTERPART ======\n",
      "Items in training set: 43 (67.19%)\n",
      "Items in eval set: 21\n",
      "Items in test set: 108\n",
      " = 172\n",
      "Number of source documents: 31 total, 14 train, 3 eval 14 test\n",
      "Absolute (training): IR 3.00, N 8.00, R 25.00, HR 7.00\n",
      "Relative (training): IR 0.07, N 0.19, R 0.58, HR 0.16\n",
      "Absolute (eval): IR 3.00, N 4.00, R 5.00, HR 9.00\n",
      "Relative (eval): IR 0.14, N 0.19, R 0.24, HR 0.43\n",
      "Role samples for COUNTERPART in train: 43, eval: 21, test: 108\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.867251272895 | std = 0.0301443685155\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.677182649456\n",
      "Accuracy | role : 0.240740740741\n",
      "Accuracy | full : 0.351851851852\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00        18\n",
      "    neutral       0.00      0.00      0.00        17\n",
      "   relevant       0.34      0.79      0.48        38\n",
      "     highly       0.50      0.23      0.31        35\n",
      "\n",
      "avg / total       0.28      0.35      0.27       108\n",
      "\n",
      "[[ 0  3 15  0]\n",
      " [ 0  0 16  1]\n",
      " [ 0  1 30  7]\n",
      " [ 0  1 26  8]]\n",
      "> NDCG Score | role | categ  | 0.80440\n",
      "> NDCG Score | role | proba* | 0.76460\n",
      "> NDCG Score | full | categ  | 0.91528\n",
      "> NDCG Score | full | proba* | 0.90318\n",
      "=== SELLER ======\n",
      "Items in training set: 18 (90.00%)\n",
      "Items in eval set: 2\n",
      "Items in test set: 49\n",
      " = 69\n",
      "Number of source documents: 19 total, 8 train, 2 eval 9 test\n",
      "Absolute (training): IR 0.00, N 0.00, R 14.00, HR 4.00\n",
      "Relative (training): IR 0.00, N 0.00, R 0.78, HR 0.22\n",
      "Absolute (eval): IR 0.00, N 1.00, R 0.00, HR 1.00\n",
      "Relative (eval): IR 0.00, N 0.50, R 0.00, HR 0.50\n",
      "Role samples for SELLER in train: 18, eval: 2, test: 49\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.868229496588 | std = 0.0342601876169\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.674252161736\n",
      "Accuracy | role : 0.34693877551\n",
      "Accuracy | full : 0.510204081633\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.00      0.00      0.00        10\n",
      "   relevant       0.40      0.82      0.54        17\n",
      "     highly       0.92      0.61      0.73        18\n",
      "\n",
      "avg / total       0.48      0.51      0.46        49\n",
      "\n",
      "[[ 0  0  4  0]\n",
      " [ 0  0 10  0]\n",
      " [ 0  2 14  1]\n",
      " [ 0  0  7 11]]\n",
      "> NDCG Score | role | categ  | 0.80424\n",
      "> NDCG Score | role | proba* | 0.90947\n",
      "> NDCG Score | full | categ  | 0.96264\n",
      "> NDCG Score | full | proba* | 0.95860\n",
      "=== AGENT ======\n",
      "Items in training set: 57 (93.44%)\n",
      "Items in eval set: 4\n",
      "Items in test set: 40\n",
      " = 101\n",
      "Number of source documents: 25 total, 15 train, 1 eval 9 test\n",
      "Absolute (training): IR 3.00, N 35.00, R 14.00, HR 5.00\n",
      "Relative (training): IR 0.05, N 0.61, R 0.25, HR 0.09\n",
      "Absolute (eval): IR 0.00, N 0.00, R 4.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for AGENT in train: 57, eval: 4, test: 40\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.85234044097 | std = 0.0362345197849\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.650212572736\n",
      "Accuracy | role : 0.2\n",
      "Accuracy | full : 0.35\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.20      0.25      0.22         8\n",
      "   relevant       0.38      0.73      0.50        15\n",
      "     highly       1.00      0.08      0.14        13\n",
      "\n",
      "avg / total       0.51      0.35      0.28        40\n",
      "\n",
      "[[ 0  3  1  0]\n",
      " [ 0  2  6  0]\n",
      " [ 0  4 11  0]\n",
      " [ 0  1 11  1]]\n",
      "> NDCG Score | role | categ  | 0.81118\n",
      "> NDCG Score | role | proba* | 0.76213\n",
      "> NDCG Score | full | categ  | 0.91181\n",
      "> NDCG Score | full | proba* | 0.85700\n",
      "=== AFFILIATE ======\n",
      "Items in training set: 149 (80.11%)\n",
      "Items in eval set: 37\n",
      "Items in test set: 129\n",
      " = 315\n",
      "Number of source documents: 41 total, 19 train, 3 eval 19 test\n",
      "Absolute (training): IR 26.00, N 40.00, R 48.00, HR 35.00\n",
      "Relative (training): IR 0.17, N 0.27, R 0.32, HR 0.23\n",
      "Absolute (eval): IR 0.00, N 0.00, R 14.00, HR 23.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 0.38, HR 0.62\n",
      "Role samples for AFFILIATE in train: 149, eval: 37, test: 129\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.892177647289 | std = 0.0210444053902\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.720304562167\n",
      "Accuracy | role : 0.542635658915\n",
      "Accuracy | full : 0.53488372093\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00        17\n",
      "    neutral       0.18      0.25      0.21        12\n",
      "   relevant       0.51      0.72      0.60        54\n",
      "     highly       0.79      0.59      0.68        46\n",
      "\n",
      "avg / total       0.51      0.53      0.51       129\n",
      "\n",
      "[[ 0  2 15  0]\n",
      " [ 0  3  9  0]\n",
      " [ 1  7 39  7]\n",
      " [ 0  5 14 27]]\n",
      "> NDCG Score | role | categ  | 0.97733\n",
      "> NDCG Score | role | proba* | 0.97030\n",
      "> NDCG Score | full | categ  | 0.95964\n",
      "> NDCG Score | full | proba* | 0.95343\n",
      "=== INSURER ======\n",
      "Items in training set: 8 (42.11%)\n",
      "Items in eval set: 11\n",
      "Items in test set: 47\n",
      " = 66\n",
      "Number of source documents: 15 total, 6 train, 2 eval 7 test\n",
      "Absolute (training): IR 0.00, N 0.00, R 7.00, HR 1.00\n",
      "Relative (training): IR 0.00, N 0.00, R 0.88, HR 0.12\n",
      "Absolute (eval): IR 1.00, N 1.00, R 1.00, HR 8.00\n",
      "Relative (eval): IR 0.09, N 0.09, R 0.09, HR 0.73\n",
      "Role samples for INSURER in train: 8, eval: 11, test: 47\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.910414417966 | std = 0.0292974869853\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.755505248883\n",
      "Accuracy | role : 0.404255319149\n",
      "Accuracy | full : 0.468085106383\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         1\n",
      "    neutral       0.00      0.00      0.00         7\n",
      "   relevant       0.39      0.58      0.47        19\n",
      "     highly       0.65      0.55      0.59        20\n",
      "\n",
      "avg / total       0.43      0.47      0.44        47\n",
      "\n",
      "[[ 0  0  1  0]\n",
      " [ 0  0  7  0]\n",
      " [ 0  2 11  6]\n",
      " [ 0  0  9 11]]\n",
      "> NDCG Score | role | categ  | 0.89620\n",
      "> NDCG Score | role | proba* | 0.88323\n",
      "> NDCG Score | full | categ  | 0.96415\n",
      "> NDCG Score | full | proba* | 0.97563\n",
      "TOTAL NDCG | role | categ  | 0.95489\n",
      "TOTAL NDCG | role | proba* | 0.94060\n",
      "TOTAL NDCG | full | categ  | 0.97355\n",
      "TOTAL NDCG | full | proba* | 0.95644\n",
      "baseline_rand 10\n",
      "baseline_worst 10\n",
      "ndcg_role 10\n",
      "ndcg_full 10\n",
      "ndcg_role_proba 10\n",
      "ndcg_full_proba 10\n",
      "acc_role 10\n",
      "acc_full 10\n",
      "f1_role 10\n",
      "f1_full 10\n",
      "\n",
      "\n",
      "==========================================================================\n",
      "===                      CROSSEVAL ITERATION 3/5                     =====\n",
      "==========================================================================\n",
      "\n",
      "\n",
      "Items in training set: 844 (86.56%)\n",
      "Items in eval set: 131\n",
      "Items in test set: 900\n",
      " = 1875\n",
      "Number of source documents: 50 total, 22 train, 3 eval 25 test\n",
      "Absolute (training): IR 63.00, N 249.00, R 255.00, HR 277.00\n",
      "Relative (training): IR 0.07, N 0.30, R 0.30, HR 0.33\n",
      "Absolute (eval): IR 27.00, N 58.00, R 28.00, HR 18.00\n",
      "Relative (eval): IR 0.21, N 0.44, R 0.21, HR 0.14\n",
      "Role samples for TRUSTEE in train: 394, eval: 26, test: 304\n",
      "Role samples for COUNTERPART in train: 52, eval: 12, test: 108\n",
      "Role samples for SELLER in train: 14, eval: 6, test: 49\n",
      "Role samples for GUARANTOR in train: 32, eval: 2, test: 28\n",
      "Role samples for ISSUER in train: 97, eval: 32, test: 98\n",
      "Role samples for AGENT in train: 52, eval: 9, test: 40\n",
      "Role samples for UNDERWRITER in train: 19, eval: 2, test: 40\n",
      "Role samples for SERVICER in train: 15, eval: 6, test: 57\n",
      "Role samples for AFFILIATE in train: 152, eval: 34, test: 129\n",
      "Role samples for INSURER in train: 17, eval: 2, test: 47\n",
      "=== GUARANTOR ======\n",
      "Items in training set: 32 (94.12%)\n",
      "Items in eval set: 2\n",
      "Items in test set: 28\n",
      " = 62\n",
      "Number of source documents: 15 total, 5 train, 1 eval 9 test\n",
      "Absolute (training): IR 0.00, N 2.00, R 22.00, HR 8.00\n",
      "Relative (training): IR 0.00, N 0.06, R 0.69, HR 0.25\n",
      "Absolute (eval): IR 1.00, N 1.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR 0.50, N 0.50, R 0.00, HR 0.00\n",
      "Role samples for GUARANTOR in train: 32, eval: 2, test: 28\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.906869289134 | std = 0.0255132135446\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.810440241115\n",
      "Accuracy | role : 0.714285714286\n",
      "Accuracy | full : 0.464285714286\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         0\n",
      "    neutral       0.00      0.00      0.00         3\n",
      "   relevant       0.77      0.45      0.57        22\n",
      "     highly       0.30      1.00      0.46         3\n",
      "\n",
      "avg / total       0.64      0.46      0.50        28\n",
      "\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  3  0]\n",
      " [ 0  5 10  7]\n",
      " [ 0  0  0  3]]\n",
      "> NDCG Score | role | categ  | 0.93792\n",
      "> NDCG Score | role | proba* | 0.91263\n",
      "> NDCG Score | full | categ  | 0.94810\n",
      "> NDCG Score | full | proba* | 0.92187\n",
      "=== ISSUER ======\n",
      "Items in training set: 97 (75.19%)\n",
      "Items in eval set: 32\n",
      "Items in test set: 98\n",
      " = 227\n",
      "Number of source documents: 46 total, 21 train, 3 eval 22 test\n",
      "Absolute (training): IR 30.00, N 25.00, R 22.00, HR 20.00\n",
      "Relative (training): IR 0.31, N 0.26, R 0.23, HR 0.21\n",
      "Absolute (eval): IR 2.00, N 20.00, R 8.00, HR 2.00\n",
      "Relative (eval): IR 0.06, N 0.62, R 0.25, HR 0.06\n",
      "Role samples for ISSUER in train: 97, eval: 32, test: 98\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.811261229913 | std = 0.0334767635479\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.621677771913\n",
      "Accuracy | role : 0.34693877551\n",
      "Accuracy | full : 0.295918367347\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.69      0.43      0.53        21\n",
      "    neutral       0.20      0.11      0.14        27\n",
      "   relevant       0.24      0.41      0.30        27\n",
      "     highly       0.25      0.26      0.26        23\n",
      "\n",
      "avg / total       0.33      0.30      0.30        98\n",
      "\n",
      "[[ 9  2  8  2]\n",
      " [ 2  3 12 10]\n",
      " [ 2  8 11  6]\n",
      " [ 0  2 15  6]]\n",
      "> NDCG Score | role | categ  | 0.83091\n",
      "> NDCG Score | role | proba* | 0.87381\n",
      "> NDCG Score | full | categ  | 0.85150\n",
      "> NDCG Score | full | proba* | 0.85186\n",
      "=== UNDERWRITER ======\n",
      "Items in training set: 19 (90.48%)\n",
      "Items in eval set: 2\n",
      "Items in test set: 40\n",
      " = 61\n",
      "Number of source documents: 16 total, 6 train, 1 eval 9 test\n",
      "Absolute (training): IR 0.00, N 0.00, R 14.00, HR 5.00\n",
      "Relative (training): IR 0.00, N 0.00, R 0.74, HR 0.26\n",
      "Absolute (eval): IR 0.00, N 0.00, R 0.00, HR 2.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 0.00, HR 1.00\n",
      "Role samples for UNDERWRITER in train: 19, eval: 2, test: 40\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.910469493423 | std = 0.0356695112374\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.721180267881\n",
      "Accuracy | role : 0.525\n",
      "Accuracy | full : 0.55\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.00      0.00      0.00         2\n",
      "   relevant       0.52      0.73      0.61        15\n",
      "     highly       0.65      0.55      0.59        20\n",
      "\n",
      "avg / total       0.52      0.55      0.53        40\n",
      "\n",
      "[[ 0  1  2  0]\n",
      " [ 0  0  0  2]\n",
      " [ 0  0 11  4]\n",
      " [ 0  1  8 11]]\n",
      "> NDCG Score | role | categ  | 0.93192\n",
      "> NDCG Score | role | proba* | 0.95056\n",
      "> NDCG Score | full | categ  | 0.94005\n",
      "> NDCG Score | full | proba* | 0.93860\n",
      "=== SERVICER ======\n",
      "Items in training set: 15 (71.43%)\n",
      "Items in eval set: 6\n",
      "Items in test set: 57\n",
      " = 78\n",
      "Number of source documents: 18 total, 5 train, 3 eval 10 test\n",
      "Absolute (training): IR 0.00, N 3.00, R 3.00, HR 9.00\n",
      "Relative (training): IR 0.00, N 0.20, R 0.20, HR 0.60\n",
      "Absolute (eval): IR 0.00, N 3.00, R 3.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.50, R 0.50, HR 0.00\n",
      "Role samples for SERVICER in train: 15, eval: 6, test: 57\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.878927360557 | std = 0.0243943259116\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.717524169319\n",
      "Accuracy | role : 0.122807017544\n",
      "Accuracy | full : 0.228070175439\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.29      0.22      0.25         9\n",
      "   relevant       0.38      0.21      0.27        38\n",
      "     highly       0.10      0.43      0.17         7\n",
      "\n",
      "avg / total       0.31      0.23      0.24        57\n",
      "\n",
      "[[ 0  0  2  1]\n",
      " [ 0  2  7  0]\n",
      " [ 0  5  8 25]\n",
      " [ 0  0  4  3]]\n",
      "> NDCG Score | role | categ  | 0.85663\n",
      "> NDCG Score | role | proba* | 0.83103\n",
      "> NDCG Score | full | categ  | 0.92354\n",
      "> NDCG Score | full | proba* | 0.85871\n",
      "=== TRUSTEE ======\n",
      "Items in training set: 394 (93.81%)\n",
      "Items in eval set: 26\n",
      "Items in test set: 304\n",
      " = 724\n",
      "Number of source documents: 40 total, 18 train, 3 eval 19 test\n",
      "Absolute (training): IR 5.00, N 154.00, R 79.00, HR 156.00\n",
      "Relative (training): IR 0.01, N 0.39, R 0.20, HR 0.40\n",
      "Absolute (eval): IR 16.00, N 10.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR 0.62, N 0.38, R 0.00, HR 0.00\n",
      "Role samples for TRUSTEE in train: 394, eval: 26, test: 304\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.930752530579 | std = 0.0098018586128\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.814144820697\n",
      "Accuracy | role : 0.480263157895\n",
      "Accuracy | full : 0.506578947368\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         9\n",
      "    neutral       0.16      0.28      0.20        47\n",
      "   relevant       0.53      0.22      0.31       124\n",
      "     highly       0.67      0.92      0.77       124\n",
      "\n",
      "avg / total       0.51      0.51      0.47       304\n",
      "\n",
      "[[  0   3   1   5]\n",
      " [  1  13  14  19]\n",
      " [  0  64  27  33]\n",
      " [  0   1   9 114]]\n",
      "> NDCG Score | role | categ  | 0.97351\n",
      "> NDCG Score | role | proba* | 0.98857\n",
      "> NDCG Score | full | categ  | 0.97343\n",
      "> NDCG Score | full | proba* | 0.99056\n",
      "=== COUNTERPART ======\n",
      "Items in training set: 52 (81.25%)\n",
      "Items in eval set: 12\n",
      "Items in test set: 108\n",
      " = 172\n",
      "Number of source documents: 31 total, 14 train, 3 eval 14 test\n",
      "Absolute (training): IR 4.00, N 10.00, R 24.00, HR 14.00\n",
      "Relative (training): IR 0.08, N 0.19, R 0.46, HR 0.27\n",
      "Absolute (eval): IR 2.00, N 2.00, R 6.00, HR 2.00\n",
      "Relative (eval): IR 0.17, N 0.17, R 0.50, HR 0.17\n",
      "Role samples for COUNTERPART in train: 52, eval: 12, test: 108\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.861172247415 | std = 0.028198091689\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.677182649456\n",
      "Accuracy | role : 0.342592592593\n",
      "Accuracy | full : 0.435185185185\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       1.00      0.06      0.11        18\n",
      "    neutral       0.50      0.24      0.32        17\n",
      "   relevant       0.35      0.50      0.41        38\n",
      "     highly       0.51      0.66      0.57        35\n",
      "\n",
      "avg / total       0.53      0.44      0.40       108\n",
      "\n",
      "[[ 1  2 14  1]\n",
      " [ 0  4  9  4]\n",
      " [ 0  2 19 17]\n",
      " [ 0  0 12 23]]\n",
      "> NDCG Score | role | categ  | 0.90525\n",
      "> NDCG Score | role | proba* | 0.78605\n",
      "> NDCG Score | full | categ  | 0.91764\n",
      "> NDCG Score | full | proba* | 0.94644\n",
      "=== SELLER ======\n",
      "Items in training set: 14 (70.00%)\n",
      "Items in eval set: 6\n",
      "Items in test set: 49\n",
      " = 69\n",
      "Number of source documents: 19 total, 8 train, 2 eval 9 test\n",
      "Absolute (training): IR 0.00, N 1.00, R 12.00, HR 1.00\n",
      "Relative (training): IR 0.00, N 0.07, R 0.86, HR 0.07\n",
      "Absolute (eval): IR 0.00, N 0.00, R 2.00, HR 4.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 0.33, HR 0.67\n",
      "Role samples for SELLER in train: 14, eval: 6, test: 49\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.868881436019 | std = 0.0351805745629\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.674252161736\n",
      "Accuracy | role : 0.34693877551\n",
      "Accuracy | full : 0.510204081633\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.00      0.00      0.00        10\n",
      "   relevant       0.40      0.59      0.48        17\n",
      "     highly       0.75      0.83      0.79        18\n",
      "\n",
      "avg / total       0.41      0.51      0.46        49\n",
      "\n",
      "[[ 0  0  4  0]\n",
      " [ 0  0  8  2]\n",
      " [ 0  4 10  3]\n",
      " [ 0  0  3 15]]\n",
      "> NDCG Score | role | categ  | 0.80424\n",
      "> NDCG Score | role | proba* | 0.93901\n",
      "> NDCG Score | full | categ  | 0.95808\n",
      "> NDCG Score | full | proba* | 0.96108\n",
      "=== AGENT ======\n",
      "Items in training set: 52 (85.25%)\n",
      "Items in eval set: 9\n",
      "Items in test set: 40\n",
      " = 101\n",
      "Number of source documents: 25 total, 14 train, 2 eval 9 test\n",
      "Absolute (training): IR 1.00, N 30.00, R 16.00, HR 5.00\n",
      "Relative (training): IR 0.02, N 0.58, R 0.31, HR 0.10\n",
      "Absolute (eval): IR 2.00, N 5.00, R 2.00, HR 0.00\n",
      "Relative (eval): IR 0.22, N 0.56, R 0.22, HR 0.00\n",
      "Role samples for AGENT in train: 52, eval: 9, test: 40\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.85234128452 | std = 0.0435860187421\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.650212572736\n",
      "Accuracy | role : 0.325\n",
      "Accuracy | full : 0.425\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.29      0.25      0.27         8\n",
      "   relevant       0.45      0.60      0.51        15\n",
      "     highly       0.46      0.46      0.46        13\n",
      "\n",
      "avg / total       0.38      0.42      0.40        40\n",
      "\n",
      "[[0 3 1 0]\n",
      " [0 2 3 3]\n",
      " [0 2 9 4]\n",
      " [0 0 7 6]]\n",
      "> NDCG Score | role | categ  | 0.83171\n",
      "> NDCG Score | role | proba* | 0.78325\n",
      "> NDCG Score | full | categ  | 0.86348\n",
      "> NDCG Score | full | proba* | 0.87004\n",
      "=== AFFILIATE ======\n",
      "Items in training set: 152 (81.72%)\n",
      "Items in eval set: 34\n",
      "Items in test set: 129\n",
      " = 315\n",
      "Number of source documents: 41 total, 19 train, 3 eval 19 test\n",
      "Absolute (training): IR 22.00, N 23.00, R 57.00, HR 50.00\n",
      "Relative (training): IR 0.14, N 0.15, R 0.38, HR 0.33\n",
      "Absolute (eval): IR 4.00, N 17.00, R 5.00, HR 8.00\n",
      "Relative (eval): IR 0.12, N 0.50, R 0.15, HR 0.24\n",
      "Role samples for AFFILIATE in train: 152, eval: 34, test: 129\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.893092574142 | std = 0.0212808638697\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.720304562167\n",
      "Accuracy | role : 0.480620155039\n",
      "Accuracy | full : 0.511627906977\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.14      0.06      0.08        17\n",
      "    neutral       0.00      0.00      0.00        12\n",
      "   relevant       0.52      0.59      0.55        54\n",
      "     highly       0.59      0.72      0.65        46\n",
      "\n",
      "avg / total       0.45      0.51      0.47       129\n",
      "\n",
      "[[ 1  2 12  2]\n",
      " [ 2  0  6  4]\n",
      " [ 4  1 32 17]\n",
      " [ 0  1 12 33]]\n",
      "> NDCG Score | role | categ  | 0.94274\n",
      "> NDCG Score | role | proba* | 0.97644\n",
      "> NDCG Score | full | categ  | 0.92611\n",
      "> NDCG Score | full | proba* | 0.97893\n",
      "=== INSURER ======\n",
      "Items in training set: 17 (89.47%)\n",
      "Items in eval set: 2\n",
      "Items in test set: 47\n",
      " = 66\n",
      "Number of source documents: 15 total, 7 train, 1 eval 7 test\n",
      "Absolute (training): IR 1.00, N 1.00, R 6.00, HR 9.00\n",
      "Relative (training): IR 0.06, N 0.06, R 0.35, HR 0.53\n",
      "Absolute (eval): IR 0.00, N 0.00, R 2.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for INSURER in train: 17, eval: 2, test: 47\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.903863743192 | std = 0.0311355147878\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.755505248883\n",
      "Accuracy | role : 0.446808510638\n",
      "Accuracy | full : 0.531914893617\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         1\n",
      "    neutral       0.00      0.00      0.00         7\n",
      "   relevant       0.45      0.47      0.46        19\n",
      "     highly       0.62      0.80      0.70        20\n",
      "\n",
      "avg / total       0.44      0.53      0.48        47\n",
      "\n",
      "[[ 0  0  1  0]\n",
      " [ 0  0  6  1]\n",
      " [ 0  1  9  9]\n",
      " [ 0  0  4 16]]\n",
      "> NDCG Score | role | categ  | 0.92402\n",
      "> NDCG Score | role | proba* | 0.93170\n",
      "> NDCG Score | full | categ  | 0.95295\n",
      "> NDCG Score | full | proba* | 0.98303\n",
      "TOTAL NDCG | role | categ  | 0.94500\n",
      "TOTAL NDCG | role | proba* | 0.96156\n",
      "TOTAL NDCG | full | categ  | 0.96773\n",
      "TOTAL NDCG | full | proba* | 0.96267\n",
      "baseline_rand 10\n",
      "baseline_worst 10\n",
      "ndcg_role 10\n",
      "ndcg_full 10\n",
      "ndcg_role_proba 10\n",
      "ndcg_full_proba 10\n",
      "acc_role 10\n",
      "acc_full 10\n",
      "f1_role 10\n",
      "f1_full 10\n",
      "\n",
      "\n",
      "==========================================================================\n",
      "===                      CROSSEVAL ITERATION 4/5                     =====\n",
      "==========================================================================\n",
      "\n",
      "\n",
      "Items in training set: 862 (88.41%)\n",
      "Items in eval set: 113\n",
      "Items in test set: 900\n",
      " = 1875\n",
      "Number of source documents: 50 total, 22 train, 3 eval 25 test\n",
      "Absolute (training): IR 70.00, N 256.00, R 249.00, HR 287.00\n",
      "Relative (training): IR 0.08, N 0.30, R 0.29, HR 0.33\n",
      "Absolute (eval): IR 20.00, N 51.00, R 34.00, HR 8.00\n",
      "Relative (eval): IR 0.18, N 0.45, R 0.30, HR 0.07\n",
      "Role samples for TRUSTEE in train: 390, eval: 30, test: 304\n",
      "Role samples for COUNTERPART in train: 55, eval: 9, test: 108\n",
      "Role samples for SELLER in train: 20, eval: 0, test: 49\n",
      "Role samples for GUARANTOR in train: 22, eval: 12, test: 28\n",
      "Role samples for ISSUER in train: 113, eval: 16, test: 98\n",
      "Role samples for AGENT in train: 49, eval: 12, test: 40\n",
      "Role samples for UNDERWRITER in train: 14, eval: 7, test: 40\n",
      "Role samples for SERVICER in train: 21, eval: 0, test: 57\n",
      "Role samples for AFFILIATE in train: 160, eval: 26, test: 129\n",
      "Role samples for INSURER in train: 18, eval: 1, test: 47\n",
      "=== GUARANTOR ======\n",
      "Items in training set: 22 (64.71%)\n",
      "Items in eval set: 12\n",
      "Items in test set: 28\n",
      " = 62\n",
      "Number of source documents: 15 total, 5 train, 1 eval 9 test\n",
      "Absolute (training): IR 1.00, N 3.00, R 10.00, HR 8.00\n",
      "Relative (training): IR 0.05, N 0.14, R 0.45, HR 0.36\n",
      "Absolute (eval): IR 0.00, N 0.00, R 12.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for GUARANTOR in train: 22, eval: 12, test: 28\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.905541640706 | std = 0.027626630816\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.810440241115\n",
      "Accuracy | role : 0.607142857143\n",
      "Accuracy | full : 0.535714285714\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         0\n",
      "    neutral       0.20      0.33      0.25         3\n",
      "   relevant       0.85      0.50      0.63        22\n",
      "     highly       0.30      1.00      0.46         3\n",
      "\n",
      "avg / total       0.72      0.54      0.57        28\n",
      "\n",
      "[[ 0  0  0  0]\n",
      " [ 0  1  2  0]\n",
      " [ 0  4 11  7]\n",
      " [ 0  0  0  3]]\n",
      "> NDCG Score | role | categ  | 0.91227\n",
      "> NDCG Score | role | proba* | 0.96087\n",
      "> NDCG Score | full | categ  | 0.94953\n",
      "> NDCG Score | full | proba* | 0.92726\n",
      "=== ISSUER ======\n",
      "Items in training set: 113 (87.60%)\n",
      "Items in eval set: 16\n",
      "Items in test set: 98\n",
      " = 227\n",
      "Number of source documents: 46 total, 21 train, 3 eval 22 test\n",
      "Absolute (training): IR 25.00, N 38.00, R 30.00, HR 20.00\n",
      "Relative (training): IR 0.22, N 0.34, R 0.27, HR 0.18\n",
      "Absolute (eval): IR 7.00, N 7.00, R 0.00, HR 2.00\n",
      "Relative (eval): IR 0.44, N 0.44, R 0.00, HR 0.12\n",
      "Role samples for ISSUER in train: 113, eval: 16, test: 98\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.815393615246 | std = 0.0325199993154\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.621677771913\n",
      "Accuracy | role : 0.336734693878\n",
      "Accuracy | full : 0.285714285714\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.67      0.38      0.48        21\n",
      "    neutral       0.14      0.07      0.10        27\n",
      "   relevant       0.23      0.44      0.30        27\n",
      "     highly       0.30      0.26      0.28        23\n",
      "\n",
      "avg / total       0.32      0.29      0.28        98\n",
      "\n",
      "[[ 8  3  7  3]\n",
      " [ 2  2 17  6]\n",
      " [ 2  8 12  5]\n",
      " [ 0  1 16  6]]\n",
      "> NDCG Score | role | categ  | 0.89409\n",
      "> NDCG Score | role | proba* | 0.75693\n",
      "> NDCG Score | full | categ  | 0.83360\n",
      "> NDCG Score | full | proba* | 0.86607\n",
      "=== UNDERWRITER ======\n",
      "Items in training set: 14 (66.67%)\n",
      "Items in eval set: 7\n",
      "Items in test set: 40\n",
      " = 61\n",
      "Number of source documents: 16 total, 6 train, 1 eval 9 test\n",
      "Absolute (training): IR 0.00, N 0.00, R 7.00, HR 7.00\n",
      "Relative (training): IR 0.00, N 0.00, R 0.50, HR 0.50\n",
      "Absolute (eval): IR 0.00, N 0.00, R 7.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for UNDERWRITER in train: 14, eval: 7, test: 40\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.913116354475 | std = 0.0325111290428\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.721180267881\n",
      "Accuracy | role : 0.475\n",
      "Accuracy | full : 0.475\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.00      0.00      0.00         2\n",
      "   relevant       0.38      0.40      0.39        15\n",
      "     highly       0.62      0.65      0.63        20\n",
      "\n",
      "avg / total       0.45      0.47      0.46        40\n",
      "\n",
      "[[ 0  0  3  0]\n",
      " [ 0  0  0  2]\n",
      " [ 0  3  6  6]\n",
      " [ 0  0  7 13]]\n",
      "> NDCG Score | role | categ  | 0.95653\n",
      "> NDCG Score | role | proba* | 0.96143\n",
      "> NDCG Score | full | categ  | 0.95765\n",
      "> NDCG Score | full | proba* | 0.96169\n",
      "=== SERVICER ======\n",
      "Items in training set: 21 (100.00%)\n",
      "Items in eval set: 0\n",
      "Items in test set: 57\n",
      " = 78\n",
      "Number of source documents: 18 total, 8 train, 0 eval 10 test\n",
      "Absolute (training): IR 0.00, N 6.00, R 6.00, HR 9.00\n",
      "Relative (training): IR 0.00, N 0.29, R 0.29, HR 0.43\n",
      "Absolute (eval): IR 0.00, N 0.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR nan, N nan, R nan, HR nan\n",
      "Role samples for SERVICER in train: 21, eval: 0, test: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/Uni/HPI/workspace/FEII/code/feiii_data.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  rating_agg.sum()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.882757704292 | std = 0.02316517598\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.717524169319\n",
      "Accuracy | role : 0.157894736842\n",
      "Accuracy | full : 0.385964912281\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.17      0.11      0.13         9\n",
      "   relevant       0.56      0.47      0.51        38\n",
      "     highly       0.16      0.43      0.23         7\n",
      "\n",
      "avg / total       0.42      0.39      0.39        57\n",
      "\n",
      "[[ 0  1  2  0]\n",
      " [ 0  1  8  0]\n",
      " [ 0  4 18 16]\n",
      " [ 0  0  4  3]]\n",
      "> NDCG Score | role | categ  | 0.85835\n",
      "> NDCG Score | role | proba* | 0.90150\n",
      "> NDCG Score | full | categ  | 0.91187\n",
      "> NDCG Score | full | proba* | 0.90128\n",
      "=== TRUSTEE ======\n",
      "Items in training set: 390 (92.86%)\n",
      "Items in eval set: 30\n",
      "Items in test set: 304\n",
      " = 724\n",
      "Number of source documents: 40 total, 19 train, 2 eval 19 test\n",
      "Absolute (training): IR 20.00, N 137.00, R 78.00, HR 155.00\n",
      "Relative (training): IR 0.05, N 0.35, R 0.20, HR 0.40\n",
      "Absolute (eval): IR 1.00, N 27.00, R 1.00, HR 1.00\n",
      "Relative (eval): IR 0.03, N 0.90, R 0.03, HR 0.03\n",
      "Role samples for TRUSTEE in train: 390, eval: 30, test: 304\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.931529735539 | std = 0.0104156144629\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.814144820697\n",
      "Accuracy | role : 0.496710526316\n",
      "Accuracy | full : 0.5\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         9\n",
      "    neutral       0.15      0.28      0.20        47\n",
      "   relevant       0.42      0.16      0.23       124\n",
      "     highly       0.70      0.96      0.81       124\n",
      "\n",
      "avg / total       0.48      0.50      0.45       304\n",
      "\n",
      "[[  0   3   2   4]\n",
      " [  1  13  21  12]\n",
      " [  0  68  20  36]\n",
      " [  0   0   5 119]]\n",
      "> NDCG Score | role | categ  | 0.97374\n",
      "> NDCG Score | role | proba* | 0.98811\n",
      "> NDCG Score | full | categ  | 0.98211\n",
      "> NDCG Score | full | proba* | 0.99244\n",
      "=== COUNTERPART ======\n",
      "Items in training set: 55 (85.94%)\n",
      "Items in eval set: 9\n",
      "Items in test set: 108\n",
      " = 172\n",
      "Number of source documents: 31 total, 14 train, 3 eval 14 test\n",
      "Absolute (training): IR 6.00, N 11.00, R 23.00, HR 15.00\n",
      "Relative (training): IR 0.11, N 0.20, R 0.42, HR 0.27\n",
      "Absolute (eval): IR 0.00, N 1.00, R 7.00, HR 1.00\n",
      "Relative (eval): IR 0.00, N 0.11, R 0.78, HR 0.11\n",
      "Role samples for COUNTERPART in train: 55, eval: 9, test: 108\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.860756498205 | std = 0.0295078982385\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.677182649456\n",
      "Accuracy | role : 0.37037037037\n",
      "Accuracy | full : 0.407407407407\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00        18\n",
      "    neutral       0.14      0.12      0.13        17\n",
      "   relevant       0.35      0.55      0.43        38\n",
      "     highly       0.62      0.60      0.61        35\n",
      "\n",
      "avg / total       0.35      0.41      0.37       108\n",
      "\n",
      "[[ 0  4 14  0]\n",
      " [ 0  2 12  3]\n",
      " [ 0  7 21 10]\n",
      " [ 0  1 13 21]]\n",
      "> NDCG Score | role | categ  | 0.93008\n",
      "> NDCG Score | role | proba* | 0.91926\n",
      "> NDCG Score | full | categ  | 0.96318\n",
      "> NDCG Score | full | proba* | 0.95639\n",
      "=== SELLER ======\n",
      "Items in training set: 20 (100.00%)\n",
      "Items in eval set: 0\n",
      "Items in test set: 49\n",
      " = 69\n",
      "Number of source documents: 19 total, 10 train, 0 eval 9 test\n",
      "Absolute (training): IR 0.00, N 1.00, R 14.00, HR 5.00\n",
      "Relative (training): IR 0.00, N 0.05, R 0.70, HR 0.25\n",
      "Absolute (eval): IR 0.00, N 0.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR nan, N nan, R nan, HR nan\n",
      "Role samples for SELLER in train: 20, eval: 0, test: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/Uni/HPI/workspace/FEII/code/feiii_data.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  rating_agg.sum()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.867105950862 | std = 0.0375874481987\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.674252161736\n",
      "Accuracy | role : 0.34693877551\n",
      "Accuracy | full : 0.469387755102\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.00      0.00      0.00        10\n",
      "   relevant       0.39      0.65      0.49        17\n",
      "     highly       0.60      0.67      0.63        18\n",
      "\n",
      "avg / total       0.36      0.47      0.40        49\n",
      "\n",
      "[[ 0  0  3  1]\n",
      " [ 0  0  9  1]\n",
      " [ 0  0 11  6]\n",
      " [ 0  1  5 12]]\n",
      "> NDCG Score | role | categ  | 0.80424\n",
      "> NDCG Score | role | proba* | 0.95403\n",
      "> NDCG Score | full | categ  | 0.94446\n",
      "> NDCG Score | full | proba* | 0.93493\n",
      "=== AGENT ======\n",
      "Items in training set: 49 (80.33%)\n",
      "Items in eval set: 12\n",
      "Items in test set: 40\n",
      " = 101\n",
      "Number of source documents: 25 total, 13 train, 3 eval 9 test\n",
      "Absolute (training): IR 2.00, N 27.00, R 17.00, HR 3.00\n",
      "Relative (training): IR 0.04, N 0.55, R 0.35, HR 0.06\n",
      "Absolute (eval): IR 1.00, N 8.00, R 1.00, HR 2.00\n",
      "Relative (eval): IR 0.08, N 0.67, R 0.08, HR 0.17\n",
      "Role samples for AGENT in train: 49, eval: 12, test: 40\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.859654460008 | std = 0.0470132507564\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.650212572736\n",
      "Accuracy | role : 0.35\n",
      "Accuracy | full : 0.375\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.14      0.12      0.13         8\n",
      "   relevant       0.40      0.53      0.46        15\n",
      "     highly       0.46      0.46      0.46        13\n",
      "\n",
      "avg / total       0.33      0.38      0.35        40\n",
      "\n",
      "[[0 2 1 1]\n",
      " [0 1 4 3]\n",
      " [0 4 8 3]\n",
      " [0 0 7 6]]\n",
      "> NDCG Score | role | categ  | 0.90456\n",
      "> NDCG Score | role | proba* | 0.82898\n",
      "> NDCG Score | full | categ  | 0.86880\n",
      "> NDCG Score | full | proba* | 0.83880\n",
      "=== AFFILIATE ======\n",
      "Items in training set: 160 (86.02%)\n",
      "Items in eval set: 26\n",
      "Items in test set: 129\n",
      " = 315\n",
      "Number of source documents: 41 total, 19 train, 3 eval 19 test\n",
      "Absolute (training): IR 15.00, N 32.00, R 57.00, HR 56.00\n",
      "Relative (training): IR 0.09, N 0.20, R 0.36, HR 0.35\n",
      "Absolute (eval): IR 11.00, N 8.00, R 5.00, HR 2.00\n",
      "Relative (eval): IR 0.42, N 0.31, R 0.19, HR 0.08\n",
      "Role samples for AFFILIATE in train: 160, eval: 26, test: 129\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.894207838815 | std = 0.0245708850483\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.720304562167\n",
      "Accuracy | role : 0.53488372093\n",
      "Accuracy | full : 0.472868217054\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00        17\n",
      "    neutral       0.15      0.17      0.16        12\n",
      "   relevant       0.44      0.50      0.47        54\n",
      "     highly       0.62      0.70      0.65        46\n",
      "\n",
      "avg / total       0.42      0.47      0.44       129\n",
      "\n",
      "[[ 0  2 15  0]\n",
      " [ 0  2  8  2]\n",
      " [ 3  6 27 18]\n",
      " [ 0  3 11 32]]\n",
      "> NDCG Score | role | categ  | 0.93934\n",
      "> NDCG Score | role | proba* | 0.97101\n",
      "> NDCG Score | full | categ  | 0.93966\n",
      "> NDCG Score | full | proba* | 0.98252\n",
      "=== INSURER ======\n",
      "Items in training set: 18 (94.74%)\n",
      "Items in eval set: 1\n",
      "Items in test set: 47\n",
      " = 66\n",
      "Number of source documents: 15 total, 7 train, 1 eval 7 test\n",
      "Absolute (training): IR 1.00, N 1.00, R 7.00, HR 9.00\n",
      "Relative (training): IR 0.06, N 0.06, R 0.39, HR 0.50\n",
      "Absolute (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for INSURER in train: 18, eval: 1, test: 47\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.910240085707 | std = 0.0270978748697\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.755505248883\n",
      "Accuracy | role : 0.404255319149\n",
      "Accuracy | full : 0.510638297872\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         1\n",
      "    neutral       1.00      0.14      0.25         7\n",
      "   relevant       0.42      0.42      0.42        19\n",
      "     highly       0.56      0.75      0.64        20\n",
      "\n",
      "avg / total       0.56      0.51      0.48        47\n",
      "\n",
      "[[ 0  0  1  0]\n",
      " [ 0  1  5  1]\n",
      " [ 0  0  8 11]\n",
      " [ 0  0  5 15]]\n",
      "> NDCG Score | role | categ  | 0.89579\n",
      "> NDCG Score | role | proba* | 0.85498\n",
      "> NDCG Score | full | categ  | 0.91815\n",
      "> NDCG Score | full | proba* | 0.97594\n",
      "TOTAL NDCG | role | categ  | 0.97528\n",
      "TOTAL NDCG | role | proba* | 0.95567\n",
      "TOTAL NDCG | full | categ  | 0.96318\n",
      "TOTAL NDCG | full | proba* | 0.97095\n",
      "baseline_rand 10\n",
      "baseline_worst 10\n",
      "ndcg_role 10\n",
      "ndcg_full 10\n",
      "ndcg_role_proba 10\n",
      "ndcg_full_proba 10\n",
      "acc_role 10\n",
      "acc_full 10\n",
      "f1_role 10\n",
      "f1_full 10\n",
      "\n",
      "\n",
      "==========================================================================\n",
      "===                      CROSSEVAL ITERATION 5/5                     =====\n",
      "==========================================================================\n",
      "\n",
      "\n",
      "Items in training set: 849 (87.08%)\n",
      "Items in eval set: 126\n",
      "Items in test set: 900\n",
      " = 1875\n",
      "Number of source documents: 50 total, 22 train, 3 eval 25 test\n",
      "Absolute (training): IR 87.00, N 240.00, R 241.00, HR 281.00\n",
      "Relative (training): IR 0.10, N 0.28, R 0.28, HR 0.33\n",
      "Absolute (eval): IR 3.00, N 67.00, R 42.00, HR 14.00\n",
      "Relative (eval): IR 0.02, N 0.53, R 0.33, HR 0.11\n",
      "Role samples for TRUSTEE in train: 373, eval: 47, test: 304\n",
      "Role samples for COUNTERPART in train: 58, eval: 6, test: 108\n",
      "Role samples for SELLER in train: 18, eval: 2, test: 49\n",
      "Role samples for GUARANTOR in train: 19, eval: 15, test: 28\n",
      "Role samples for ISSUER in train: 116, eval: 13, test: 98\n",
      "Role samples for AGENT in train: 53, eval: 8, test: 40\n",
      "Role samples for UNDERWRITER in train: 21, eval: 0, test: 40\n",
      "Role samples for SERVICER in train: 18, eval: 3, test: 57\n",
      "Role samples for AFFILIATE in train: 154, eval: 32, test: 129\n",
      "Role samples for INSURER in train: 19, eval: 0, test: 47\n",
      "=== GUARANTOR ======\n",
      "Items in training set: 19 (55.88%)\n",
      "Items in eval set: 15\n",
      "Items in test set: 28\n",
      " = 62\n",
      "Number of source documents: 15 total, 4 train, 2 eval 9 test\n",
      "Absolute (training): IR 1.00, N 1.00, R 17.00, HR 0.00\n",
      "Relative (training): IR 0.05, N 0.05, R 0.89, HR 0.00\n",
      "Absolute (eval): IR 0.00, N 2.00, R 5.00, HR 8.00\n",
      "Relative (eval): IR 0.00, N 0.13, R 0.33, HR 0.53\n",
      "Role samples for GUARANTOR in train: 19, eval: 15, test: 28\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.901606874435 | std = 0.023864473144\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.810440241115\n",
      "Accuracy | role : 0.785714285714\n",
      "Accuracy | full : 0.5\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         0\n",
      "    neutral       0.00      0.00      0.00         3\n",
      "   relevant       0.72      0.59      0.65        22\n",
      "     highly       0.33      0.33      0.33         3\n",
      "\n",
      "avg / total       0.60      0.50      0.55        28\n",
      "\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  3  0]\n",
      " [ 0  7 13  2]\n",
      " [ 0  0  2  1]]\n",
      "> NDCG Score | role | categ  | 0.84067\n",
      "> NDCG Score | role | proba* | 0.91304\n",
      "> NDCG Score | full | categ  | 0.93433\n",
      "> NDCG Score | full | proba* | 0.93720\n",
      "=== ISSUER ======\n",
      "Items in training set: 116 (89.92%)\n",
      "Items in eval set: 13\n",
      "Items in test set: 98\n",
      " = 227\n",
      "Number of source documents: 46 total, 21 train, 3 eval 22 test\n",
      "Absolute (training): IR 29.00, N 40.00, R 28.00, HR 19.00\n",
      "Relative (training): IR 0.25, N 0.34, R 0.24, HR 0.16\n",
      "Absolute (eval): IR 3.00, N 5.00, R 2.00, HR 3.00\n",
      "Relative (eval): IR 0.23, N 0.38, R 0.15, HR 0.23\n",
      "Role samples for ISSUER in train: 116, eval: 13, test: 98\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.81810181662 | std = 0.0400383470383\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.621677771913\n",
      "Accuracy | role : 0.357142857143\n",
      "Accuracy | full : 0.285714285714\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.71      0.48      0.57        21\n",
      "    neutral       0.20      0.07      0.11        27\n",
      "   relevant       0.21      0.37      0.27        27\n",
      "     highly       0.22      0.26      0.24        23\n",
      "\n",
      "avg / total       0.32      0.29      0.28        98\n",
      "\n",
      "[[10  2  9  0]\n",
      " [ 2  2 13 10]\n",
      " [ 2  4 10 11]\n",
      " [ 0  2 15  6]]\n",
      "> NDCG Score | role | categ  | 0.81841\n",
      "> NDCG Score | role | proba* | 0.80151\n",
      "> NDCG Score | full | categ  | 0.84495\n",
      "> NDCG Score | full | proba* | 0.86778\n",
      "=== UNDERWRITER ======\n",
      "Items in training set: 21 (100.00%)\n",
      "Items in eval set: 0\n",
      "Items in test set: 40\n",
      " = 61\n",
      "Number of source documents: 16 total, 7 train, 0 eval 9 test\n",
      "Absolute (training): IR 0.00, N 0.00, R 14.00, HR 7.00\n",
      "Relative (training): IR 0.00, N 0.00, R 0.67, HR 0.33\n",
      "Absolute (eval): IR 0.00, N 0.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR nan, N nan, R nan, HR nan\n",
      "Role samples for UNDERWRITER in train: 21, eval: 0, test: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/Uni/HPI/workspace/FEII/code/feiii_data.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  rating_agg.sum()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.916107269535 | std = 0.0308838179733\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.721180267881\n",
      "Accuracy | role : 0.375\n",
      "Accuracy | full : 0.625\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.00      0.00      0.00         2\n",
      "   relevant       0.58      0.73      0.65        15\n",
      "     highly       0.70      0.70      0.70        20\n",
      "\n",
      "avg / total       0.57      0.62      0.59        40\n",
      "\n",
      "[[ 0  0  3  0]\n",
      " [ 0  0  0  2]\n",
      " [ 0  0 11  4]\n",
      " [ 0  1  5 14]]\n",
      "> NDCG Score | role | categ  | 0.86908\n",
      "> NDCG Score | role | proba* | 0.88818\n",
      "> NDCG Score | full | categ  | 0.96397\n",
      "> NDCG Score | full | proba* | 0.96922\n",
      "=== SERVICER ======\n",
      "Items in training set: 18 (85.71%)\n",
      "Items in eval set: 3\n",
      "Items in test set: 57\n",
      " = 78\n",
      "Number of source documents: 18 total, 7 train, 1 eval 10 test\n",
      "Absolute (training): IR 0.00, N 6.00, R 3.00, HR 9.00\n",
      "Relative (training): IR 0.00, N 0.33, R 0.17, HR 0.50\n",
      "Absolute (eval): IR 0.00, N 0.00, R 3.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for SERVICER in train: 18, eval: 3, test: 57\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.877986695032 | std = 0.0293329630637\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.717524169319\n",
      "Accuracy | role : 0.122807017544\n",
      "Accuracy | full : 0.228070175439\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.14      0.11      0.12         9\n",
      "   relevant       0.46      0.29      0.35        38\n",
      "     highly       0.04      0.14      0.06         7\n",
      "\n",
      "avg / total       0.33      0.23      0.26        57\n",
      "\n",
      "[[ 0  0  3  0]\n",
      " [ 0  1  7  1]\n",
      " [ 0  3 11 24]\n",
      " [ 0  3  3  1]]\n",
      "> NDCG Score | role | categ  | 0.85663\n",
      "> NDCG Score | role | proba* | 0.90505\n",
      "> NDCG Score | full | categ  | 0.87517\n",
      "> NDCG Score | full | proba* | 0.89168\n",
      "=== TRUSTEE ======\n",
      "Items in training set: 373 (88.81%)\n",
      "Items in eval set: 47\n",
      "Items in test set: 304\n",
      " = 724\n",
      "Number of source documents: 40 total, 18 train, 3 eval 19 test\n",
      "Absolute (training): IR 21.00, N 124.00, R 72.00, HR 156.00\n",
      "Relative (training): IR 0.06, N 0.33, R 0.19, HR 0.42\n",
      "Absolute (eval): IR 0.00, N 40.00, R 7.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.85, R 0.15, HR 0.00\n",
      "Role samples for TRUSTEE in train: 373, eval: 47, test: 304\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.930382398812 | std = 0.0110172153556\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.814144820697\n",
      "Accuracy | role : 0.5\n",
      "Accuracy | full : 0.503289473684\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         9\n",
      "    neutral       0.20      0.40      0.27        47\n",
      "   relevant       0.43      0.12      0.19       124\n",
      "     highly       0.69      0.96      0.80       124\n",
      "\n",
      "avg / total       0.49      0.50      0.45       304\n",
      "\n",
      "[[  0   2   3   4]\n",
      " [  1  19  14  13]\n",
      " [  0  73  15  36]\n",
      " [  0   2   3 119]]\n",
      "> NDCG Score | role | categ  | 0.97358\n",
      "> NDCG Score | role | proba* | 0.98646\n",
      "> NDCG Score | full | categ  | 0.98180\n",
      "> NDCG Score | full | proba* | 0.99168\n",
      "=== COUNTERPART ======\n",
      "Items in training set: 58 (90.62%)\n",
      "Items in eval set: 6\n",
      "Items in test set: 108\n",
      " = 172\n",
      "Number of source documents: 31 total, 14 train, 3 eval 14 test\n",
      "Absolute (training): IR 6.00, N 9.00, R 28.00, HR 15.00\n",
      "Relative (training): IR 0.10, N 0.16, R 0.48, HR 0.26\n",
      "Absolute (eval): IR 0.00, N 3.00, R 2.00, HR 1.00\n",
      "Relative (eval): IR 0.00, N 0.50, R 0.33, HR 0.17\n",
      "Role samples for COUNTERPART in train: 58, eval: 6, test: 108\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.86074657219 | std = 0.0350449813983\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.677182649456\n",
      "Accuracy | role : 0.342592592593\n",
      "Accuracy | full : 0.37037037037\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00        18\n",
      "    neutral       0.17      0.06      0.09        17\n",
      "   relevant       0.33      0.47      0.39        38\n",
      "     highly       0.45      0.60      0.51        35\n",
      "\n",
      "avg / total       0.29      0.37      0.32       108\n",
      "\n",
      "[[ 0  3 11  4]\n",
      " [ 0  1 13  3]\n",
      " [ 0  1 18 19]\n",
      " [ 0  1 13 21]]\n",
      "> NDCG Score | role | categ  | 0.93940\n",
      "> NDCG Score | role | proba* | 0.87305\n",
      "> NDCG Score | full | categ  | 0.89533\n",
      "> NDCG Score | full | proba* | 0.94950\n",
      "=== SELLER ======\n",
      "Items in training set: 18 (90.00%)\n",
      "Items in eval set: 2\n",
      "Items in test set: 49\n",
      " = 69\n",
      "Number of source documents: 19 total, 9 train, 1 eval 9 test\n",
      "Absolute (training): IR 0.00, N 1.00, R 12.00, HR 5.00\n",
      "Relative (training): IR 0.00, N 0.06, R 0.67, HR 0.28\n",
      "Absolute (eval): IR 0.00, N 0.00, R 2.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for SELLER in train: 18, eval: 2, test: 49\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.873967470984 | std = 0.0366774022704\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.674252161736\n",
      "Accuracy | role : 0.34693877551\n",
      "Accuracy | full : 0.510204081633\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.25      0.10      0.14        10\n",
      "   relevant       0.45      0.76      0.57        17\n",
      "     highly       0.69      0.61      0.65        18\n",
      "\n",
      "avg / total       0.46      0.51      0.46        49\n",
      "\n",
      "[[ 0  0  3  1]\n",
      " [ 0  1  8  1]\n",
      " [ 0  1 13  3]\n",
      " [ 0  2  5 11]]\n",
      "> NDCG Score | role | categ  | 0.80424\n",
      "> NDCG Score | role | proba* | 0.95179\n",
      "> NDCG Score | full | categ  | 0.95020\n",
      "> NDCG Score | full | proba* | 0.95388\n",
      "=== AGENT ======\n",
      "Items in training set: 53 (86.89%)\n",
      "Items in eval set: 8\n",
      "Items in test set: 40\n",
      " = 101\n",
      "Number of source documents: 25 total, 13 train, 3 eval 9 test\n",
      "Absolute (training): IR 3.00, N 28.00, R 17.00, HR 5.00\n",
      "Relative (training): IR 0.06, N 0.53, R 0.32, HR 0.09\n",
      "Absolute (eval): IR 0.00, N 7.00, R 1.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.88, R 0.12, HR 0.00\n",
      "Role samples for AGENT in train: 53, eval: 8, test: 40\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.856246878107 | std = 0.0424902383107\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.650212572736\n",
      "Accuracy | role : 0.175\n",
      "Accuracy | full : 0.45\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.33      0.25      0.29         8\n",
      "   relevant       0.44      0.53      0.48        15\n",
      "     highly       0.50      0.62      0.55        13\n",
      "\n",
      "avg / total       0.40      0.45      0.42        40\n",
      "\n",
      "[[0 3 0 1]\n",
      " [0 2 5 1]\n",
      " [0 1 8 6]\n",
      " [0 0 5 8]]\n",
      "> NDCG Score | role | categ  | 0.80376\n",
      "> NDCG Score | role | proba* | 0.76183\n",
      "> NDCG Score | full | categ  | 0.91745\n",
      "> NDCG Score | full | proba* | 0.94028\n",
      "=== AFFILIATE ======\n",
      "Items in training set: 154 (82.80%)\n",
      "Items in eval set: 32\n",
      "Items in test set: 129\n",
      " = 315\n",
      "Number of source documents: 41 total, 19 train, 3 eval 19 test\n",
      "Absolute (training): IR 26.00, N 30.00, R 42.00, HR 56.00\n",
      "Relative (training): IR 0.17, N 0.19, R 0.27, HR 0.36\n",
      "Absolute (eval): IR 0.00, N 10.00, R 20.00, HR 2.00\n",
      "Relative (eval): IR 0.00, N 0.31, R 0.62, HR 0.06\n",
      "Role samples for AFFILIATE in train: 154, eval: 32, test: 129\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.887242476356 | std = 0.0245925933893\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.720304562167\n",
      "Accuracy | role : 0.457364341085\n",
      "Accuracy | full : 0.550387596899\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.25      0.06      0.10        17\n",
      "    neutral       0.29      0.17      0.21        12\n",
      "   relevant       0.52      0.57      0.54        54\n",
      "     highly       0.64      0.80      0.71        46\n",
      "\n",
      "avg / total       0.50      0.55      0.51       129\n",
      "\n",
      "[[ 1  1 14  1]\n",
      " [ 0  2  6  4]\n",
      " [ 3  4 31 16]\n",
      " [ 0  0  9 37]]\n",
      "> NDCG Score | role | categ  | 0.90354\n",
      "> NDCG Score | role | proba* | 0.98552\n",
      "> NDCG Score | full | categ  | 0.92719\n",
      "> NDCG Score | full | proba* | 0.98248\n",
      "=== INSURER ======\n",
      "Items in training set: 19 (100.00%)\n",
      "Items in eval set: 0\n",
      "Items in test set: 47\n",
      " = 66\n",
      "Number of source documents: 15 total, 8 train, 0 eval 7 test\n",
      "Absolute (training): IR 1.00, N 1.00, R 8.00, HR 9.00\n",
      "Relative (training): IR 0.05, N 0.05, R 0.42, HR 0.47\n",
      "Absolute (eval): IR 0.00, N 0.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR nan, N nan, R nan, HR nan\n",
      "Role samples for INSURER in train: 19, eval: 0, test: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/Uni/HPI/workspace/FEII/code/feiii_data.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  rating_agg.sum()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.902033514386 | std = 0.0306103766175\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.755505248883\n",
      "Accuracy | role : 0.617021276596\n",
      "Accuracy | full : 0.446808510638\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         1\n",
      "    neutral       0.25      0.14      0.18         7\n",
      "   relevant       0.27      0.16      0.20        19\n",
      "     highly       0.53      0.85      0.65        20\n",
      "\n",
      "avg / total       0.37      0.45      0.39        47\n",
      "\n",
      "[[ 0  0  1  0]\n",
      " [ 0  1  4  2]\n",
      " [ 0  3  3 13]\n",
      " [ 0  0  3 17]]\n",
      "> NDCG Score | role | categ  | 0.92645\n",
      "> NDCG Score | role | proba* | 0.94048\n",
      "> NDCG Score | full | categ  | 0.90220\n",
      "> NDCG Score | full | proba* | 0.98437\n",
      "TOTAL NDCG | role | categ  | 0.96265\n",
      "TOTAL NDCG | role | proba* | 0.96103\n",
      "TOTAL NDCG | full | categ  | 0.96790\n",
      "TOTAL NDCG | full | proba* | 0.97142\n",
      "baseline_rand 10\n",
      "baseline_worst 10\n",
      "ndcg_role 10\n",
      "ndcg_full 10\n",
      "ndcg_role_proba 10\n",
      "ndcg_full_proba 10\n",
      "acc_role 10\n",
      "acc_full 10\n",
      "f1_role 10\n",
      "f1_full 10\n"
     ]
    }
   ],
   "source": [
    "res, macro_res, conf_matrix_role, conf_matrix_full = kfold(5, data, pipeline, score_func, predict_on='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.458573</td>\n",
       "      <td>0.404886</td>\n",
       "      <td>0.882620</td>\n",
       "      <td>0.716242</td>\n",
       "      <td>0.437448</td>\n",
       "      <td>0.320963</td>\n",
       "      <td>0.927835</td>\n",
       "      <td>0.934710</td>\n",
       "      <td>0.885366</td>\n",
       "      <td>0.896086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.099130</td>\n",
       "      <td>0.157694</td>\n",
       "      <td>0.032637</td>\n",
       "      <td>0.061140</td>\n",
       "      <td>0.099786</td>\n",
       "      <td>0.174052</td>\n",
       "      <td>0.040567</td>\n",
       "      <td>0.046428</td>\n",
       "      <td>0.056323</td>\n",
       "      <td>0.068369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.228070</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.807877</td>\n",
       "      <td>0.621678</td>\n",
       "      <td>0.240732</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.833604</td>\n",
       "      <td>0.838799</td>\n",
       "      <td>0.803756</td>\n",
       "      <td>0.756926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.400463</td>\n",
       "      <td>0.343679</td>\n",
       "      <td>0.860860</td>\n",
       "      <td>0.674252</td>\n",
       "      <td>0.372816</td>\n",
       "      <td>0.182598</td>\n",
       "      <td>0.904603</td>\n",
       "      <td>0.901757</td>\n",
       "      <td>0.840668</td>\n",
       "      <td>0.852156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.480921</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.885455</td>\n",
       "      <td>0.718914</td>\n",
       "      <td>0.455710</td>\n",
       "      <td>0.287273</td>\n",
       "      <td>0.933542</td>\n",
       "      <td>0.947459</td>\n",
       "      <td>0.890098</td>\n",
       "      <td>0.907261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.510530</td>\n",
       "      <td>0.480531</td>\n",
       "      <td>0.906200</td>\n",
       "      <td>0.755505</td>\n",
       "      <td>0.506500</td>\n",
       "      <td>0.425648</td>\n",
       "      <td>0.957972</td>\n",
       "      <td>0.975861</td>\n",
       "      <td>0.936421</td>\n",
       "      <td>0.951481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.931530</td>\n",
       "      <td>0.814145</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.691429</td>\n",
       "      <td>0.991694</td>\n",
       "      <td>0.992436</td>\n",
       "      <td>0.978115</td>\n",
       "      <td>0.988567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc_full   acc_role  baseline_rand  baseline_worst    f1_full  \\\n",
       "count  50.000000  50.000000      50.000000       50.000000  50.000000   \n",
       "mean    0.458573   0.404886       0.882620        0.716242   0.437448   \n",
       "std     0.099130   0.157694       0.032637        0.061140   0.099786   \n",
       "min     0.228070   0.122807       0.807877        0.621678   0.240732   \n",
       "25%     0.400463   0.343679       0.860860        0.674252   0.372816   \n",
       "50%     0.480921   0.375000       0.885455        0.718914   0.455710   \n",
       "75%     0.510530   0.480531       0.906200        0.755505   0.506500   \n",
       "max     0.680851   0.785714       0.931530        0.814145   0.642857   \n",
       "\n",
       "         f1_role  ndcg_full  ndcg_full_proba  ndcg_role  ndcg_role_proba  \n",
       "count  50.000000  50.000000        50.000000  50.000000        50.000000  \n",
       "mean    0.320963   0.927835         0.934710   0.885366         0.896086  \n",
       "std     0.174052   0.040567         0.046428   0.056323         0.068369  \n",
       "min     0.026864   0.833604         0.838799   0.803756         0.756926  \n",
       "25%     0.182598   0.904603         0.901757   0.840668         0.852156  \n",
       "50%     0.287273   0.933542         0.947459   0.890098         0.907261  \n",
       "75%     0.425648   0.957972         0.975861   0.936421         0.951481  \n",
       "max     0.691429   0.991694         0.992436   0.978115         0.988567  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(res).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>GUARANTOR</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.583069</td>\n",
       "      <td>0.888915</td>\n",
       "      <td>0.757137</td>\n",
       "      <td>0.449566</td>\n",
       "      <td>0.513951</td>\n",
       "      <td>0.932977</td>\n",
       "      <td>0.933457</td>\n",
       "      <td>0.873171</td>\n",
       "      <td>0.866518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.074788</td>\n",
       "      <td>0.252112</td>\n",
       "      <td>0.023027</td>\n",
       "      <td>0.072988</td>\n",
       "      <td>0.127639</td>\n",
       "      <td>0.247514</td>\n",
       "      <td>0.016366</td>\n",
       "      <td>0.020814</td>\n",
       "      <td>0.062850</td>\n",
       "      <td>0.061798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.860437</td>\n",
       "      <td>0.677183</td>\n",
       "      <td>0.270559</td>\n",
       "      <td>0.178709</td>\n",
       "      <td>0.915284</td>\n",
       "      <td>0.903181</td>\n",
       "      <td>0.804398</td>\n",
       "      <td>0.764600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.398148</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.867251</td>\n",
       "      <td>0.677183</td>\n",
       "      <td>0.364652</td>\n",
       "      <td>0.317714</td>\n",
       "      <td>0.919376</td>\n",
       "      <td>0.921867</td>\n",
       "      <td>0.840668</td>\n",
       "      <td>0.851214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.904693</td>\n",
       "      <td>0.810440</td>\n",
       "      <td>0.498430</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.930722</td>\n",
       "      <td>0.939955</td>\n",
       "      <td>0.840668</td>\n",
       "      <td>0.902075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.905325</td>\n",
       "      <td>0.810440</td>\n",
       "      <td>0.542929</td>\n",
       "      <td>0.691429</td>\n",
       "      <td>0.948101</td>\n",
       "      <td>0.948476</td>\n",
       "      <td>0.937921</td>\n",
       "      <td>0.902075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.906869</td>\n",
       "      <td>0.810440</td>\n",
       "      <td>0.571259</td>\n",
       "      <td>0.691429</td>\n",
       "      <td>0.951402</td>\n",
       "      <td>0.953807</td>\n",
       "      <td>0.942201</td>\n",
       "      <td>0.912627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><h2>ISSUER</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.393878</td>\n",
       "      <td>0.338776</td>\n",
       "      <td>0.834192</td>\n",
       "      <td>0.642708</td>\n",
       "      <td>0.368801</td>\n",
       "      <td>0.256569</td>\n",
       "      <td>0.897518</td>\n",
       "      <td>0.892053</td>\n",
       "      <td>0.829137</td>\n",
       "      <td>0.874016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.107167</td>\n",
       "      <td>0.018254</td>\n",
       "      <td>0.030302</td>\n",
       "      <td>0.028796</td>\n",
       "      <td>0.080802</td>\n",
       "      <td>0.074041</td>\n",
       "      <td>0.055830</td>\n",
       "      <td>0.045148</td>\n",
       "      <td>0.024649</td>\n",
       "      <td>0.027340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.295918</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.807877</td>\n",
       "      <td>0.621678</td>\n",
       "      <td>0.295756</td>\n",
       "      <td>0.178726</td>\n",
       "      <td>0.851504</td>\n",
       "      <td>0.851856</td>\n",
       "      <td>0.804237</td>\n",
       "      <td>0.833358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.316327</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.811261</td>\n",
       "      <td>0.621678</td>\n",
       "      <td>0.314412</td>\n",
       "      <td>0.178726</td>\n",
       "      <td>0.853683</td>\n",
       "      <td>0.857326</td>\n",
       "      <td>0.804237</td>\n",
       "      <td>0.871015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.336735</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.817504</td>\n",
       "      <td>0.621678</td>\n",
       "      <td>0.320325</td>\n",
       "      <td>0.276018</td>\n",
       "      <td>0.865819</td>\n",
       "      <td>0.875407</td>\n",
       "      <td>0.830913</td>\n",
       "      <td>0.873814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.866089</td>\n",
       "      <td>0.674252</td>\n",
       "      <td>0.456201</td>\n",
       "      <td>0.316005</td>\n",
       "      <td>0.953944</td>\n",
       "      <td>0.917076</td>\n",
       "      <td>0.849032</td>\n",
       "      <td>0.882422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>0.674252</td>\n",
       "      <td>0.457309</td>\n",
       "      <td>0.333371</td>\n",
       "      <td>0.962637</td>\n",
       "      <td>0.958599</td>\n",
       "      <td>0.857266</td>\n",
       "      <td>0.909472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><h2>UNDERWRITER</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.505000</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.886326</td>\n",
       "      <td>0.692793</td>\n",
       "      <td>0.478200</td>\n",
       "      <td>0.235669</td>\n",
       "      <td>0.940803</td>\n",
       "      <td>0.939043</td>\n",
       "      <td>0.863808</td>\n",
       "      <td>0.848404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.124248</td>\n",
       "      <td>0.137614</td>\n",
       "      <td>0.029954</td>\n",
       "      <td>0.038871</td>\n",
       "      <td>0.141670</td>\n",
       "      <td>0.131871</td>\n",
       "      <td>0.043365</td>\n",
       "      <td>0.051621</td>\n",
       "      <td>0.045118</td>\n",
       "      <td>0.070396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.852340</td>\n",
       "      <td>0.650213</td>\n",
       "      <td>0.278373</td>\n",
       "      <td>0.147826</td>\n",
       "      <td>0.886260</td>\n",
       "      <td>0.857001</td>\n",
       "      <td>0.811185</td>\n",
       "      <td>0.762135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.854805</td>\n",
       "      <td>0.650213</td>\n",
       "      <td>0.398088</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.911807</td>\n",
       "      <td>0.933275</td>\n",
       "      <td>0.837770</td>\n",
       "      <td>0.811598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.906009</td>\n",
       "      <td>0.721180</td>\n",
       "      <td>0.526464</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.940046</td>\n",
       "      <td>0.938600</td>\n",
       "      <td>0.869081</td>\n",
       "      <td>0.845257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.908007</td>\n",
       "      <td>0.721180</td>\n",
       "      <td>0.545219</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.974210</td>\n",
       "      <td>0.982893</td>\n",
       "      <td>0.869081</td>\n",
       "      <td>0.872469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.910469</td>\n",
       "      <td>0.721180</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.991694</td>\n",
       "      <td>0.983446</td>\n",
       "      <td>0.931923</td>\n",
       "      <td>0.950562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><h2>SERVICER</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.444798</td>\n",
       "      <td>0.336026</td>\n",
       "      <td>0.885952</td>\n",
       "      <td>0.718636</td>\n",
       "      <td>0.435237</td>\n",
       "      <td>0.286383</td>\n",
       "      <td>0.922130</td>\n",
       "      <td>0.901465</td>\n",
       "      <td>0.905524</td>\n",
       "      <td>0.923599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.123198</td>\n",
       "      <td>0.185316</td>\n",
       "      <td>0.006011</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.109749</td>\n",
       "      <td>0.223507</td>\n",
       "      <td>0.026386</td>\n",
       "      <td>0.053396</td>\n",
       "      <td>0.055626</td>\n",
       "      <td>0.057351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.228070</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.878927</td>\n",
       "      <td>0.717524</td>\n",
       "      <td>0.240732</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.895692</td>\n",
       "      <td>0.848930</td>\n",
       "      <td>0.856627</td>\n",
       "      <td>0.831025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.882676</td>\n",
       "      <td>0.717524</td>\n",
       "      <td>0.471120</td>\n",
       "      <td>0.077913</td>\n",
       "      <td>0.898584</td>\n",
       "      <td>0.858713</td>\n",
       "      <td>0.856627</td>\n",
       "      <td>0.918088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.350877</td>\n",
       "      <td>0.883668</td>\n",
       "      <td>0.717524</td>\n",
       "      <td>0.475761</td>\n",
       "      <td>0.347827</td>\n",
       "      <td>0.923540</td>\n",
       "      <td>0.883264</td>\n",
       "      <td>0.886104</td>\n",
       "      <td>0.926324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.496124</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.892178</td>\n",
       "      <td>0.720305</td>\n",
       "      <td>0.479382</td>\n",
       "      <td>0.453620</td>\n",
       "      <td>0.933194</td>\n",
       "      <td>0.953429</td>\n",
       "      <td>0.950930</td>\n",
       "      <td>0.970300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.542636</td>\n",
       "      <td>0.892312</td>\n",
       "      <td>0.720305</td>\n",
       "      <td>0.509189</td>\n",
       "      <td>0.525691</td>\n",
       "      <td>0.959643</td>\n",
       "      <td>0.962991</td>\n",
       "      <td>0.977332</td>\n",
       "      <td>0.972260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><h2>TRUSTEE</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.531761</td>\n",
       "      <td>0.435694</td>\n",
       "      <td>0.922001</td>\n",
       "      <td>0.790689</td>\n",
       "      <td>0.515563</td>\n",
       "      <td>0.345039</td>\n",
       "      <td>0.968958</td>\n",
       "      <td>0.972104</td>\n",
       "      <td>0.939481</td>\n",
       "      <td>0.944232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.085394</td>\n",
       "      <td>0.039366</td>\n",
       "      <td>0.012559</td>\n",
       "      <td>0.032118</td>\n",
       "      <td>0.075589</td>\n",
       "      <td>0.095892</td>\n",
       "      <td>0.021797</td>\n",
       "      <td>0.026979</td>\n",
       "      <td>0.040592</td>\n",
       "      <td>0.046479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.906264</td>\n",
       "      <td>0.755505</td>\n",
       "      <td>0.442245</td>\n",
       "      <td>0.232753</td>\n",
       "      <td>0.933890</td>\n",
       "      <td>0.925168</td>\n",
       "      <td>0.896197</td>\n",
       "      <td>0.883234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.486842</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.910414</td>\n",
       "      <td>0.755505</td>\n",
       "      <td>0.472523</td>\n",
       "      <td>0.254049</td>\n",
       "      <td>0.964152</td>\n",
       "      <td>0.975627</td>\n",
       "      <td>0.896197</td>\n",
       "      <td>0.911696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.506579</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.930753</td>\n",
       "      <td>0.814145</td>\n",
       "      <td>0.510991</td>\n",
       "      <td>0.392077</td>\n",
       "      <td>0.973431</td>\n",
       "      <td>0.979899</td>\n",
       "      <td>0.953381</td>\n",
       "      <td>0.950314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.516447</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.931202</td>\n",
       "      <td>0.814145</td>\n",
       "      <td>0.511631</td>\n",
       "      <td>0.395992</td>\n",
       "      <td>0.985966</td>\n",
       "      <td>0.989272</td>\n",
       "      <td>0.973513</td>\n",
       "      <td>0.987349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.480263</td>\n",
       "      <td>0.931370</td>\n",
       "      <td>0.814145</td>\n",
       "      <td>0.640426</td>\n",
       "      <td>0.450324</td>\n",
       "      <td>0.987353</td>\n",
       "      <td>0.990557</td>\n",
       "      <td>0.978115</td>\n",
       "      <td>0.988567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><h2>COUNTERPART</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.429894</td>\n",
       "      <td>0.494444</td>\n",
       "      <td>0.880211</td>\n",
       "      <td>0.730486</td>\n",
       "      <td>0.415232</td>\n",
       "      <td>0.414508</td>\n",
       "      <td>0.926225</td>\n",
       "      <td>0.931984</td>\n",
       "      <td>0.886088</td>\n",
       "      <td>0.843314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.057498</td>\n",
       "      <td>0.240694</td>\n",
       "      <td>0.023785</td>\n",
       "      <td>0.072988</td>\n",
       "      <td>0.108353</td>\n",
       "      <td>0.258042</td>\n",
       "      <td>0.013593</td>\n",
       "      <td>0.019213</td>\n",
       "      <td>0.061112</td>\n",
       "      <td>0.066697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.860437</td>\n",
       "      <td>0.677183</td>\n",
       "      <td>0.270559</td>\n",
       "      <td>0.178709</td>\n",
       "      <td>0.915284</td>\n",
       "      <td>0.903181</td>\n",
       "      <td>0.804398</td>\n",
       "      <td>0.764600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.398148</td>\n",
       "      <td>0.342593</td>\n",
       "      <td>0.861172</td>\n",
       "      <td>0.677183</td>\n",
       "      <td>0.364652</td>\n",
       "      <td>0.194213</td>\n",
       "      <td>0.917641</td>\n",
       "      <td>0.921867</td>\n",
       "      <td>0.840668</td>\n",
       "      <td>0.786054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.435185</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.867251</td>\n",
       "      <td>0.677183</td>\n",
       "      <td>0.399587</td>\n",
       "      <td>0.317714</td>\n",
       "      <td>0.919376</td>\n",
       "      <td>0.939955</td>\n",
       "      <td>0.905253</td>\n",
       "      <td>0.851214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.905325</td>\n",
       "      <td>0.810440</td>\n",
       "      <td>0.498430</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.930722</td>\n",
       "      <td>0.946443</td>\n",
       "      <td>0.937921</td>\n",
       "      <td>0.902075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.906869</td>\n",
       "      <td>0.810440</td>\n",
       "      <td>0.542929</td>\n",
       "      <td>0.691429</td>\n",
       "      <td>0.948101</td>\n",
       "      <td>0.948476</td>\n",
       "      <td>0.942201</td>\n",
       "      <td>0.912627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><h2>SELLER</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.432653</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.844468</td>\n",
       "      <td>0.653222</td>\n",
       "      <td>0.396962</td>\n",
       "      <td>0.237111</td>\n",
       "      <td>0.918397</td>\n",
       "      <td>0.909187</td>\n",
       "      <td>0.820178</td>\n",
       "      <td>0.895146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.107167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031897</td>\n",
       "      <td>0.028796</td>\n",
       "      <td>0.081641</td>\n",
       "      <td>0.080182</td>\n",
       "      <td>0.054851</td>\n",
       "      <td>0.052851</td>\n",
       "      <td>0.023734</td>\n",
       "      <td>0.028845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.295918</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.807877</td>\n",
       "      <td>0.621678</td>\n",
       "      <td>0.295756</td>\n",
       "      <td>0.178726</td>\n",
       "      <td>0.851504</td>\n",
       "      <td>0.851856</td>\n",
       "      <td>0.804237</td>\n",
       "      <td>0.871015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.336735</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.811261</td>\n",
       "      <td>0.621678</td>\n",
       "      <td>0.320325</td>\n",
       "      <td>0.178726</td>\n",
       "      <td>0.865819</td>\n",
       "      <td>0.857326</td>\n",
       "      <td>0.804237</td>\n",
       "      <td>0.873814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.866089</td>\n",
       "      <td>0.674252</td>\n",
       "      <td>0.455220</td>\n",
       "      <td>0.178726</td>\n",
       "      <td>0.953944</td>\n",
       "      <td>0.917076</td>\n",
       "      <td>0.804237</td>\n",
       "      <td>0.882422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>0.674252</td>\n",
       "      <td>0.456201</td>\n",
       "      <td>0.316005</td>\n",
       "      <td>0.958081</td>\n",
       "      <td>0.958599</td>\n",
       "      <td>0.830913</td>\n",
       "      <td>0.909472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.868881</td>\n",
       "      <td>0.674252</td>\n",
       "      <td>0.457309</td>\n",
       "      <td>0.333371</td>\n",
       "      <td>0.962637</td>\n",
       "      <td>0.961080</td>\n",
       "      <td>0.857266</td>\n",
       "      <td>0.939006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><h2>AGENT</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.485000</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.875193</td>\n",
       "      <td>0.678600</td>\n",
       "      <td>0.448395</td>\n",
       "      <td>0.244531</td>\n",
       "      <td>0.918658</td>\n",
       "      <td>0.916473</td>\n",
       "      <td>0.856334</td>\n",
       "      <td>0.836004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.128209</td>\n",
       "      <td>0.135785</td>\n",
       "      <td>0.030225</td>\n",
       "      <td>0.038871</td>\n",
       "      <td>0.139708</td>\n",
       "      <td>0.130740</td>\n",
       "      <td>0.049831</td>\n",
       "      <td>0.052321</td>\n",
       "      <td>0.047078</td>\n",
       "      <td>0.076302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.852340</td>\n",
       "      <td>0.650213</td>\n",
       "      <td>0.278373</td>\n",
       "      <td>0.147826</td>\n",
       "      <td>0.863483</td>\n",
       "      <td>0.857001</td>\n",
       "      <td>0.811185</td>\n",
       "      <td>0.762135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.852341</td>\n",
       "      <td>0.650213</td>\n",
       "      <td>0.396190</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.886260</td>\n",
       "      <td>0.870042</td>\n",
       "      <td>0.831712</td>\n",
       "      <td>0.783254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.854805</td>\n",
       "      <td>0.650213</td>\n",
       "      <td>0.398088</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.911807</td>\n",
       "      <td>0.933275</td>\n",
       "      <td>0.837770</td>\n",
       "      <td>0.811598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.906009</td>\n",
       "      <td>0.721180</td>\n",
       "      <td>0.526464</td>\n",
       "      <td>0.248857</td>\n",
       "      <td>0.940046</td>\n",
       "      <td>0.938600</td>\n",
       "      <td>0.869081</td>\n",
       "      <td>0.872469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.910469</td>\n",
       "      <td>0.721180</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.991694</td>\n",
       "      <td>0.983446</td>\n",
       "      <td>0.931923</td>\n",
       "      <td>0.950562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><h2>AFFILIATE</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.448878</td>\n",
       "      <td>0.361975</td>\n",
       "      <td>0.888036</td>\n",
       "      <td>0.719192</td>\n",
       "      <td>0.434619</td>\n",
       "      <td>0.305327</td>\n",
       "      <td>0.928214</td>\n",
       "      <td>0.920598</td>\n",
       "      <td>0.916851</td>\n",
       "      <td>0.933623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.125437</td>\n",
       "      <td>0.196652</td>\n",
       "      <td>0.006385</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.109472</td>\n",
       "      <td>0.233794</td>\n",
       "      <td>0.021890</td>\n",
       "      <td>0.061732</td>\n",
       "      <td>0.056443</td>\n",
       "      <td>0.062128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.228070</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.878927</td>\n",
       "      <td>0.717524</td>\n",
       "      <td>0.240732</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.898584</td>\n",
       "      <td>0.848930</td>\n",
       "      <td>0.856627</td>\n",
       "      <td>0.831025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.883668</td>\n",
       "      <td>0.717524</td>\n",
       "      <td>0.471120</td>\n",
       "      <td>0.077913</td>\n",
       "      <td>0.923540</td>\n",
       "      <td>0.858713</td>\n",
       "      <td>0.856627</td>\n",
       "      <td>0.918088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.496124</td>\n",
       "      <td>0.480620</td>\n",
       "      <td>0.892178</td>\n",
       "      <td>0.720305</td>\n",
       "      <td>0.472670</td>\n",
       "      <td>0.442545</td>\n",
       "      <td>0.926111</td>\n",
       "      <td>0.953429</td>\n",
       "      <td>0.942739</td>\n",
       "      <td>0.970300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.892312</td>\n",
       "      <td>0.720305</td>\n",
       "      <td>0.479382</td>\n",
       "      <td>0.453620</td>\n",
       "      <td>0.933194</td>\n",
       "      <td>0.962991</td>\n",
       "      <td>0.950930</td>\n",
       "      <td>0.972260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.542636</td>\n",
       "      <td>0.893093</td>\n",
       "      <td>0.720305</td>\n",
       "      <td>0.509189</td>\n",
       "      <td>0.525691</td>\n",
       "      <td>0.959643</td>\n",
       "      <td>0.978927</td>\n",
       "      <td>0.977332</td>\n",
       "      <td>0.976444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><h2>INSURER</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.534854</td>\n",
       "      <td>0.446109</td>\n",
       "      <td>0.916533</td>\n",
       "      <td>0.778961</td>\n",
       "      <td>0.509757</td>\n",
       "      <td>0.332948</td>\n",
       "      <td>0.962354</td>\n",
       "      <td>0.972730</td>\n",
       "      <td>0.933608</td>\n",
       "      <td>0.940510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.084980</td>\n",
       "      <td>0.032026</td>\n",
       "      <td>0.013470</td>\n",
       "      <td>0.032118</td>\n",
       "      <td>0.077067</td>\n",
       "      <td>0.092219</td>\n",
       "      <td>0.020308</td>\n",
       "      <td>0.027240</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.046615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.903864</td>\n",
       "      <td>0.755505</td>\n",
       "      <td>0.442245</td>\n",
       "      <td>0.232753</td>\n",
       "      <td>0.933890</td>\n",
       "      <td>0.925168</td>\n",
       "      <td>0.896197</td>\n",
       "      <td>0.883234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.486842</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.906264</td>\n",
       "      <td>0.755505</td>\n",
       "      <td>0.472523</td>\n",
       "      <td>0.254049</td>\n",
       "      <td>0.952946</td>\n",
       "      <td>0.975627</td>\n",
       "      <td>0.896197</td>\n",
       "      <td>0.911696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.506579</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.910414</td>\n",
       "      <td>0.755505</td>\n",
       "      <td>0.482602</td>\n",
       "      <td>0.331623</td>\n",
       "      <td>0.964152</td>\n",
       "      <td>0.983025</td>\n",
       "      <td>0.924019</td>\n",
       "      <td>0.931703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.930753</td>\n",
       "      <td>0.814145</td>\n",
       "      <td>0.510991</td>\n",
       "      <td>0.395992</td>\n",
       "      <td>0.973431</td>\n",
       "      <td>0.989272</td>\n",
       "      <td>0.973513</td>\n",
       "      <td>0.987349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.480263</td>\n",
       "      <td>0.931370</td>\n",
       "      <td>0.814145</td>\n",
       "      <td>0.640426</td>\n",
       "      <td>0.450324</td>\n",
       "      <td>0.987353</td>\n",
       "      <td>0.990557</td>\n",
       "      <td>0.978115</td>\n",
       "      <td>0.988567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = \"\"\n",
    "for ir, r in enumerate(data.get_roles()):\n",
    "    out+=\"<h2>\"+r.upper()+\"</h2>\"\n",
    "    out+=pd.DataFrame(res).iloc[[ir+(ii*5) for ii in range(5)]].describe().to_html()\n",
    "HTML(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GUARANTOR   : 0.7260 (ndcg), 0.5426 (rand) | min: 0.7571, rand: 0.8889, ndcg: 0.9335\n",
      "ISSUER      : 0.6979 (ndcg), 0.5359 (rand) | min: 0.6427, rand: 0.8342, ndcg: 0.8921\n",
      "UNDERWRITER : 0.8016 (ndcg), 0.6300 (rand) | min: 0.6928, rand: 0.8863, ndcg: 0.9390\n",
      "SERVICER    : 0.6498 (ndcg), 0.5947 (rand) | min: 0.7186, rand: 0.8860, ndcg: 0.9015\n",
      "TRUSTEE     : 0.8667 (ndcg), 0.6274 (rand) | min: 0.7907, rand: 0.9220, ndcg: 0.9721\n",
      "COUNTERPART : 0.7476 (ndcg), 0.5555 (rand) | min: 0.7305, rand: 0.8802, ndcg: 0.9320\n",
      "SELLER      : 0.7381 (ndcg), 0.5515 (rand) | min: 0.6532, rand: 0.8445, ndcg: 0.9092\n",
      "AGENT       : 0.7401 (ndcg), 0.6117 (rand) | min: 0.6786, rand: 0.8752, ndcg: 0.9165\n",
      "AFFILIATE   : 0.7172 (ndcg), 0.6013 (rand) | min: 0.7192, rand: 0.8880, ndcg: 0.9206\n",
      "INSURER     : 0.8766 (ndcg), 0.6224 (rand) | min: 0.7790, rand: 0.9165, ndcg: 0.9727\n",
      "ALL         : 0.7699 (ndcg), 0.5863 (rand) | min: 0.7162, rand: 0.8826, ndcg: 0.9347\n"
     ]
    }
   ],
   "source": [
    "for ir, r in enumerate(data.get_roles()):\n",
    "    tmp = pd.DataFrame(res).iloc[[ir+(ii*5) for ii in range(5)]].describe()\n",
    "    m = tmp.loc['mean']['baseline_worst']\n",
    "    print(\"{: <12}: {:.4f} (ndcg), {:.4f} (rand) | min: {:.4f}, rand: {:.4f}, ndcg: {:.4f}\".format(\n",
    "        r.upper(),\n",
    "        (tmp.loc['mean']['ndcg_full_proba']-m)/(1-m),\n",
    "        (tmp.loc['mean']['baseline_rand']-m)/(1-m),\n",
    "        m,tmp.loc['mean']['baseline_rand'], tmp.loc['mean']['ndcg_full_proba']) )\n",
    "    \n",
    "tmp = pd.DataFrame(res).describe()\n",
    "m = tmp.loc['mean']['baseline_worst']\n",
    "print(\"{: <12}: {:.4f} (ndcg), {:.4f} (rand) | min: {:.4f}, rand: {:.4f}, ndcg: {:.4f}\".format(\n",
    "    'ALL',\n",
    "    (tmp.loc['mean']['ndcg_full_proba']-m)/(1-m),\n",
    "    (tmp.loc['mean']['baseline_rand']-m)/(1-m),\n",
    "    m,tmp.loc['mean']['baseline_rand'], tmp.loc['mean']['ndcg_full_proba']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6d6e973ac8>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAE0CAYAAADJxUOPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmYXGWZ///3JwlRdtBAhIAJjixBwFECiKIGFQiLAUWU\nMPITBeJCcMEFGBAFQQQ3QBHkRxBQDCoCEyFD3NIiIkoUQSADE1EkuLApTlCEkPv7x32KlE2Srq6u\nrlPn9Od1XX11V9WpruepOnWfZ38UEZiZWT2NKjsBZmY2fBzkzcxqzEHezKzGHOTNzGrMQd7MrMYc\n5M3MasxB3sysxhzkzcxqzEHezKzGxpT1wuPGjYtJkyZ17fUee+wx1l577a69Xrc5f9VV57yB89dp\nv/jFLx6KiI1aPb60ID9p0iQWLlzYtdfr6+tj6tSpXXu9bnP+qqvOeQPnr9Mk3TuY491cY2ZWYw7y\nZmY15iBvZlZjDvJmZjXmIG9mVmMDBnlJF0l6QNLtq3hcks6RtFjSbZJe2vlkmplZO1opyV8MTFvN\n43sDWxY/M4Hzhp4sMzPrhAGDfERcDzyymkP2By6NdBOwgaRNOpVAMzNrXycmQ00A7mu6vaS474/9\nD5Q0kyztM378ePr6+gb9Ykf94DEee3Llj917xn6D/n8NE4+9ZqX3r70GnPva7s1mq3v+2rV06dK2\nzpcqqHPewPkrXUQM+ANMAm5fxWPXALs13f4BMGWg/7njjjtGOyYee01bz1uwYEFXX69ddc9fu9rN\nXxXUOW8Rzl+nAQujhbjd+OnE6Jr7gc2bbm9W3GdmZiXrRHPNXGCWpMuBXYBHI+IZTTVmVl+S2n5u\nFk57W5XzN2CQlzQHmAqMk7QE+BiwBkBEnA/MA/YBFgN/B94+XIm1eqvyF2mkW937P+m4a/ndp/bt\nYmo6r8r5GzDIR8SMAR4P4KiOpchGrCp/kcx6VWlLDZtZtbz45O/y6D9WMfRrAJOOu3bQz1l/zTW4\n9WN7tvV6toKDvJm15NF/PNlWbard9dbbuTDYMznIm3WB+xusLF6gzKwLVjeOeeKx1ww0T8WsbQ7y\nZmY15iBvZlZjDvJmZjXmjlfrKg/DM+suB/kes+7k49j+kuPae/Il7bweQPcmGXkYXnXV/dysKwf5\nHvN/iz7lIGg9qe7nZl1rmQ7yZmbUt5bpjlczsxpzkDczqzEHeTOzGnOQNzOrMXe8mnVIXUdnWLU5\nyFtX1XmsdV1HZ1i1OchbV9V9rLVZr3GbvJlZjTnIm5nVmIO8mVmNOcibmdWYg7yZWY21FOQlTZN0\nl6TFkp4x/k3SREk/kHSbpD5Jm3U+qWZmNlgDBnlJo4Fzgb2BbYEZkrbtd9hngEsjYgfgFOD0TifU\nzMwGr5Vx8jsDiyPiHgBJlwP7A3c2HbMtcEzx9wLg6k4m0sx6Q9vzDq5rb0avDV0rQX4CcF/T7SXA\nLv2OuRV4I3A28AZgXUnPjYiHmw+SNBOYCTB+/Hj6+vraSnQ7z1u6dGlXX28o6p6/bgaKtdfobv7q\n/NldPG3ttp532HWPtf3cbuavjNnYfX3tvS+DEhGr/QHeBFzYdPtQ4Iv9jtkUuBK4hQz0S4ANVvd/\nd9xxx2jHxGOvaet5CxYs6Orrtavu+WtXFdLpz27l6p7Obn9+wMIYIG43/7RSkr8f2Lzp9mbFfc0X\nij+QJXkkrQMcGBF/bf/SY2ZmndDK6JqbgS0lbSFpLHAwMLf5AEnjJDX+1/HARZ1NppmZtWPAknxE\nLJM0C5gPjAYuiog7JJ1CVhvmAlOB0yUFcD1w1HAluM6rGFq1+dy0XtTSKpQRMQ+Y1+++k5r+vgK4\norNJWzmvYmi9yuem9SLPeDUzqzEHeTOzGnOQNzOrMQd5M7Mac5A3M6sxB3kzsxrzRt49yItAmVmn\nOMj3mHbGWUNeGNp9rpnVl4O8WQe5FlZtdfz8HOTNOsS1sGqr6+fnjlczsxpzkDczqzEHeTOzGnOQ\nNzOrMXe8mtmQSVr942es+rHc0c6Gi0vyZjZkq9tjdMGCBQPtI23DyEHezKzGHOTNzGrMQd7MrMbc\n8Wo9w513Zp3nkrz1DHfemXWeg7yZWY05yJuZ1VhLQV7SNEl3SVos6biVPP58SQsk3SLpNkn7dD6p\nZmY2WAMGeUmjgXOBvYFtgRmStu132InANyPiJcDBwJc6nVAzMxu8VkryOwOLI+KeiHgCuBzYv98x\nAaxX/L0+8IfOJdHMzNrVyhDKCcB9TbeXALv0O+bjwHclHQ2sDbxuZf9I0kxgJsD48ePp6+sbZHJT\nO89bunRpV1+vDFVJZzuG8vlVQZ3zVvfPDnr78+vUOPkZwMUR8VlJuwJflbRdRCxvPigiLgAuAJgy\nZUpMnTp18K903bW087y+vr62ntfu63VdVdLZprY/vyrwZ1dtPf75tdJccz+wedPtzYr7mh0OfBMg\nIn4KPBsY14kEmplZ+1oJ8jcDW0raQtJYsmN1br9jfg+8FkDSZDLIP9jJhJqZ2eANGOQjYhkwC5gP\nLCJH0dwh6RRJ04vDPggcKelWYA5wWHgaoplZ6Vpqk4+IecC8fved1PT3ncArOps0MzMbKi9QZtYF\nXnzNyuJlDcy6wIuvWVkc5M3MasxB3sysxhzkzcxqzEHezKzGHOTNzGrMQd7MrMYc5M3MasxB3sys\nxjzj1cxsAFWeseySvJnZAKo8Y9lB3sysxhzkzcxqzEHezKzGHOTNzGrMQd7MrMYc5M3MasxB3sys\nxhzkzcxqzEHezKzGKrmswaTjrm3vidcN/nnrr7lGe69lZtYDKhfkf/epfdt63qTjrm37uWZmVdVS\nc42kaZLukrRY0nErefzzkn5V/Nwt6a+dT6qZmQ3WgCV5SaOBc4E9gCXAzZLmRsSdjWMi4gNNxx8N\nvGQY0mpmZoPUSkl+Z2BxRNwTEU8AlwP7r+b4GcCcTiTOzMyGppU2+QnAfU23lwC7rOxASROBLYAf\nruLxmcBMgPHjx9PX1zeYtA5Zt1+v2+qcv6VLl9Y2f3XOGzh/Zet0x+vBwBUR8dTKHoyIC4ALAKZM\nmRJTp07t8MuvxnXX0tXX67aa56+vr6+2+atz3sD5K1srzTX3A5s33d6suG9lDsZNNWZmPaOVIH8z\nsKWkLSSNJQP53P4HSdoG2BD4aWeTaGZm7RowyEfEMmAWMB9YBHwzIu6QdIqk6U2HHgxcHr2w35WZ\nmQEttslHxDxgXr/7Tup3++OdS5aZmXWC164xM6sxB3kzsxpzkDczqzEHeTOzGnOQNzOrMQd5M7Ma\nc5A3M6uxym0aMpJJWv3jZ6z6Mc9RMxuZXJKvkIhY5c+CBQtW+7iZjUwO8mZmNeYgb2ZWYw7yZmY1\n5iBvZlZjDvJmZjXmIF9xc+bMYbvttuO1r30t2223HXPmeGMuM1vB4+QrbM6cOZxwwgnMnj2bp556\nitGjR3P44YcDMGPGjJJTZ2a9wCX5CjvttNOYPXs2u+++O2PGjGH33Xdn9uzZnHbaaWUnzcx6hIN8\nhS1atIjddtvtX+7bbbfdWLRoUUkpMrNe4yBfYZMnT+aGG274l/tuuOEGJk+eXFKKzKzXOMhX2Akn\nnMDhhx/OggULWLZsGQsWLODwww/nhBNOKDtpZtYj3PFaYY3O1aOPPppFixYxefJkTjvtNHe6mtnT\nHOQrbsaMGcyYMYO+vj6mTp1adnLMrMe4ucbMrMZaCvKSpkm6S9JiScet4pg3S7pT0h2Svt7ZZJqZ\nWTsGbK6RNBo4F9gDWALcLGluRNzZdMyWwPHAKyLiL5I2Hq4Em5lZ61opye8MLI6IeyLiCeByYP9+\nxxwJnBsRfwGIiAc6m0wzM2tHK0F+AnBf0+0lxX3NtgK2kvQTSTdJmtapBJqZWfs6NbpmDLAlMBXY\nDLhe0vYR8dfmgyTNBGYCjB8/nr6+vg69fGu6/XrdtHTpUuevouqcN3D+ytZKkL8f2Lzp9mbFfc2W\nAD+LiCeB30q6mwz6NzcfFBEXABcATJkyJbo65O+6a2s9xLDuQyjrnL865w2cv7K10lxzM7ClpC0k\njQUOBub2O+ZqshSPpHFk8809HUynmZm1YcAgHxHLgFnAfGAR8M2IuEPSKZKmF4fNBx6WdCewAPhw\nRDw8XIk2M7PWtNQmHxHzgHn97jup6e8Ajil+zMysR3jGq5lZjTnIm5nVmIO8mVmNOcibmdWYg7yZ\nWY05yJuZ1ZiDvJlZjTnIm5nVmIO8mVmNOcibmdWYg7yZWY05yJuZ1ZiDvJlZjTnIm5nVmIO8mVmN\nOcibmdWYg7yZWY05yJuZ1ZiDvJlZjTnIm5nVmIO8mVmNOcibmdWYg7yZWY21FOQlTZN0l6TFko5b\nyeOHSXpQ0q+KnyM6n1QzMxusMQMdIGk0cC6wB7AEuFnS3Ii4s9+h34iIWcOQRjMza1MrJfmdgcUR\ncU9EPAFcDuw/vMkyM7NOaCXITwDua7q9pLivvwMl3SbpCkmbdyR1ZmY2JAM217ToO8CciPinpHcC\nlwCv6X+QpJnATIDx48fT19fXoZdvTbdfr5uWLl3q/FVUnfMGzl/ZWgny9wPNJfPNivueFhEPN928\nEDhzZf8oIi4ALgCYMmVKTJ06dTBpHZrrrqWrr9dlfX19zl9F1Tlv4PyVrZXmmpuBLSVtIWkscDAw\nt/kASZs03ZwOLOpcEs3MrF0DluQjYpmkWcB8YDRwUUTcIekUYGFEzAXeK2k6sAx4BDhsGNNsZmYt\naqlNPiLmAfP63XdS09/HA8d3NmlmZjZUnvFqZlZjDvJmZjXmIG9mVmMO8mZmNeYgb2ZWYw7yZmY1\n5iBvZlZjDvJmZjXmIG9mVmMO8mZmNeYgb2ZWYw7yZmY15iBvZlZjDvJmZjXmIG9mVmMO8mZmNeYg\nb2ZWYw7yZmY15iBvZlZjDvJmZjXmIG9mVmMO8mZmNeYgb2ZWYy0FeUnTJN0labGk41Zz3IGSQtKU\nziXRzMzaNWCQlzQaOBfYG9gWmCFp25Ucty7wPuBnnU6kmZm1p5WS/M7A4oi4JyKeAC4H9l/JcZ8A\nzgAe72D6zMxsCFoJ8hOA+5puLynue5qklwKbR8S1HUybmZkN0Zih/gNJo4DPAYe1cOxMYCbA+PHj\n6evrG+rLD0q3X6+bli5d6vxVVJ3zBs5f2VoJ8vcDmzfd3qy4r2FdYDugTxLA84C5kqZHxMLmfxQR\nFwAXAEyZMiWmTp3afsoH67pr6errdVlfX5/zV1F1zhs4f2VrpbnmZmBLSVtIGgscDMxtPBgRj0bE\nuIiYFBGTgJuAZwR4MzPrvgGDfEQsA2YB84FFwDcj4g5Jp0iaPtwJNDOz9rXUJh8R84B5/e47aRXH\nTh16sszMrBOG3PHaS4o+gVU/fsaqH4uIDqfGzKx8tVrWICJW+bNgwYLVPm5mVke1CvJmZvavHOTN\nzGrMQd7MrMYc5M3MasxB3sysxhzkzcxqzEHezKzGHOTNzGpMZU0EkvQgcG8XX3Ic8FAXX6/bnL/q\nqnPewPnrtIkRsVGrB5cW5LtN0sKIqO3es85fddU5b+D8lc3NNWZmNeYgb2ZWYyMpyF9QdgKGmfNX\nXXXOGzh/pRoxbfJmZiPRSCrJm5mNOA7yZmY15iBvZlZjDvJtkrSGpLWKv9csOz2dJGmCpIslrVF2\nWgZL0kaSXld2Osx6hYN8GySNAV4DTJH0VuBsSc8uOVkdExH3A5sDF0kaW3Z6Bulg4C2SppWdkOGi\nYjPjoqAxuuz02PDSQJtXD8BBvg0RsQx4EjgTOBX474h4vNxUdUZT6f19wMuBbxcXtZ4mabykXYD/\nH/gfYC9J+5ScrGERESFpOvAl4DJJO5WdpjI0XewmSJpUbmqGhyQVn/erJb1H0h6SNhjM/3CQHyRJ\njffsJ8BPgcXAk5LGl5eqzomIJyXtB3wB+AzwQmBuLwf6ojS7H3AMsCNwLvBHYI86BnpJrwROAD5O\nrptyYgVrXENWBL/9gW8D50v6pKTnlZ2uTiry+Hrg88CaZOHruMHEGwf5QSiuqsslvRY4FjieDIYH\nA3sVx7xA0mYlJnNIiovYDOAbEXFeREwGRgPX9mobfUQ8BfwQ6AMOB6YA57Ai0Fe66aYoqR7cdNcO\nwGnAS4C1gPdGxBN16xsaiKRXASeSF/jrgHcD75e0SakJ66AilhwCTAd+A0wC1gE+IqmlRcoc5Aeh\nuKruC5wP3BARj0fEfwGXAntL+gxwMzChzHQORUQsB+7ud/fbgN2A2UNtH+y0pvT8kWyquQV4BysC\n/RJg/+Jzq6rtgXdLeltx+2HgncBHgLdGxL2SDgE+38s1rmHwOHA0sAtZ0Nob2BM4Q9ILykxYu1by\n/XqYvJBtDJxEBvsfANOAj7dSg3OQb0FT29+GZCn3LRHxQ0nTJJ0D3AV8DLgNOCgiflZeagenKW87\nSNqyyGMf8E5JuxQl++cBlwGzo8emSBcX3s2B75KB/XxWBPpG0829wD2lJXLofkw2nR0o6SBgHrAu\nWXv5u6RXAP8JzC36i2pN0nMkjY2In0fETcC+wFnF35cDWwE9dZ62QtKoxvdL0k6SdgO2iYjfkDW2\nhRFxD/AI2VR8bkQ8MeD/7bHvbM8qhuWNBfYgSwy/JNeQfjZZfZ5atc7Xpk6d1wJfJ6u8G5DV3pcB\ns8iS8CuBmRHxvcZzSks0T38ZljfdXgc4DNifDHa/AmaStY8vkbWuyp3oze91UWLbBziSzNMtZOAX\n2S5/dkRc0wufT6cV7c+viYg5RdPbB8l8XwIsIEu1bwC+RdY6T4iIG8tKbzskbUxevHcGtgWuBX4E\njCcD+pnAH8j+h72AIyLiulb+90iq2rVN0ouBI4AzI+IDkv4X+HFE/FrSRLJZoHLvZRHgp5BB/ADg\n12Sb9qVkSfiNwKbA5yLiV43nlJRcACT9G9l8cbWk7SPi1xGxVNLFwD/JL8P7gQvJz+TRstPcjqYL\n8E7khXdpRFwtaTl5Ef5SRBxS1MQ2jog/1zTAizw/p0t6Pnmh+wCwHRkMJ5A1zw2A/488VysV4AEi\n4gFJtwP/Sxa23hIRN0nagryY3UrWUF4HXBgRPx3MP/fPKn7IDseNgX8A31zJ4weQpcY3lp3WNvI2\nqsjfj8khhy8o7l8PeC9Zetip7HT2S/OmZNPY9mRJro/sIG48vh7ZPLOQ7JRU2WkeYn73JJuZPgXc\nR5biNybbZRcAhxfHVTqfLbwPzyE7H79CDldu3D+VLP1uV9xep4rvBzC66e8zgOVkzaVx3xvJi1db\n/99t8qsREU9FxAPAfwAHNEZpSBotaW2ymeajEXFlr3VIrkpTOkdHjkqZRgaQjwBExN/IksPl9F6f\nzRrA/WRp5nSyeWacpK/A02m/BbgDeFYU35AqKppnZgFHR8RxZHPEQcBeETEXOJss3VHlfK5O41yN\niEeAuWS/y0aS3lXc3wf8iWxaBPh7cX9l3o+i9vWUpOcARMSxwKeBr0latzhsLLCdpDXbiTNuk++n\nqZr8MvLkWQR8nwwsVwBvioj5xbGjIodUVqKa3JS315H5WUS2xa8BzAdui4ijimMbF4Ge0JT2c8h2\n189FxMnFsM75ZN/BTcB7gEMj4pYSkzskkl4O/J6sUd1O1lb+KWkvcqTFa4Cnoqlfom6aPu9Xkk0y\nTwH/RV7odidHnVxFNi2+LQbTfNFjJO1NNpM+DnwvIi6RdAY5guoc4EXApZEj+Qat10pqpStOrL2B\nr5JT+99BNgHcRp5g/90o0Te+ZFUI8PB03vYix/bfAHwUOItsAtgT2FXSBcWxPRPg4V/e4yXAl4Ed\nJO0fEU+SHVGLgYnAf1Y8wO9KfibjyI62ncix0QAPksFtVJ0DPDx9rr4OuBjYjJwX8CHg52Sh60Dg\nFOCoigf4F5Md6ecANwJbSzq1KNFfAnwY+FhE/Fe7rQUuya+EpM8D10XEfElbkaNpNihKjgcDf2mU\n5qugqVT0HLKa/0lgE/LEupksJZ0E/AX4917/0hTDOt8GvInsgLy23+OVqFn1V5xrJwJ3R8SpRS3l\nLLKvYRQwGfhERFxVYjK7RtJXgfkR8TXlTNbzgV9ExCckHUoOKVxUbioHr+n7OIaslRwUETOLx15G\nztz+z4hYLGnriLhrKK9XuREhw0nSZHIi0Giy1D4/Iu6WNIGcSrxORFxeHFuZQFKcUPsAy8jhZ2uR\npaAdyU6tW8kS4sm9HuAha1CSriLz8yFJNAf6qnwuK7ExGcx3lbRdRNwu6Why/P+GwMMRsbBK5147\nJO1OFjjuJNvg14qIP0n6IPBVSZ8GLi9qcZXSFOD3AV5Nthi8TNK0iLguckTNU+TAgcXkaJshxRs3\n1xSKjq6jgEPJEsMySUcVD/+ZfK8aHSGVCiSSdgbeBTxedCSvTa6D8RQZ8G8BLouIv5eXylXTivWC\nnhYRfwW+Q7bJ/rnriRoGEXEDWXL/H3LI4DYRsTxy0s/8iFhYHFeZc2+wiiGjp5MjTH5HFkS2LUq9\nawFPAGOrGODh6QLXy4G3kK0Ft5M16jdJOrJovtmOXMKgI03Cbq4BJD0/In4v6UByeN4XyanSHyBX\nm5xITrCoXDVZ0vrkcLs/RcQ+TSWJs8mJFxsCH4qIa0pN6EpI2h54IHIM+ErboSWt0fjCV7mE25z2\norNxP3Lc/9ciov8yE7Uk6YVkP9HfIuLo4r6PAluThZKJwGlV/B42FBer2eTnOzFyjsdEcqTeMeTy\nHFdExJUde82Kfic6Rjm55iqy2vR1srPjqog4t/hA/h34a9E+VskgUnQUX0q2811Y3Lcm8FLg773U\nUdl0EdoWOJks0c2KiAf7B/rGCCBJY6Ki0/mb89Qv0E8lh4h+MXJae+0VfRKzyElOp0TE9cX9k8mZ\n5csiJyBW6nvYdE4/K3KU1PrAHOCJiDig6bixZKf6453M44gO8kXH1vrAN8hhWicC25BT5A+PiB+V\nl7rOUi5d8FngnIi4qOz0rI6kA8jlCX4CbAn8DTimaJdtDFttBPgNyHwdGxEPlZjsQWmxlvKcyDHi\nla6lrEpT8NuZLKk/TE7+OpFce+aaKvQRtaIYsXcQmcc55Mip08n5HIcM52uP2Db5otTwYTK4H0p2\ncmwC/BZ4AdlGVpulWyPiB+R0/+MlHVF2elalqD3NAD4YER8gO4rvBc6UtFER4McWAX59shZ2cRUC\nfGMIXFFLOQk4pylPo5qPKwL/I0VBpJbt8EWA3x24mpzV+X3g9WRz6XLgzcVok0pTDov9BHANufzC\nf5BzHT4ErC3piuF8/REb5MlO1EfIMfD7kVfXRyPiMnKdmisi4h8lpq9tjWDSX+QMwXeSHXs9oyn4\nrVc0u6xPtlFCXnxvJtftOFnShpFrp29ABviPRsSPy0j3YBVB7QBy7PcSsgP8bEnP6xfoG7WVDcjN\nMMaVlORhJeml5NIgh0XE+8jmqU+ScwO+QE4OeqS8FA5d0d5+LDnJ6UpWTHCbBiwl19s5bVgTET2w\ndkOZP2Snznzge+S6KGs3PVaJNTBY0ew2gSwpjBnM83ok7XuR07lHk0PLvgscWDz2CuAisl9hF3Ld\nmi+TK3+W/v4PIq9jyKbBVxa3tyar7JcCGxX3jS1+NzrMX1l2uofx/fgieRE/DHh2cd90spkGirVo\nqvxDzlY9hxzBtnPT/TcAL+9GGkZySb7R6XUXKxY/eoIcnwpUp4oc8XQJ8RKyBHR8UYL4Fyo2fVax\nBkbZ+Sve/1Du8HMu8J3ImbY/Bc4DPi3pQjIwnkOWfBr5OimyZtLTRkotpR0RMYtck2Z3YIvi7seA\n0ZKeVfxdaRFxBzks9grgSEmvK0YRjQMe7UYaRlSQ79+MESvWnXk4Ir5OXmlvWFVzR6+S9CKgsYjV\nQ+SX5pHmfPTrqLwB+LdSEptpeb6kFxTv/2iyFP+FiLi+CPxPRA6T2x34Grna4Frk+vC/jNTzY+Ob\nOhb3Aj5a5PV0cqeqA4sL2p/JST/rAFsVn9kZ5OiSG0pL/DDofz4CRMQxZLPM+ZK+SHa6XhAR/yy7\nENIpkRt9XEU2Pc0mCzTviog7uhFrah3km0pRE4rgNrr/MY0TqfhCNtrgKxXkybHu88gNTXYhNxT4\nP3KkUHOAX5/cWOEDEbG4tNRmJ9t3Jb2wKdBtKmnNWDGc8GXAWkVpfR2yk/zQktPdspFQS2mFpI2U\na9A0apyNlSWfagr07ySXh96ami7bEBF3kk2M55GzWO/r1mvXfghl0Ywxi5xkcDe5mtu9/Y5pBME1\nowKdrU0lxEYH3aZkCWFzYHpE3KPc0/R9wIyIeFi5rd9VwIlllhCb0n4qOcLgIHL3m5PIYHcHud3g\nbHIY663KZZ2fHREPl5XuVik3thhTfAajyeUjHoiIs/WvY+Inkk0US8jlDM4n9yWoxEWsVcplGXYA\nvh3FTkbNTYVqWu1U0rnke3Es8NuqlOSbzuk1gOWxmsX9JG1HNg+PJS/mwz7LvO4l+UE3YxTtZT2t\nOKGmASdIel9E/IEcfnY9sE8R4M8gm0AagfE/KLkJoOnLMI0snT+LXLf+9+SIk7eTfSPnkTMbby0C\n42NVCPCF2tdSWiFpvKRdyM3V/wfYS7ley+pK9EeR48crtZtXkZ/p5GqSlymXZniG4vy/nTznj+9G\ngG8ksLY/ZBvuSeQX70ZW7H40ufg9uvi9Pjm65lVlp3mA/Iwqfu9EjgR6N9mee25x/wHkl+qLwD7F\nfaWPoOmXh8ms2Df2xeQG6DcCG5GFji2aPqeeSnsLeWvUjE8t8jSBnFV8NVlrGV/keSHw4uLYtYHn\nlp32Dr8Po8n10b9Bjox6Nrkpzecb52W/96vxPRRwcNnpbyO/rwR+Vnze3yfXvR/b/z0pfq9H1qjH\ndS19Zb9BHX6zGydNIxhuCvw3OS61ETj2JYfnPbe4vSG5jdxuZad/NfnauHFSkFXfc1mx9dsazYG+\nuO9ZjfejFwJlcxqKID678TmRpflvkRujv7DstHbg3JtGjqb4Bbm14rji4vtVst/kRuANjfyXne5h\nfD+2IAshF5GFreZAP63puDHF7w3I/opdyk57C3mb0HwxIhc2nE7Ot7mRXJMGYM1+eVwf+AFdLkzW\naqnhiKdAMM6jAAAMSElEQVSbAnaS9LfIdtDvkyNJ9pH0W7IZ4/jooWaM1VGuZ3EIOVvuIeC55Njb\nkLRZRCyR9O/A3cpp8DPIoaBEcWaVrfhcXk12BN8L7C3pHZHLK/xT0k/Iae0bk8MKK6fI42RyA/EZ\nZLPTAeQQwf2L3xPJi8E9RdW9dht/NLW3/5GsVY4hN96B7HM5ihxdNBqYFxHLiqbSK4CPRMTPykj3\nIG0PvFu5Fs0l5FIF7yQnWL41Iu6VdAjwKklHR8STRZ/YFcDHo9vDYsu+Knboylr5ZoxV5Gujprxt\nRk4WWpesAl9GLsewSfH4GvTYxBlWlG53ITtUrySnd59Ntr1+hOx4vYViM+aq/TACailtvCebk/1D\nLyvOy6PJEn2j6eY4VjSZrk1ORuzpptJ++VubXH5hbnH+rlfk9+PkoIFXkK0HjVgzujjndy8jvZUe\nXSNpY7I3+yFJO5BX019GxOyip/tWYEGs2Le0sQpco9OnZzNfpP8MclOPI8jhZUcBfyWnQe9KTom+\nniwR/aF4XumTnJopF586hSyl3SbpreTaQJuQF7HfADdFhYfN9aulXESOYLqoeOz95H66n4yIG8tL\n5fDRM1cHXYecxbo/udDcr4CZZLPNl4AbGueopK3J5sXbup3uweo3KmgssA9wJJmnW4DPkE2k44Cz\no1i+W7ke0zqReyB0XWWba+rQjDGAZeQM1pnkyXMMOZv1CHLN7VPIz+/tZL8D0JN524AMcnuQ++Re\nDryZLOXeTX4ZKrMZekPTSKFdyC/5XWRt5VvAqcr1Zn5LblN4aOSoitpRLtW9PXC1pO0j4teRa6Rf\nTK6Hfya5MN6F5Pn6aFOgVAxxa7tuafq8dyLP6aURcbWk5WTLwZci4pCiALlx5OqijT6xZWThrBxl\nV33arC5Vuhmjhfw1algiR6NcSFb3RpFrbZ9Jdu5VYmQG2Sl1BzlmH7L6egiwbdlpG2K+dgauA3Yo\nbr+VHM11Htn+egZFJ2sdf8iBDXeRQV7kAIZvND2+HjlIYCG5XEjPN5EOkN89yaWQP0VOZjqS7Eea\nTq4z1BgM0VP5rNw4+aIZ43jgoqIatD4Z7I4nq4VfIUuN+0raNCKejIgfN4+N72VNJYbNgc0jNyo+\nk2zL/Bw55virxeGbRQXGj0fEXHIs+IckvS0inoqIr0fOAqyy5loKZC1lMTl9/SdkB/9VVTn32rAG\ncD/5HpxONs+Mk/QVgIj4G9mMcQfZJFOZmlp/RcvBLODoiGjMvTkI2Ks4v88mm4fptXxWrk2++MLs\nQDZjPEk2Y2xNNmM8STZjvIpsxjgmIu4vKaltk/R6cvz4PeTMuHeTbfNHkaX595DDsyq1gFMxYeRT\nZFD4U9RgdEmRp9OBUyNiTjFq5C3Ar2pwEVulpsLIOWST1Oci4uSiEDafnAtxE3muHho9tPvYYCn3\nZP09K5YJ/kZk395e5Fo7rwGe6tXzuVIl+aZ229vIkTLrkONu7yKnwY8i16P+MfCeigb4F5EbZexN\nTqrYmtzz8g4yz88CtqlagIenS/Svjog/9OoXYrBqXEtZrabS6hJyTZYdJO0fud/uXmSNZiK55WSV\nA/yuZNPoOHJE2E7ApOLhB8nhkyvd2atXVKYk368ZQ5Ebb29FBsR/kCX6F5Ez7c6LinTo9CfpBeSE\nmuXkCIUZEfFbSbtGxE8lrRXdmg5tLatjLaVVys1O3ga8ieyAvLbf45XqVG8o4suJwN0RcWpRSzmL\n7GsYRfaX9fyCapUJ8lDPZox+vfYPkiMQvkTOxJ0eEX8shuidRXbi/a681NrqKLfye7DsdJShmND0\nenLi02f6B/oqkrQb8C7yu3hsRNxeXNCmFPc9HBELe/0iVpnmmro2YxQBfjq5CuHzIxepuowcgbK3\npPeSIxQ+6gDf20ZKgFfTfrQNkWPAv0PuctXza/23InIW/FnkYIfpkraJiOUR8fOImB8RC4vjejbA\nQ4VK8nVtxijydSW5zOw9kjYjd8R5CTlj8DnkZKcf9nqJwepN0vbkssl/7j8BqumYNYp2+co208Az\nJj69klyX5p/A1yLi7lITN0g9OxlqJc0Yo8jlCZ7RjCGpcs0YTSfRhuSGES+U9C5ylcJXkPuXfrL5\nOVX9wlh1NX0PtyXnACyXNCsiHlzJTNfRkeu0jImIZVU8Xxt5KvKsSD8uRk3tD6xyrfhe1bPNNXVt\nxmgaM/1cgIj4BfAjcoOPhRGxF7kGxqsljVpZ1disW4rv4QHkev9LyDkpZ0t6XuRM5VHwjH0ZvlzM\n+K0MSdtLGt+cp+aLVOTa/5+IiN8Ux1dm7kPPNtfUuRlD0n7kok2/JtfxuLrpsZeTM1zfHRE/KimJ\nZsDT665cBnyxKNFuTTaXTgA+WJTox0ZuQL4+uXb+SVGBDcj71VJOJpuCn1FLaVqeYHlzc1RV9Fwp\nsekK2dyMcSY5Dv4+srP1kxHxoYj4IVSrGUPSVHKBsaPIVQs/KOkYSRsUQ7a+QvbkO8BbKRrfQUnr\nRa67sj45ARFy/PvNwFbAyZI2LAL8BuRmGB+tQoCH1mspFOPgizyeX7VaSs8E+To3YxTteQ3bAAeT\no4MmAl8nx1a/A/gdsGdEfKdK1UGrj6bS7V7AR4tz93RyDfgDY8WWhneSkxG3Ks7VM+jhfRlWpqil\nzCBrJB8gR+/dC5xZDIddXtRSnipqKVcBF0fEQyUme9B6JlAWJ9Z+5B6Jn5F0QEScEBH7RsQ3i2aM\nd5ABf/nKevZ7jaR14el9LF8l6c3kGP+/kTtUHRgR55Gfww7AplFsMl6l2onVQ9FEEZJeRfZ3facI\n6j8lF137tKQLyW39ziFr2hOLp59UtFv3tJFSS2nWM0G+bs0YktYCrpV0oKRtyKnf+5Ilhy+Qa1Hv\noRz/vw7w+Sp1IFt9SHq+pBcUJdfR5LIEX4iI64vA/0TkrM7dga8BU4G1yPXhf1mMQOn5sfEjqZbS\nrNQhlI0e+eJm/2aM2eQMOsjJTntGbqtViU7WiPi7pM+Tu+A8Ri5DemPRobwv2cnzHnJhtTMj4tby\nUmsj3BuBWZKmRcRiSX8GJkhaMyL+ASDpZeRa8H3KfRo+TC48VontGhsdqU21lHcUNexGLeWzkvYm\n5+LsRy6AOBH4OVlL6fmL2KqUUpIfKc0YRennBGBHcqU6yM7j35MbZryKHPN/pdvgrQxFoekscpnk\nSyVNAG4gC1u7Shov6cVkQWts8bT/BY6oQsFkpNRSVqfrJfmmZoyzyXWmv0xeLZeT7WMvBX5ZXGEr\n34wREd+XdBjZnvmbyOVo/0qebJ+LiAeK4ypz8bJ6aGq+mEZ+155FBvs3kCNO3k4OhNgAOC0ibi0C\n42Nk7bQKal9LGUgp4+QlvYEVzRgn9mvGeDW5B+gTZDPGlV1P4DBQLq52CbkM8uPk9OjvlJsqG+kk\nTQa+R/YV/Y2cVb4XObvzYbLJQsVclUo0lTY0XcROJWvSBwHjyZm755CFzOeRTcOHFxextYFnRwU2\n42lVKW3ykbvl/B/wbfLNv5F/bcY4DFgrIh6o2om1KsWwyCPITU2OjFxvpxZ5s2rpd949DsyPnOg0\nilyMazty4483N5dmq3SujpBaSktKnfFalOg/TQ5NmqNci+Zz5JZalRqL2irlpuKPlJ0OG9mK79o2\n5Ljwi8ga9UXFY+8n5258MiJuLC+VQ1PnWspglDq6pijRLwMukXQwWar4eF0DPIADvJWlqXS7C7ln\nwV1kk8W3gFOVMzl/S24AcmhE3F5eatszEmopg1X6OPmiXfoI4N+As8KzPc2GRRHgdybXaZkREW8k\nA/3D5B4NO5MbYpxSxQAPT+fx1ZLeSY4Q2lvSOyInUP6T3GD9D8DGpSa0i3pmgTI3Y5gNP0l7AvPI\niYWfVU7tfzO5deZDwNnFcMNKNV/0q6VcxIpaynpkh+tZZC3lP6loLaVdPRPkzaw7lEt4nw6cWvSF\njQbeAvwqKrwBeVFLOQX4SETcJumt5Ei9TYCNgN8AN0WP78naaT27aYiZDY+ImFv0hX1CuQDXJeRC\neVW3AdlhvAdwGzma5s3kyJq7qWgtZahckjcboYoS/afIwPinqMCifwOpay1lKBzkzUYw5ZK6tdqA\nXNI+wCeAc4payojmIG9mtVPHWkq7HOTNrJbqWEtph4O8mVmNlT4ZyszMho+DvJlZjTnIm5nVmIO8\nmVmNOcibmdWYg7yZWY05yJuZ1dj/A+pLQKFgOoqAAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6cfc524d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "pd.DataFrame(res)[['baseline_worst','baseline_rand','ndcg_full','ndcg_role','ndcg_full_proba','ndcg_role_proba']]\\\n",
    "    .boxplot(figsize=(5,8), rot=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAEECAYAAAAlJU69AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFHBERETEdxRwRERExHcUcERERMR3/h/T4bal/Y0/bwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fde84a1b6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm as colm\n",
    "plt.figure(figsize=(8,5))\n",
    "sub = plt.subplot(121)\n",
    "normed = conf_matrix_full/conf_matrix_full.sum()\n",
    "plt.imshow(normed, cmap=colm.Blues, vmax=0.5)\n",
    "plt.title('conf matrix FULL')\n",
    "sub.set_yticks([0,1,2,3])\n",
    "sub.set_yticklabels(['irrelevant', 'neutral','relevant', 'highly'])\n",
    "sub.set_xticks([0,1,2,3])\n",
    "sub.set_xticklabels(['irrelevant', 'neutral','relevant', 'highly'])\n",
    "\n",
    "for i in range(normed.shape[0]):\n",
    "    for j in range(normed.shape[1]):\n",
    "        v = normed.T[i][j]\n",
    "        c='%.2f'%v if v>0.005 else ''\n",
    "        sub.text(i, j, c, va='center', ha='center')\n",
    "\n",
    "\n",
    "\n",
    "sub = plt.subplot(122)\n",
    "normed = conf_matrix_role/conf_matrix_role.sum()\n",
    "plt.imshow(normed, cmap=colm.Blues, vmax=0.5)\n",
    "sub.yaxis.tick_right()\n",
    "sub.set_yticks([0,1,2,3])\n",
    "sub.set_yticklabels(['irrelevant', 'neutral','relevant', 'highly'])\n",
    "sub.set_xticks([0,1,2,3])\n",
    "sub.set_xticklabels(['irrelevant', 'neutral','relevant', 'highly'])\n",
    "\n",
    "for i in range(normed.shape[0]):\n",
    "    for j in range(normed.shape[1]):\n",
    "        v = normed.T[i][j]\n",
    "        c='%.2f'%v if v>0.01 else ''\n",
    "        sub.text(i, j, c, va='center', ha='center')\n",
    "        \n",
    "plt.title('conf matrix ROLE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affiliate 129\n",
      "agent 40\n",
      "counterpart 108\n",
      "guarantor 28\n",
      "insurer 47\n",
      "issuer 98\n",
      "seller 49\n",
      "servicer 57\n",
      "trustee 304\n",
      "underwriter 40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x7fde73627f98>], dtype=object)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6gAAAF1CAYAAADlfsfwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3X+4JGV95/33RyBKBhQRczLC6JhI3MXMBtkJ4mp2D2FX\nR0xEn01YfIiCYibrAxtNZjeOJtdKVnkeshtk/ZEYx2AGIook6sIKiSHoeYzZgIISENB11CHMODKK\nMDLijwx+94+u0WY8Z+b06V91ut+v6+qrq+6qrv7ed/XpOt+uu+5KVSFJkiRJ0rg9YtwBSJIkSZIE\nJqiSJEmSpJYwQZUkSZIktYIJqiRJkiSpFUxQJUmSJEmtYIIqSZIkSWoFE1RJkiRNpCRPTXJLkgeS\n/PoB1q0kT2mmNyd542iilNTNBFXaDw9skiQta78FfLSqDq+qt4w7GEkHdvC4A5Babu+B7fhxByJJ\nknr2JOCKcQeRJECq6nvjjkVqO8+gSvv3JOD2cQexFEn8AUqSNLWSfAQ4GXhbkt1JvpzkFV3Lz07y\n8T62/9gkH0ry1ST3NdPHdC2fS3JBkr8FHgR+IsmTk3ys6Zn110n+IMm7m/VXN72xXpbk7mab/z7J\nzya5Ncn9Sd7Wtf2fTPKRJPcm+VqSy5Mc0bXs60lOaOaf0MQ5u9T6SqNigiotYAQHtqOag9n9zUHk\nb5I8olm2KskHmoPJvXsPSEkekeR3ktyVZGeSy5I8plm298B2TpJ/AD7SlJ+U5H817/P3HpwkSdOg\nqn4e+BvgvKo6DPjfA36LRwB/QufH7CcC3wLets86LwHWA4cDdwHvAT4BPA44v1m+r2cAxwL/Dvjv\nwG8D/xp4GnB6kn/VrBfg/wOeAPxTYFWzTarqC8BrgHcn+dEmzkuraq6vGksjYIIqLWAEB7YNwDbg\n8cAM8DqgkhwEfIjOgWw1cDQ/6J50dvM4GfgJ4DB++GD4r+gcqJ6b5GjgGuCNwJHAfwTen+TxA66L\nJElTparurar3V9WDVfUAcAGdY3C3zVV1e1XtAVYCPwv856r6blV9HLh6nk2/oaq+XVV/BXwTeG9V\n7ayq7XT+L3l68/5bquq6qvpOVX0VeFP3+1fVO4EtwI3Ne//2IOsvDYsJqjQ+/0jngPGkqvrHqvqb\nqirgRDq/hv6nqvpmc5Dae6b2TOBNVfXFqtoNvBY4Y5/uvOc3r/sW8CvAtVV1bVV9r6quA24CTh1V\nJSVJmkRJfjTJO5peTd8APgYc0fzQvNfdXdNPAL5eVQ8usHyve7qmvzXP/GHN+88kuSLJ9ub93w0c\ntc+23gn8NPDWqvpOL/WTxsUEVRqf/0bnl82/SvLFJBub8lXAXc2vrft6Ap0zq3vdRWews5musu6D\n3ZOAX266996f5H7g2XQSY0mSpsk3gR/tmv/xPre3AXgq8IyqejTwL5vydK1TXdM7gCObLrd7rerj\n/f/fZvtrmvf/le73TnIYnS7ClwDnJzmyj/eSRsYEVVq8gR7YquqBqtpQVT8BvAD4zSSn0Ekwn7jA\nIEdfppN07vVEYA8P/3W1+2B4N/CnVXVE12NFVV3YT+ySJC1DtwD/V3Pm8ynAOX1u73A6ZzTvb5K/\n1+9v5aq6i04vpvOT/EiSZwK/2Of77wZ2NZf0/Kd9lr8ZuKmqXkHncp8/6uO9pJExQZUWb6AHtiS/\nkOQpzdDzu4CHgO/RGTxhB3BhkhVJHpXkWc3L3gv8RjMK4GF0fj193wJnW6HT3ecXkzw3yUHNtma7\nRxmUJGlKXAx8l86PupcCl/e5vf8OHAp8DbgB+MtFvOZM4JnAvXTGh3gfsNSut78LnEDnf4hrgA/s\nXZDkNGAd8Mqm6DeBE5KcucT3kkYmnUveJM0nyRzw7qr64yRH0Rl975nArcB1wL+uqmc36xZwbFVt\nSbIZ2FZVv7Ofbf8G8Co6gyTdB7yjqt7QLHsi8Bbg5+icEX1PVf16M8rv7wC/CjwK+DDwH6rqviSr\ngS8Bh3QnrEmeAfxXYA2dJPgTwCur6h/6byFJkrRUSd4HfLaq9nv2VZomJqiSJEnSCCT5WeDrdH5Q\nfg7wP4BnVtWnxxqY1CJ28ZUkSZLmkeR1zb3Q9338xRI3+ePAHJ1rR99Cp0eTyanUxTOo0hAleR2d\n+5vu62+q6nmjjkeSJElqMxNUSZIkSVIr2MVXkiRJktQK891nceSOOuqoWr16dd/b+eY3v8mKFSv6\nD2jC2C4Ls20WZtvMz3ZZ2CDa5uabb/5aVT1+QCGpBQZxjJ+mvzvrOpms62SalroOqp6LPca3IkFd\nvXo1N910U9/bmZubY3Z2tv+AJoztsjDbZmG2zfxsl4UNom2S3DWYaNQWgzjGT9PfnXWdTNZ1Mk1L\nXQdVz8Ue4+3iK0mSJElqBRNUSZIkSVIrmKBKkjTFkmxNcluSW5Lc1JQdmeS6JJ9vnh/blCfJW5Js\nSXJrkhPGG70kadKYoEqSpJOr6viqWtvMbwSur6pjgeubeYDnAcc2j/XA20ceqSRporVikCRJ2p/V\nG68Zdwjft3nd5I/WJwGnAbPN9KXAHPCapvyy6txE/YYkRyRZWVU7xhKlpJ4N+5i6Yc0ezl7ke2y9\n8PlDjUXLk2dQJUmabgX8VZKbk6xvyma6ks6vADPN9NHA3V2v3daUSZI0EJ5BlSRpuj27qrYn+THg\nuiSf7V5YVZWketlgk+iuB5iZmWFubq6vAHfv3t33NpYL6zqZ2lTXDWv2DHX7M4cu/j3a0iZL1ab9\nOkyjrqcJqiRJU6yqtjfPO5N8EDgRuGdv190kK4GdzerbgVVdLz+mKdt3m5uATQBr166tfu+fNy33\nGgTrOqnaVNfFdr9dqg1r9nDRbYtLMbaeOTvUWIatTft1mEZdT7v4SpI0pZKsSHL43mngOcBngKuB\ns5rVzgKuaqavBl7ajOZ7ErDL608lSYPkGVRJkqbXDPDBJND5n+A9VfWXST4JXJnkHOAu4PRm/WuB\nU4EtwIPAy0YfsiRpkh0wQU2yCriMzkGsgE1V9eYk5wO/Cny1WfV1VXVt85rXAucADwG/XlUfHkLs\nkiSpD1X1ReBn5im/FzhlnvICzh1BaJKkKbWYM6h7gA1V9ammG9DNSa5rll1cVb/fvXKS44AzgKcB\nTwD+OslPVdVDgwxckiRJkjRZDngNalXtqKpPNdMPAHey/yHlTwOuqKrvVNWX6HQDOnEQwUqSJEmS\nJldPgyQlWQ08HbixKTovya1J3pXksU2Z90iTJEmSJPVs0YMkJTkMeD/w6qr6RpK3A2+gc13qG4CL\ngJf3sL2B3iMNpudeRL2yXRZm2yysTW0z7Hu29aJN7dI2to0kSerXohLUJIfQSU4vr6oPAFTVPV3L\n3wl8qJkdyz3SYHruRdQr22Vhts3C2tQ2w75nWy82r1vRmnZpmzZ9ZiRJ0vJ0wC6+6Yw9fwlwZ1W9\nqat8ZddqL6Jz3zTo3CPtjCSPTPJk4FjgE4MLWZIkSZI0iRZzBvVZwEuA25Lc0pS9DnhxkuPpdPHd\nCvwaQFXdnuRK4A46IwCf6wi+kiRJkqQDOWCCWlUfBzLPomv385oLgAv6iEuSJEmSNGV6GsVXkiRJ\nkqRhMUGVJEmSJLWCCaokSZIkqRVMUCVJkiRJrWCCKkmSJElqBRNUSZIkSVIrmKBKkiRJklrBBFWS\nJEmS1AomqJIkSZKkVjBBlSRJkiS1ggmqJElTLslBST6d5EPN/JOT3JhkS5L3JfmRpvyRzfyWZvnq\nccYtSZo8JqiSJOlVwJ1d878HXFxVTwHuA85pys8B7mvKL27WkyRpYExQJUmaYkmOAZ4P/HEzH+Dn\ngT9vVrkUeGEzfVozT7P8lGZ9SZIG4uBxByBJksbqvwO/BRzezD8OuL+q9jTz24Cjm+mjgbsBqmpP\nkl3N+l/r3mCS9cB6gJmZGebm5voKcPfu3X1vY7mwrpOpTXXdsGbPgVfqw8yhi3+PtrTJUrVpvw7T\nqOtpgipJ0pRK8gvAzqq6OcnsoLZbVZuATQBr166t2dn+Nj03N0e/21gurOtkalNdz954zVC3v2HN\nHi66bXEpxtYzZ4cay7C1ab8O06jraYIqSdL0ehbwgiSnAo8CHg28GTgiycHNWdRjgO3N+tuBVcC2\nJAcDjwHuHX3YkqRJ5TWokiRNqap6bVUdU1WrgTOAj1TVmcBHgV9qVjsLuKqZvrqZp1n+kaqqEYYs\nSZpwB0xQk6xK8tEkdyS5PcmrmvIjk1yX5PPN82Ob8iR5SzME/a1JThh2JSRJ0kC9BvjNJFvoXGN6\nSVN+CfC4pvw3gY1jik+SNKEW08V3D7Chqj6V5HDg5iTXAWcD11fVhUk20jlIvQZ4HnBs83gG8Pbm\nWZIktVRVzQFzzfQXgRPnWefbwC+PNDBJ0lQ54BnUqtpRVZ9qph+gc5+0o3n4UPP7DkF/WXXcQOc6\nlpUDj1ySJEmSNFF6GiQpyWrg6cCNwExV7WgWfQWYaaa/PwR9Y+/w9Du6ygY+BD1Mz1DPvbJdFmbb\nLKxNbTPsIfF70aZ2aRvbRpIk9WvRCWqSw4D3A6+uqm9035e7qipJT4MkDHoIepieoZ57ZbsszLZZ\nWJvaZthD4vdiw5o9XPTxb447DAC2Xvj8cYfwMG36zEiSpOVpUaP4JjmETnJ6eVV9oCm+Z2/X3eZ5\nZ1O+dwj6vbqHp5ckSZIkaV6LGcU3dEbtu7Oq3tS1qHuo+X2HoH9pM5rvScCurq7AkiRJkiTNazFd\nfJ8FvAS4LcktTdnrgAuBK5OcA9wFnN4suxY4FdgCPAi8bKARS5IkSZIm0gET1Kr6OJAFFp8yz/oF\nnNtnXJIkSZKkKbOoa1AlSZIkSRo2E1RJkiRJUiuYoEqSJEmSWsEEVZIkSZLUCiaokiRJkqRWMEGV\nJEmSJLWCCaokSZIkqRVMUCVJkiRJrWCCKkmSJElqBRNUSZIkSVIrmKBKkiRJklrBBFWSpCmV5FFJ\nPpHk75PcnuR3m/InJ7kxyZYk70vyI035I5v5Lc3y1eOMX5I0eUxQJUmaXt8Bfr6qfgY4HliX5CTg\n94CLq+opwH3AOc365wD3NeUXN+tJkjQwJqiSJE2p6tjdzB7SPAr4eeDPm/JLgRc206c18zTLT0mS\nEYUrSZoCB487AEmSND5JDgJuBp4C/AHwBeD+qtrTrLINOLqZPhq4G6Cq9iTZBTwO+No+21wPrAeY\nmZlhbm6urxh3797d9zaWC+s6mdpU1w1r9hx4pT7MHLr492hLmyxVm/brMI26niaokiRNsap6CDg+\nyRHAB4F/MoBtbgI2Aaxdu7ZmZ2f72t7c3Bz9bmO5sK6TqU11PXvjNUPd/oY1e7jotsWlGFvPnB1q\nLMPWpv06TKOu5wG7+CZ5V5KdST7TVXZ+ku1Jbmkep3Yte20zeMLnkjx3WIFLkqTBqar7gY8CzwSO\nSLL3P8xjgO3N9HZgFUCz/DHAvSMOVZI0wRZzDepmYN085RdX1fHN41qAJMcBZwBPa17zh03XIUmS\n1DJJHt+cOSXJocC/Ae6kk6j+UrPaWcBVzfTVzTzN8o9UVY0uYknSpDvg+feq+lgPw8ifBlxRVd8B\nvpRkC3Ai8HdLjlCSJA3LSuDS5sfkRwBXVtWHktwBXJHkjcCngUua9S8B/rQ5vn+dzo/SkiQNTD/X\noJ6X5KXATcCGqrqPzuAJN3St0z2wwsMMegAFmJ4LlXtluyzMtllYm9pm2AM69KKXwR+GrS37Z682\nfWa0OFV1K/D0ecq/SOcH5n3Lvw388ghCkyRNqaUmqG8H3kBnKPo3ABcBL+9lA4MeQAGm50LlXtku\nC7NtFtamthn2gA696GXwh2Fr2+ASbfrMSJKk5WlJ90Gtqnuq6qGq+h7wTn7wK+v3B09odA+sIEmS\nJEnSgpaUoCZZ2TX7ImDvCL9XA2ckeWSSJwPHAp/oL0RJkiRJ0jQ4YD+1JO8FZoGjkmwDXg/MJjme\nThffrcCvAVTV7UmuBO4A9gDnNvdXkyRJkiRpvxYziu+L5ym+ZJ6yvetfAFzQT1CSJEmSpOmzpC6+\nkiRJkiQNmgmqJEmSJKkVTFAlSZIkSa1ggipJkiRJaoV23G1eU2X1xmvGHcL3bV63YtwhSEvWpr8l\n8O9JkiT1zzOokiRJkqRWMEGVJEmSJLWCCaokSZIkqRVMUCVJkiRJrWCCKkmSJElqBRNUSZIkSVIr\nmKBKkiRJklrBBFWSJEmS1AomqJIkTakkq5J8NMkdSW5P8qqm/Mgk1yX5fPP82KY8Sd6SZEuSW5Oc\nMN4aSJImjQmqJEnTaw+woaqOA04Czk1yHLARuL6qjgWub+YBngcc2zzWA28ffciSpElmgipJ0pSq\nqh1V9alm+gHgTuBo4DTg0ma1S4EXNtOnAZdVxw3AEUlWjjhsSdIEO/hAKyR5F/ALwM6q+umm7Ejg\nfcBqYCtwelXdlyTAm4FTgQeBs/ce+CRJUnslWQ08HbgRmKmqHc2irwAzzfTRwN1dL9vWlO3oKiPJ\nejpnWJmZmWFubq6v2Hbv3t33NpaL5VzX27bv6mn9mUPhrZdfNaRoYM3RjxnatnvVpv26Yc2eoW5/\n5tDFv0db2mSp2rRfh2nU9TxgggpsBt4GXNZVtrfrz4VJNjbzr+HhXX+eQafrzzMGGbAkSRqsJIcB\n7wdeXVXf6Pze3FFVlaR62V5VbQI2Aaxdu7ZmZ2f7im9ubo5+t7FcLOe6nr3xmp7W37BmDxfdtph/\nRZdm65mzQ9t2r9q0X3vdT73qZb+2aR8tRZv26zCNup4H7OJbVR8Dvr5PsV1/JEmaAEkOoZOcXl5V\nH2iK79l7/G6edzbl24FVXS8/pimTJGkglvqzVV9df2Dw3X9gek6z96pt7TLsriW9aFvbtEmb2qZN\nn5leui5NmzZ9ZrQ4zaU5lwB3VtWbuhZdDZwFXNg8X9VVfl6SK+j0kNrV9f+AJEl967tfxVK6/jSv\nG2j3H5ie0+y9alu7DLtrSS82r1vRqrZpkzZ9btr0mRl2l7TlzL+nZelZwEuA25Lc0pS9jk5iemWS\nc4C7gNObZdfSGWdiC52xJl422nAlSZNuqf9l3ZNkZVXtsOuPJEnLU1V9HMgCi0+ZZ/0Czh1qUJKk\nqbbU28zs7foDP9z156XNjbxPwq4/kiRJkqRFWsxtZt4LzAJHJdkGvB67/kiSJEmSBuyACWpVvXiB\nRXb9kSRJkiQNzFK7+EqSJEmSNFAmqJIkSZKkVjBBlSRJkiS1ggmqJEmSJKkVTFAlSZIkSa1ggipJ\nkiRJagUTVEmSJElSK5igSpIkSZJawQRVkiRJktQKB487gEG6bfsuzt54zbjDAGDrhc8fdwiSJEmS\ntKx4BlWSJEmS1AoTdQZVkiRJkpa71S3pFQqwed2Kkb6fZ1AlSZIkSa1ggipJkiRJagUTVEmSplSS\ndyXZmeQzXWVHJrkuyeeb58c25UnyliRbktya5ITxRS5JmlQmqJIkTa/NwLp9yjYC11fVscD1zTzA\n84Bjm8d64O0jilGSNEX6SlCTbE1yW5JbktzUlM37y6skSWqXqvoY8PV9ik8DLm2mLwVe2FV+WXXc\nAByRZOVoIpUkTYtBnEE9uaqOr6q1zfxCv7xKkqT2m6mqHc30V4CZZvpo4O6u9bY1ZZIkDcwwbjNz\nGjDbTF8KzAGvGcL7SJKkIaqqSlK9vi7JejrdgJmZmWFubq6vOHbv3t33NpaL5VzXDWv29LT+zKG9\nv6YXbWrHNu3XYbY59LZf29ImSzXM/Trs/dSLUX9+U9XzcecHL06+BNwHFPCOqtqU5P6qOqJZHuC+\nvfP7vLb74PXPr7jiiiXHsdfOr+/inm/1vZmBWHP0Y8Ydwvft3r2bww47bNxhfN9t23eNO4Tve/Jj\nDmpV27RJmz43bfrMzBxKa75n2mYQf08nn3zyzV09cjQCSVYDH6qqn27mPwfMVtWOpgvvXFU9Nck7\nmun37rve/ra/du3auummm/qKcW5ujtnZ2b62sVws57r2et/GDWv2cNFtwzhX0rH1wucPbdu9atN+\nHfb9NXvZr23aR0sxzP3atvugDqKeSRZ1jO/3W+HZVbU9yY8B1yX5bPfC/f3yWlWbgE3QOXgNotJv\nvfyqoX7R9WLrmbPjDuH72vSlCHD2BP7BTaI2fW7a9JkZ9j9Uy5l/TxPjauAs4MLm+aqu8vOSXAE8\nA9h1oORUkqRe9fVfVlVtb553JvkgcCJwT5KVXb+87hxAnJIkacCSvJfOZTlHJdkGvJ5OYnplknOA\nu4DTm9WvBU4FtgAPAi8becCSpIm35AQ1yQrgEVX1QDP9HOC/sPAvr5IkqUWq6sULLDplnnULOHe4\nEUmSpl0/Z1BngA92LjPlYOA9VfWXST7J/L+8SpIkSZK0oCUnqFX1ReBn5im/l3l+eZUkSZIkaX8G\ncR9USZIkSZL65lCUkiSp1W7bvqtVo3kv91tjSFKbeQZVkiRJktQKnkGdAm375VmSJEmS5uMZVEmS\nJElSK3gGVVOtTWeXvaZJkiRJ084EVWqJ1S1JlPfavG7FuEOQJEnSlLGLryRJkiSpFUxQJUmSJEmt\nYIIqSZIkSWoFE1RJkiRJUiuYoEqSJEmSWsFRfCXNq0234JEkSdJ08AyqJEmSJKkVTFAlSZIkSa1g\ngipJkiRJaoWhJahJ1iX5XJItSTYO630kSdLoeHyXJA3TUBLUJAcBfwA8DzgOeHGS44bxXpIkaTQ8\nvkuShm1YZ1BPBLZU1Rer6rvAFcBpQ3ovSZI0Gh7fJUlDlaoa/EaTXwLWVdUrmvmXAM+oqvO61lkP\nrG9mnwp8bgBvfRTwtQFsZ9LYLguzbRZm28zPdlnYINrmSVX1+EEEo8FbzPG9KR/0MX6a/u6s62Sy\nrpNpWuo6qHou6hg/tvugVtUmYNMgt5nkpqpaO8htTgLbZWG2zcJsm/nZLguzbbTXoI/x0/TZsq6T\nybpOpmmp66jrOawuvtuBVV3zxzRlkiRp+fL4LkkaqmElqJ8Ejk3y5CQ/ApwBXD2k95IkSaPh8V2S\nNFRD6eJbVXuSnAd8GDgIeFdV3T6M99rHQLsMTxDbZWG2zcJsm/nZLguzbSacx/eRsK6TybpOpmmp\n60jrOZRBkiRJkiRJ6tWwuvhKkiRJktQTE1RJkiRJUissywQ1ybokn0uyJcnGeZY/Msn7muU3Jlk9\n+ihHbxHt8ptJ7khya5LrkzxpHHGOw4Hapmu9f5ukkkz8kOGwuHZJcnrzubk9yXtGHeO4LOLv6YlJ\nPprk083f1KnjiHPUkrwryc4kn1lgeZK8pWm3W5OcMOoYtXxN0/F9EXU9O8lXk9zSPF4xjjj7NU3f\nGYuo62ySXV379D+POsZBSbKqOQbu/f/gVfOss+z37SLrORH7Ncmjknwiyd83df3dedYZzXdwVS2r\nB51BGb4A/ATwI8DfA8fts87/A/xRM30G8L5xx92SdjkZ+NFm+pXT0C6LbZtmvcOBjwE3AGvHHXcb\n2gU4Fvg08Nhm/sfGHXeL2mYT8Mpm+jhg67jjHlHb/EvgBOAzCyw/FfgLIMBJwI3jjtnH8nhM0/F9\nkXU9G3jbuGMdQF2n5jtjEXWdBT407jgHVNeVwAnN9OHA/57nM7zs9+0i6zkR+7XZT4c104cANwIn\n7bPOSL6Dl+MZ1BOBLVX1xar6LnAFcNo+65wGXNpM/zlwSpKMMMZxOGC7VNVHq+rBZvYGOvevmwaL\n+cwAvAH4PeDbowxujBbTLr8K/EFV3QdQVTtHHOO4LKZtCnh0M/0Y4MsjjG9squpjwNf3s8ppwGXV\ncQNwRJKVo4lOy9w0Hd8Xe1xa9qbpO2MRdZ0YVbWjqj7VTD8A3Akcvc9qy37fLrKeE6HZT7ub2UOa\nx76j6Y7kO3g5JqhHA3d3zW/jhz8o31+nqvYAu4DHjSS68VlMu3Q7h86vWtPggG3TdDtZVVXXjDKw\nMVvMZ+angJ9K8rdJbkiybmTRjddi2uZ84FeSbAOuBf7DaEJrvV6/i6S9pun4vti/k3/bdI388ySr\nRhPayE3bd8Yzmy6Uf5HkaeMOZhCabp5Pp3PGrdtE7dv91BMmZL8mOSjJLcBO4LqqWnCfDvM7eDkm\nqOpTkl8B1gL/bdyxtEGSRwBvAjaMO5YWOphON99Z4MXAO5McMdaI2uPFwOaqOoZON6Y/bT5LkjQI\n/xNYXVX/DLiOH5y10PL1KeBJVfUzwFuB/zHmePqW5DDg/cCrq+ob445nWA5Qz4nZr1X1UFUdT6eX\n5YlJfnoccSzHf6a2A92/Ih7TlM27TpKD6XS/u3ck0Y3PYtqFJP8a+G3gBVX1nRHFNm4HapvDgZ8G\n5pJspXOdxNWZ/IGSFvOZ2QZcXVX/WFVfonPtxbEjim+cFtM25wBXAlTV3wGPAo4aSXTttqjvImke\n03R8P2Bdq+reruP0HwP/fESxjdrUfGdU1Tf2dqGsqmuBQ5Is2+NGkkPoJG2XV9UH5lllIvbtgeo5\nafsVoKruBz4K7NtzbiTfwcsxQf0kcGySJyf5EToX6F69zzpXA2c1078EfKSaq3kn2AHbJcnTgXfQ\nSU6n5VpCOEDbVNWuqjqqqlZX1Wo61+e+oKpuGk+4I7OYv6X/QefsKc2X7U8BXxxlkGOymLb5B+AU\ngCT/lE6C+tWRRtlOVwMvbUZvPAnYVVU7xh2UloVpOr4v5pjdfa3eC+hc+zaJpuY7I8mP771eL8mJ\ndP4PX44/sNDU4xLgzqp60wKrLft9u5h6Tsp+TfL4vb3kkhwK/Bvgs/usNpLv4IMHvcFhq6o9Sc4D\nPkxnFLx3VdXtSf4LcFNVXU3ng/SnSbbQuVj9jPFFPBqLbJf/BhwG/Fnzd/QPVfWCsQU9Iotsm6mz\nyHb5MPCcJHcADwH/qaqW3ZdurxbZNhvodHn+DTqDCJy9TP9R7kmS99L50eKo5vrb19MZSIGq+iM6\n1+OeCmwBHgReNp5ItdxM0/F9kXX99SQvAPbQqevZYwu4D9P0nbGIuv4S8Moke4BvAWcs4+PGs4CX\nALc11ywCvA54IkzUvl1MPSdlv64ELk1yEJ0k+8qq+tA4voOzPNtPkiRJkjRplmMXX0mSJEnSBDJB\nlSRJkiTYGMMAAAAgAElEQVS1ggmqJEmSJKkVTFAlSZIkSa1ggipJkiRJagUTVEmSJElSK5igSpIk\nSZJawQRVkiRJktQKJqiSJEmSpFYwQZUkSZIktYIJqiRJkiSpFUxQJUmSNJWSbE7yxnHHIekHTFCl\nPnlwkyRJkgbDBFWSJEkTKcnB446hW9vikdrIBFU6gOV+MEmHf+uSpKmQZGuS1yS5FfhmkjVJ5pLc\nn+T2JC/Yz2t/Icktzbr/K8k/W8T7bUzyhSQPJLkjyYu6lp2d5G+TXJzkXuD8JAcluSjJ15J8Kcl5\nSWrv/xtNrG9s3n93kv+Z5HFJLk/yjSSfTLK66z3enOTuZtnNSX6ua9m1SS7qmr8iybt6bVNplPyn\nVZrHGA5ur0myvTm4fS7JKU35QUle13XguznJqmbZv2gOUrua53/Rtb25JBck+VvgQeAnkjwmySVJ\ndjTv9cYkB/XdWJIktc+LgecDRwEfBP4K+DHgPwCXJ3nqvi9I8nTgXcCvAY8D3gFcneSRB3ivLwA/\nBzwG+F3g3UlWdi1/BvBFYAa4APhV4HnA8cAJwAvn2eYZwEuAo4GfBP4O+BPgSOBO4PVd636y2daR\nwHuAP0vyqGbZy4GXJPn5JGcCJwKvOkB9pLEyQZUWNpKDW7Od84CfrarDgecCW5vFv9nEcSrwaDoH\nmgeTHAlcA7yleZ83AdckeVzXpl8CrAcOB+4CNgN7gKcATweeA7yih/aQJGm5eEtV3U0ncTsMuLCq\nvltVHwE+ROfYuq/1wDuq6saqeqiqLgW+A5y0vzeqqj+rqi9X1feq6n3A5+kkgnt9uareWlV7qupb\nwOnAm6tqW1XdB1w4z2b/pKq+UFW7gL8AvlBVf11Ve4A/o3Mc3/v+766qe5vtXwQ8Enhqs+wrwCuB\nS4E3Ay+tqgf233TSeJmgSgsb1cHtIToHk+OSHFJVW6vqC82yVwC/U1Wfq46/r6p76STOn6+qP20O\nSO8FPgv8Ytd2N1fV7c3B7Eg6Se6rq+qbVbUTuJjOL7SSJE2au5vnJwB3V9X3upbdRefM5L6eBGxo\nekDdn+R+YFWzjQUleWlXz6n7gZ+m8+P2vrHs9YR9yvZdDnBP1/S35pk/rOv9/2OSO5seVffTOZPb\n/f7/EzgI+FxVfXx/dZHawARVWthIDm5VtQV4NXA+sLO5PmTv+qvodB3a1xOaGLrtG1P3Ae9JwCHA\njq643kHnjLAkSZOmmucvA6v2GYvhicD2eV5zN3BBVR3R9fjR5kfgeSV5EvBOOj2hHldVRwCfATJP\nLHvtAI7pml+1qBrN//4/B/wWnbOyj23ef9c+738BnW7BK5PM9+O61ComqNLCRnJwA6iq91TVs+kk\nkgX8Xtf2fnKel3y5WbfbvjF1HxDvpnMm96iuuB5dVU/bX1ySJC1zN9IZi+G3khySZJZOb6Mr5ln3\nncC/T/KMdKxI8vwkh+9n+yvoHG+/CpDkZXTOoO7PlcCrkhyd5AjgNb1V6WEOp3P5zleBg5P8ZzqX\nBNHE8y+BlwEvBc4C3ppkvh/YpdYwQZUObKgHtyRPbQYveCTwbTpdd/aerf1j4A1Jjm2298+a60yv\nBX4qyf+d5OAk/w44jk7X4x9SVTvoXEN7UZJHJ3lEkp9M8q+W0iCSJC0HVfVdOsfs5wFfA/6QznWY\nn51n3ZvoDGD0NuA+YAtw9gG2fwdwEZ1BjO4B1gB/e4Cw3knnmHwr8Gk6x/Q9dC756dWHgb8E/jed\nnlTfpulBleTRwGXAeVW1var+BrgE+JMkWWB70tilat9eB5KSbAVeUVV/3cw/jc5B7Xg6Zyl/u6o+\n2CzbDGyrqt9p5tcBbwCOpZNsfhx4+UKDEjSj/P4x8E+BfwT+F7C+qr7cjLL7WuAcOteTfBZ4UVVt\nS/JsOgMePIXOQfRVe68tSTIHvLuq/rjrfR5DZyCGX6Tzi+sXgd+rqvkSbUmSNAJJngf8UVXt2zNK\nmkomqJIkSdKIJDkUOJnOWdQZ4P3ADVX16rEGJrWECaokSZK0gCRPBO5YYPFxVfUPPW7vR4H/H/gn\ndHpaXUOnF9Q3+gpUmhAmqNIIDPrgJkmDkORRwMfo3OrqYODPq+r1SZ5M5zr7xwE3Ay+pqu8218pf\nBvxz4F7g31XV1rEEL0maSA6SJI1AVf1DVR22wMPkVNK4fAf4+ar6GTrX2K9LchKdkcQvrqqn0Bks\n5pxm/XOA+5ryi/nBiOOSJA1EK86gHnXUUbV69eq+t/PNb36TFStW9B/QhLFdFmbbLMy2mZ/tsrBB\ntM3NN9/8tap6/IBCUg+abocfB15Jp8vhj1fVniTPBM6vqucm+XAz/XdJDga+Ajy+9vPPxCCO8dP0\nd2ddJ5N1nUzTUtdB1XOxx/iD+36nAVi9ejU33XRT39uZm5tjdna2/4AmjO2yMNtmYbbN/GyXhQ2i\nbZLcNZhotFjNaOE30xkR/A+ALwD3V9WeZpVtwN77Jh5NcwuLJnndRacb8Nf22eZ6YD3AzMwMv//7\nv99XjLt37+awww7raxvLhXWdTNZ1Mk1LXQdVz5NPPnlRx/hWJKiSJGk8quoh4PgkRwAfpDNwS7/b\n3ARsAli7dm31+8PFNP0wZF0nk3WdTNNS11HX02tQJUkSVXU/8FHgmcARTRdegGPo3P+Z5nkVQLP8\nMXQGS5IkaSBMUCVJmlJJHt+cOd17b8Z/A9xJJ1H9pWa1s4Crmumrm3ma5R/Z3/WnkiT1yi6+kiRN\nr5XApc11qI8ArqyqDyW5A7giyRuBTwOXNOtfAvxpki3A14EzxhG0JGlymaBKkjSlqupW4OnzlH8R\nOHGe8m8DvzyC0CRJU8oEVZKWqdUbrxl3CA+zed3kD7UvScvdsI8dG9bs4exFvsfWC58/1Fi0PHkN\nqiRJkiSpFQ6YoCZZleSjSe5IcnuSVzXlRya5Lsnnm+fHNuVJ8pYkW5LcmuSEYVdCkiRJkrT8LeYM\n6h5gQ1UdB5wEnJvkOGAjcH1VHQtc38wDPA84tnmsB94+8KglSZIkSRPngAlqVe2oqk810w/QGX7+\naOA04NJmtUuBFzbTpwGXVccNdO6ltnLgkUuSJEmSJkpPgyQlWU1ntL8bgZmq2tEs+gow00wfDdzd\n9bJtTdkOJEmSJEn71aaBEEc9COKiE9QkhwHvB15dVd9I8v1lVVVJerpRd5L1dLoAMzMzw9zcXC8v\nn9fu3bsHsp1JY7sszLZZmG0zvza1y4Y1e8YdwsO0qW0kSdLytKgENckhdJLTy6vqA03xPUlWVtWO\npgvvzqZ8O7Cq6+XHNGUPU1WbgE0Aa9eurdnZ2aXVoMvc3ByD2M6ksV0WZtsszLaZX5vaZbHD+I/K\n5nUrWtM2kiRpeVrMKL4BLgHurKo3dS26GjirmT4LuKqr/KXNaL4nAbu6ugJLkiRJkjSvxZxBfRbw\nEuC2JLc0Za8DLgSuTHIOcBdwerPsWuBUYAvwIPCygUYsSZIkSZpIB0xQq+rjQBZYfMo86xdwbp9x\nSZIkSZKmTE+j+EqSJEnz6XXU0Q1r9gz1WvqtFz5/aNuWNDwHvAZVkiRJkqRRMEGVJEmSJLWCCaok\nSVMqyaokH01yR5Lbk7yqKT8/yfYktzSPU7te89okW5J8Lslzxxe9JGkSeQ2qJEnTaw+woao+leRw\n4OYk1zXLLq6q3+9eOclxwBnA04AnAH+d5Keq6qGRRi1JmlieQZUkaUpV1Y6q+lQz/QBwJ3D0fl5y\nGnBFVX2nqr5E55ZyJw4/UknStDBBlSRJJFkNPB24sSk6L8mtSd6V5LFN2dHA3V0v28b+E1pJknpi\nF19JkqZcksOA9wOvrqpvJHk78AagmueLgJf3sL31wHqAmZkZ5ubm+opv9+7dfW9juVjOdd2wZk9P\n688c2vtretGmdmzTfh1mm0Nv+7UtbbJUw9yvw95PvRj159cEVZKkKZbkEDrJ6eVV9QGAqrqna/k7\ngQ81s9uBVV0vP6Ype5iq2gRsAli7dm3Nzs72FePc3Bz9bmO5WM517fWephvW7OGi24b3r+jWM2eH\ntu1etWm/DvPes9Dbfm3TPlqKYe7XYe+nXmxet2Kkn1+7+EqSNKWSBLgEuLOq3tRVvrJrtRcBn2mm\nrwbOSPLIJE8GjgU+Map4JUmTzzOokiRNr2cBLwFuS3JLU/Y64MVJjqfTxXcr8GsAVXV7kiuBO+iM\nAHyuI/hKkgbJBFWSpClVVR8HMs+ia/fzmguAC4YWlCRpqtnFV5IkSZLUCiaokiRJkqRWMEGVJEmS\nJLWCCaokSZIkqRVMUCVJkiRJrWCCKkmSJElqBRNUSZIkSVIrmKBKkiRJklrBBFWSJEmS1AomqJIk\nSZKkVjBBlSRJkiS1wgET1CTvSrIzyWe6ys5Psj3JLc3j1K5lr02yJcnnkjx3WIFLkiRJkibLYs6g\nbgbWzVN+cVUd3zyuBUhyHHAG8LTmNX+Y5KBBBStJkiRJmlwHTFCr6mPA1xe5vdOAK6rqO1X1JWAL\ncGIf8UmSJEmSpkQ/16Cel+TWpgvwY5uyo4G7u9bZ1pRJkiRJkrRfBy/xdW8H3gBU83wR8PJeNpBk\nPbAeYGZmhrm5uSWG8gO7d+8eyHYmje2yMNtmYbbN/NrULhvW7Bl3CA/TprbR4iRZBVwGzNA5pm+q\nqjcnORJ4H7Aa2AqcXlX3JQnwZuBU4EHg7Kr61DhilyRNpiUlqFV1z97pJO8EPtTMbgdWda16TFM2\n3zY2AZsA1q5dW7Ozs0sJ5WHm5uYYxHYmje2yMNtmYbbN/NrULmdvvGbcITzM5nUrWtM2WrQ9wIaq\n+lSSw4Gbk1wHnA1cX1UXJtkIbAReAzwPOLZ5PIPOD9bPGEvkkqSJtKQuvklWds2+CNg7wu/VwBlJ\nHpnkyXQOYJ/oL0RJkjQMVbVj7xnQqnoAuJPOpTmnAZc2q10KvLCZPg24rDpuAI7Y538CSZL6csAz\nqEneC8wCRyXZBrwemE1yPJ3uQFuBXwOoqtuTXAncQedX2XOr6qHhhC5JkgYlyWrg6cCNwExV7WgW\nfYVOF2BYeKyJHV1lA7+MZ5q6jy/nuvZ62cHMocO9VKFN7dim/Trsy0N62a9taZOlGuZ+bdNlPKP+\n/B4wQa2qF89TfMl+1r8AuKCfoCRJ0ugkOQx4P/DqqvpG51LTjqqqJNXL9gZ9GU+butYP23Kua6+X\nHWxYs4eLblvqcCgHtvXM2aFtu1dt2q/Dvjykl/3apn20FMPcr226jGfUl/D0M4qvJEla5pIcQic5\nvbyqPtAU37O3627zvLMpX/RYE5IkLYUJqiRJU6oZlfcS4M6qelPXoquBs5rps4Cruspfmo6TgF1d\nXYElSerb8PpVSJKktnsW8BLgtiS3NGWvAy4ErkxyDnAXcHqz7Fo6t5jZQuc2My8bbbiSpElngipJ\n0pSqqo8DWWDxKfOsX8C5Qw1KkjTV7OIrSZIkSWoFE1RJkiRJUiuYoEqSJEmSWsFrUDVyq1t2XydJ\nkiRJ7eAZVEmSJElSK5igSpIkSZJaYaK6+N62fRdnt6T76NYLnz/uECRJkiRpWfEMqiRJkiSpFUxQ\nJUmSJEmtYIIqSZIkSWoFE1RJkiRJUiuYoEqSJEmSWsEEVZIkSZLUCiaokiRJkqRWMEGVJGlKJXlX\nkp1JPtNVdn6S7UluaR6ndi17bZItST6X5LnjiVqSNMlMUCVJml6bgXXzlF9cVcc3j2sBkhwHnAE8\nrXnNHyY5aGSRSpKmggmqJElTqqo+Bnx9kaufBlxRVd+pqi8BW4AThxacJGkqHTzuACRJUuucl+Sl\nwE3Ahqq6DzgauKFrnW1N2Q9Jsh5YDzAzM8Pc3FxfwezevbvvbSwXy7muG9bs6Wn9mUN7f00v2tSO\nbdqvw2xz6G2/tqVNlmqY+3XY+6kXo/78mqBKkqRubwfeAFTzfBHw8l42UFWbgE0Aa9eurdnZ2b4C\nmpubo99tLBfLua5nb7ymp/U3rNnDRbcN71/RrWfODm3bvWrTfu11P/Wql/3apn20FMPcr8PeT73Y\nvG7FSD+/dvGVJEnfV1X3VNVDVfU94J38oBvvdmBV16rHNGWSJA3MARPUBUb4OzLJdUk+3zw/tilP\nkrc0I/zdmuSEYQYvSZIGK8nKrtkXAXuP/1cDZyR5ZJInA8cCnxh1fJKkybaYM6ib+eER/jYC11fV\nscD1zTzA8+gcsI6lc+3J2wcTpiRJGrQk7wX+Dnhqkm1JzgH+a5LbktwKnAz8BkBV3Q5cCdwB/CVw\nblU9NKbQJUkT6oAdxKvqY0lW71N8GjDbTF8KzAGvacovq6oCbkhyRJKVVbVjUAFLkqTBqKoXz1N8\nyX7WvwC4YHgRSZKm3VKvTJ/pSjq/Asw000cDd3ett3eEvx9KUAc9wh8MfzS4XrRpVLKdX9/FWy+/\natxhfN+GNeOO4AfaNKpe29g282tTu7Tl+26vNrWNJElanvoeOq2qKkkt4XUDHeEP4K2XXzXU0eB6\n0aZRydrULm0z6lHJlpM2jTjYJm1qlzaN8Af+PUmSpP4tdRTfe/YOotA872zKHeFPkiRJkrQkS01Q\nrwbOaqbPAq7qKn9pM5rvScAurz+VJEmSJC3GAft9NiP8zQJHJdkGvB64ELiyGe3vLuD0ZvVrgVOB\nLcCDwMuGELMkSZIkaQItZhTf+Ub4AzhlnnULOLffoCRJkiRJ02epXXwlSZIkSRooE1RJkiRJUiuY\noEqSJEmSWsEEVZIkSZLUCiaokiRJkqRWOOAovpJGY/XGa8YdwsNsXrdi3CFIkiRpyngGVZIkSZLU\nCiaokiRJkqRWMEGVJGlKJXlXkp1JPtNVdmSS65J8vnl+bFOeJG9JsiXJrUlOGF/kkqRJ5TWoQ9Km\n6wk3rBl3BJKkltoMvA24rKtsI3B9VV2YZGMz/xrgecCxzeMZwNubZ0mSBsYzqJIkTamq+hjw9X2K\nTwMubaYvBV7YVX5ZddwAHJFk5WgilSRNCxNUSZLUbaaqdjTTXwFmmumjgbu71tvWlEmSNDB28ZUk\nSfOqqkpSvb4uyXpgPcDMzAxzc3N9xbF79+6+t7FcLOe6blizp6f1Zw7t/TW9aFM7tmm/DrPNobf9\n2pY2Waph7tdh76dejPrza4IqSZK63ZNkZVXtaLrw7mzKtwOrutY7pin7IVW1CdgEsHbt2pqdne0r\noLm5OfrdxnKxnOt6do/jb2xYs4eLbhvev6Jbz5wd2rZ71ab92ut+6lUv+7VN+2gphrlfh72ferF5\n3YqRfn7t4itJkrpdDZzVTJ8FXNVV/tJmNN+TgF1dXYElSRoIz6BKkjSlkrwXmAWOSrINeD1wIXBl\nknOAu4DTm9WvBU4FtgAPAi8becCSpIlngipJ0pSqqhcvsOiUedYt4NzhRiRJmnZ28ZUkSZIktYIJ\nqiRJkiSpFUxQJUmSJEmtYIIqSZIkSWoFE1RJkiRJUiuYoEqSJEmSWqGv28wk2Qo8ADwE7KmqtUmO\nBN4HrAa2AqdX1X39hSlJkiRJmnSDOIN6clUdX1Vrm/mNwPVVdSxwfTMvSZIkSdJ+9XUGdQGnAbPN\n9KXAHPCaIbyPJI3cbdt3cfbGa8YdhjRV2vZ3t/XC5487BEmaWP0mqAX8VZIC3lFVm4CZqtrRLP8K\nMDPfC5OsB9YDzMzMMDc312coMHMobFizp+/tTBrbZWG7d+8eyGdvENq2j9rUNm3i39PC/MxIkqR+\n9ZugPruqtif5MeC6JJ/tXlhV1SSvP6RJZjcBrF27tmZnZ/sMBd56+VVcdNswTgovbxvW7LFdFrB5\n3QoG8dkbhDadHYB2tU2b+D2zMD8zkiSpX339l1VV25vnnUk+CJwI3JNkZVXtSLIS2DmAOCVNsdUt\nSt43rBl3BJIkSZNryYMkJVmR5PC908BzgM8AVwNnNaudBVzVb5CSJEmSpMnXzxnUGeCDSfZu5z1V\n9ZdJPglcmeQc4C7g9P7DlCRJkiRNuiUnqFX1ReBn5im/Fziln6AkSZIkSdPHkT4kSdIPSbIVeAB4\nCNhTVWuTHAm8D1gNbAVOr6r7xhWjJGnymKBqqrXt3nqS1DInV9XXuuY3AtdX1YVJNjbz3utckjQw\nSx4kSZIkTZ3TgEub6UuBF44xFknSBPIMqiRJmk8Bf9Xcz/wdzf3LZ6pqR7P8K3QGTPwhSdYD6wFm\nZmaYm5vrK5CZQzv39G6LfuuzP7t37x7q9oep13007P3apnZs034d9t9SL/u1LW2yVMPcr236zhv1\n59cEVZIkzefZVbU9yY8B1yX5bPfCqqomef0hTTK7CWDt2rU1OzvbVyBvvfwqLrqtPf+ybD1zdmjb\nnpubo9/2GpdeL5nZsGbPUPfrMPdTr9q0X4d9aVMv+7VN+2gphrlf23QJ2uZ1K0b6+bWLryRJ+iFV\ntb153gl8EDgRuCfJSoDmeef4IpQkTSITVEmS9DBJViQ5fO808BzgM8DVwFnNamcBV40nQknSpGpP\nfxlJreIIx9JU+z/t3WuoZWUdx/HvrxnNQstorMTbFIzRJJkmkxGEYcXki5kXioygNaEJhl0lkIKu\nr0LwRSWYlWSSqVnJZJpIGUKkKd7ykjFNpmOCZjUWVjby78VaY6fpHM8aZu+19uX7gQ1r77PY+///\nP2s/z372ftY6rwZ+mASazwpXVNVPktwOXJ3kTOAPwKkDxihJmkFOUCVJ0v+oqm3A0Ys8/hRwYv8R\nSZLmhUt8JUmSJEkTwQmqJEmSJGkiOEGVJEmSJE0EJ6iSJEmSpIngBFWSJEmSNBGcoEqSJEmSJoIT\nVEmSJEnSRHCCKkmSJEmaCE5QJUmSJEkTwQmqJEmSJGkiOEGVJEmSJE0EJ6iSJEmSpIngBFWSJEmS\nNBGcoEqSJEmSJsLYJqhJ1id5KMnWJOeP63UkSVJ/HN8lSeM0lglqkhXARcB7gbXAaUnWjuO1JElS\nPxzfJUnjNq5fUNcBW6tqW1U9C1wJbBzTa0mSpH44vkuSxmpcE9RDgEcX3N/ePiZJkqaX47skaaxS\nVaN/0uQUYH1VndXePwN4a1Wdu2Cfs4Gz27uvBx4awUuvAv40gueZNdZladZmadZmcdZlaaOozRFV\nddAogtHodRnf28dHPcbP0/vOXGeTuc6mecl1VHl2GuNXjuCFFvMYcNiC+4e2jz2vqi4BLhnliya5\no6qOG+VzzgLrsjRrszRrszjrsjRrMxeWHd9h9GP8PB1b5jqbzHU2zUuufec5riW+twNrkrw2yb7A\nJmDLmF5LkiT1w/FdkjRWY/kFtap2JjkXuBFYAVxaVfeP47UkSVI/HN8lSeM2riW+VNX1wPXjev4l\njHTJ8AyxLkuzNkuzNouzLkuzNnPA8X3szHU2metsmpdce81zLBdJkiRJkiRpT43rHFRJkiRJkvbI\nVE5Qk6xP8lCSrUnOX+TvL05yVfv325Ks7j/K/nWoyyeSPJDk3iQ/TXLEEHEOYbnaLNjv5CSVZOav\nyAbd6pLk1Pa4uT/JFX3HOJQO76fDk9yc5K72PXXSEHH2LcmlSZ5Ict8Sf0+SL7d1uzfJsX3HqOk1\nT+N7h1w3J3kyyd3t7awh4txb89RndMj1hCQ7FrTpZ/qOcVSSHNaOgbs+H3x0kX2mvm075jkT7Zpk\nvyS/SnJPm+vnF9mnnz64qqbqRnNRht8BrwP2Be4B1u62z4eAi9vtTcBVQ8c9IXV5J/DSdvuceahL\n19q0+x0A3ALcChw3dNyTUBdgDXAX8Ir2/quGjnuCanMJcE67vRZ4eOi4e6rNO4BjgfuW+PtJwA1A\ngOOB24aO2dt03OZpfO+Y62bgq0PHOoJc56bP6JDrCcB1Q8c5olwPBo5ttw8AfrvIMTz1bdsxz5lo\n17ad9m+39wFuA47fbZ9e+uBp/AV1HbC1qrZV1bPAlcDG3fbZCFzWbl8DnJgkPcY4hGXrUlU3V9Uz\n7d1baf5/3TzocswAfBH4EvDPPoMbUJe6fBC4qKr+AlBVT/Qc41C61KaAl7XbLwf+2GN8g6mqW4A/\nv8AuG4FvV+NW4MAkB/cTnabcPI3vXcelqTdPfUaHXGdGVT1eVXe2238DHgQO2W23qW/bjnnOhLad\n/t7e3ae97X6xol764GmcoB4CPLrg/nb+/0B5fp+q2gnsAF7ZS3TD6VKXhc6k+VZrHixbm3bZyWFV\n9eM+AxtYl2PmSODIJL9IcmuS9b1FN6wutfkccHqS7TRXNP1wP6FNvD3ti6Rd5ml87/o+ObldGnlN\nksP6Ca1389ZnvK1dQnlDkjcOHcwotMs8j6H5xW2hmWrbF8gTZqRdk6xIcjfwBHBTVS3ZpuPsg6dx\ngqq9lOR04DjggqFjmQRJXgRcCJw3dCwTaCXNMt8TgNOAryc5cNCIJsdpwLeq6lCaZUyXt8eSJI3C\nj4DVVfUm4Cb++6uFptedwBFVdTTwFeDagePZa0n2B74PfKyqnh46nnFZJs+Zadeqeq6q3kyzynJd\nkqOGiGMaP0w9Biz8FvHQ9rFF90mykmb53VO9RDecLnUhybuATwMbqupfPcU2tOVqcwBwFPDzJA/T\nnCexJbN/oaQux8x2YEtV/buqfk9z7sWanuIbUpfanAlcDVBVvwT2A1b1Et1k69QXSYuYp/F92Vyr\n6qkF4/Q3gLf0FFvf5qbPqKqndy2hrOb/Ce+TZGrHjST70EzavlNVP1hkl5lo2+XynLV2BaiqvwI3\nA7uvnOulD57GCertwJokr02yL80Jult222cL8P52+xTgZ9WezTvDlq1LkmOAr9FMTuflXEJYpjZV\ntaOqVlXV6qpaTXN+7oaqumOYcHvT5b10Lc2vp7Sd7ZHAtj6DHEiX2jwCnAiQ5A00E9Qne41yMm0B\n3tdevfF4YEdVPT50UJoK8zS+dxmzF56rt4Hm3LdZNDd9RpLX7DpfL8k6ms/h0/gFC20e3wQerKoL\nl9ht6tu2S56z0q5JDtq1Si7JS4B3A7/Zbbde+uCVo37CcauqnUnOBW6kuQrepVV1f5IvAHdU1Raa\nA04y6zsAAAEISURBVOnyJFtpTlbfNFzE/ehYlwuA/YHvte+jR6pqw2BB96RjbeZOx7rcCLwnyQPA\nc8Anq2rqOt091bE259Esef44zUUENk/pB+U9kuS7NF9arGrPv/0szYUUqKqLac7HPQnYCjwDfGCY\nSDVt5ml875jrR5JsAHbS5Lp5sID3wjz1GR1yPQU4J8lO4B/ApikeN94OnAH8uj1nEeBTwOEwU23b\nJc9ZadeDgcuSrKCZZF9dVdcN0QdnOusnSZIkSZo107jEV5IkSZI0g5ygSpIkSZImghNUSZIkSdJE\ncIIqSZIkSZoITlAlSZIkSRPBCaokSZIkaSI4QZUkSZIkTQQnqJIkSZKkifAf8g2kpAusleIAAAAA\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fde77d6c978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "def score_func(x, norm=True):\n",
    "    kind = 'mulmax'\n",
    "    if kind=='last':\n",
    "        xx = x[-1]\n",
    "    elif kind=='mulmax':\n",
    "        xx = x.argmax(axis=1)*x.max(axis=1)\n",
    "    elif kind=='maxadjust':\n",
    "        xx = np.array([ai-xi[ai:ai+1].sum()+xi[ai+1:].sum() for xi,ai in zip(x,x.argmax(axis=1))])\n",
    "    elif kind == '1234':\n",
    "        xx = np.sum(x * np.array([1, 2, 3, 4]), axis=1)\n",
    "    elif kind == '1246':\n",
    "        xx = np.sum(x * np.array([1, 2, 4, 6]), axis=1)\n",
    "    elif kind=='1245':\n",
    "        xx = np.sum(x * np.array([1, 2, 4, 5]), axis=1)\n",
    "    else:\n",
    "        raise AttributeError('meh.')\n",
    "        \n",
    "    return (xx-xx.min())/(xx-xx.min()).max()\n",
    "\n",
    "distribution_full = []\n",
    "distribution_role = []\n",
    "scores_full = []\n",
    "scores_role = []\n",
    "\n",
    "scored_full = []\n",
    "scored_role = []\n",
    "\n",
    "for role, frm in data.test.groupby('grp'):\n",
    "    print(role, len(frm))\n",
    "    fmpred, fmppred = fm.predict(frm)\n",
    "    mpred, mppred = m[role].predict(frm)\n",
    "    \n",
    "    tmp = frm.copy()\n",
    "    tmp['SCORE'] = score_func(fmppred)\n",
    "    scored_full.append(tmp)\n",
    "    \n",
    "    tmp = frm.copy()\n",
    "    tmp['SCORE'] = score_func(mppred)\n",
    "    scored_role.append(tmp)\n",
    "    \n",
    "    distribution_full += list(fmpred)\n",
    "    distribution_role += list(mpred)\n",
    "    scores_full += list(score_func(fmppred))\n",
    "    scores_role += list(score_func(mppred))\n",
    "    \n",
    "classifier = 'bow'\n",
    "cols = ['SCORE', 'UNIQUE_ID', 'DOCUMENT_TYPE', 'FILER_NAME', 'FILER_CIK', 'FILING_INTERVAL', 'FILING_DATE', \n",
    "        'MENTIONED_FINANCIAL_ENTITY', 'ROLE', 'THREE_SENTENCES']\n",
    "\n",
    "pd.concat(scored_full).sort_values(by=['grp', 'SCORE'], \n",
    "                                   ascending=[True, False])[cols].to_csv('scored_full_'+classifier+'.csv',\n",
    "                                                                         index=False)\n",
    "pd.concat(scored_role).sort_values(by=['grp', 'SCORE'], \n",
    "                                   ascending=[True, False])[cols].to_csv('scored_role_'+classifier+'.csv',\n",
    "                                                                         index=False)\n",
    "\n",
    "\n",
    "f, ax = plt.subplots(figsize=(16,6), nrows=2, ncols=2)\n",
    "pd.DataFrame({'full_score':scores_full}).hist(ax=ax[0][0])\n",
    "pd.DataFrame({'full_argmax':distribution_full}).hist(ax=ax[0][1])\n",
    "pd.DataFrame({'role_score':scores_role}).hist(ax=ax[1][0])\n",
    "pd.DataFrame({'role_argmax':distribution_role}).hist(ax=ax[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with scoring functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7AAAAF1CAYAAADRBwbsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwEqSJEmSWuH/AOQehma4/r1rAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fde6e94fac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.522213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.221250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.384295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.489785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.698920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             test\n",
       "count  900.000000\n",
       "mean     0.522213\n",
       "std      0.221250\n",
       "min      0.000000\n",
       "25%      0.384295\n",
       "50%      0.489785\n",
       "75%      0.698920\n",
       "max      1.000000"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "def score_func(x, norm=True):\n",
    "    #return x.argmax(axis=1)\n",
    "    #return np.sum(x * (np.ones_like(x)+ x.argmax(axis=1)), axis=1)\n",
    "    #xx = ((x.argmax(axis=1))*x.argmax(axis=1)*x.max(axis=1))\n",
    "    #xx= np.array([ai-xi[ai:ai+1].sum()+xi[ai+1:].sum() for xi,ai in zip(x,a)])\n",
    "    #xx = x.argmax(axis=1)+x.max(axis=1)\n",
    "    #xx = x.argmax(axis=1)*x.max(axis=1)*x.max(axis=1)\n",
    "    #return xx\n",
    "    kind = 'mulmax'\n",
    "    if kind=='last':\n",
    "        xx = x[-1]\n",
    "    elif kind=='mulmax':\n",
    "        xx = x.argmax(axis=1)*x.max(axis=1)\n",
    "    elif kind=='maxadjust':\n",
    "        xx = np.array([ai-xi[ai:ai+1].sum()+xi[ai+1:].sum() for xi,ai in zip(x,x.argmax(axis=1))])\n",
    "    elif kind == '1234':\n",
    "        xx = np.sum(x * np.array([1, 2, 3, 4]), axis=1)\n",
    "    elif kind == '1246':\n",
    "        xx = np.sum(x * np.array([1, 2, 4, 6]), axis=1)\n",
    "    elif kind=='1245':\n",
    "        xx = np.sum(x * np.array([1, 2, 4, 5]), axis=1)\n",
    "    else:\n",
    "        raise AttributeError('meh.')\n",
    "        \n",
    "    return (xx-xx.min())/(xx-xx.min()).max()\n",
    "\n",
    "#pred, ppred = fm.predict(data.test)\n",
    "#epred, eppred = fm.predict(data.eval)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(16,6), nrows=2, ncols=3)\n",
    "pd.DataFrame({'test_score':score_func(ppred)}).hist(ax=ax[0][0])\n",
    "pd.DataFrame({'test_argmax':pred}).hist(ax=ax[0][1])\n",
    "pd.DataFrame({'eval_true':data.get_target(frm='eval')}).hist(ax=ax[1][2])\n",
    "pd.DataFrame({'eval_score':score_func(eppred)}).hist(ax=ax[1][0])\n",
    "pd.DataFrame({'eval_argmax':epred}).hist(ax=ax[1][1])\n",
    "plt.show()\n",
    "pd.DataFrame({'test':score_func(ppred, norm=False)}).describe()\n",
    "#pd.DataFrame({'eval':score_func(eppred)})\n",
    "#pd.DataFrame({'ppred':score_func(ppred), 'eppred':score_func(eppred)}).describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "218px",
    "left": "1647.38px",
    "right": "20px",
    "top": "114px",
    "width": "230px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
