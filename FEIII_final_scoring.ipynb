{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pprint\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pre-trained embedding. See readme for training your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from code.feiii_transformers import _EmbeddingHolder\n",
    "embedding = _EmbeddingHolder()\n",
    "embedding.load('embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "reading file STATE-STREET_2016.csv with 23 entries.\n",
      "reading file STATE-STREET_2014.csv with 26 entries.\n",
      "reading file PNC_2016.csv with 33 entries.\n",
      "reading file PNC_2014.csv with 38 entries.\n",
      "reading file JPM_2016.csv with 52 entries.\n",
      "reading file COMERICA_2016.csv with 11 entries.\n",
      "reading file FIFTH-THIRD_2014.csv with 36 entries.\n",
      "reading file CITIGROUP_2014.csv with 52 entries.\n",
      "reading file AMERICAN-EXPRESS_2015.csv with 11 entries.\n",
      "reading file BANK-OF-AMERICA_2015.csv with 74 entries.\n",
      "reading file ALLY_2016.csv with 44 entries.\n",
      "reading file CITIGROUP_2016.csv with 50 entries.\n",
      "reading file ALLY_2014.csv with 40 entries.\n",
      "reading file SUNTRUST_2013.csv with 35 entries.\n",
      "reading file DISCOVER_2014.csv with 41 entries.\n",
      "reading file MORGAN-STANLEY_2015.csv with 128 entries.\n",
      "reading file SUNTRUST_2016.csv with 27 entries.\n",
      "reading file BBT_2014.csv with 14 entries.\n",
      "reading file GENERAL-ELECTRIC_2013.csv with 21 entries.\n",
      "reading file FIFTH-THIRD_2015.csv with 46 entries.\n",
      "reading file CAPITAL-ONE_2013.csv with 25 entries.\n",
      "reading file BANK-OF-AMERICA_2013.csv with 88 entries.\n",
      "reading file NORTHERN-TRUST_2013.csv with 18 entries.\n",
      "reading file MT_2015.csv with 21 entries.\n",
      "reading file MT_2013.csv with 21 entries.\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th></th><th>TRAINING</th><th>TESTING SET</th></tr><tr><td>#ROWS</td><td>1875</td><td>8778</td></tr><tr><td>ROLES</td><td>{'affiliate': 315,<br /> 'agent': 101,<br /> 'counterpart': 172,<br /> 'guarantor': 62,<br /> 'insurer': 66,<br /> 'issuer': 227,<br /> 'seller': 69,<br /> 'servicer': 78,<br /> 'trustee': 724,<br /> 'underwriter': 61}</td><td>{'affiliate': 1145,<br /> 'agent': 508,<br /> 'counterpart': 1119,<br /> 'guarantor': 335,<br /> 'insurer': 473,<br /> 'issuer': 1106,<br /> 'seller': 520,<br /> 'servicer': 614,<br /> 'trustee': 2399,<br /> 'underwriter': 559}</td></tr><tr><td>DOCUMENTS</td><td>{'1026214-2011-Q2': 64,<br /> '1026214-2013-FY': 99,<br /> '109380-2014-FY': 13,<br /> '1390777-2015-Q1': 26,<br /> '1393612-2010-FY': 11,<br /> '1393612-2013-FY': 41,<br /> '19617-2011-FY': 67,<br /> '19617-2015-FY': 52,<br /> '28412-2012-FY': 11,<br /> '28412-2015-FY': 11,<br /> '310522-2012-FY': 59,<br /> '310522-2013-Q2': 21,<br /> '316709-2015-FY': 14,<br /> '35527-2013-FY': 36,<br /> '35527-2014-FY': 46,<br /> '36104-_2015-FY': 8,<br /> '36270-m&t-2010': 18,<br /> '36270-m&t-2012': 21,<br /> '36270-m&t-2014': 21,<br /> '40545-2012-FY': 21,<br /> '40545-2015-FY': 27,<br /> '40729-2012-Q3': 23,<br /> '40729-2013-FY': 40,<br /> '40729-2015-FY': 44,<br /> '4962-2014-FY': 11,<br /> '4962-2015-FY': 19,<br /> '70858-2012-FY': 88,<br /> '70858-2013-FY': 81,<br /> '70858-2014-FY': 74,<br /> '713676-2013-FY': 38,<br /> '713676-2014-FY': 33,<br /> '713676-2015-FY': 33,<br /> '73124-2012-FY': 18,<br /> '73124-2015-FY': 28,<br /> '750556-2012-FY': 35,<br /> '750556-2015-FY': 27,<br /> '831001-2011-FY': 37,<br /> '831001-2013-FY': 52,<br /> '831001-2015-FY': 50,<br /> '886982-2013-FY': 36,<br /> '895421-2014-FY': 128,<br /> '895421-2015-FY': 134,<br /> '91576-2012-FY': 10,<br /> '92230-2010-FY': 21,<br /> '92230-2013-FY': 14,<br /> '927628-2010-FY': 20,<br /> '927628-2012-FY': 25,<br /> '93751-2010-FY': 20,<br /> '93751-2013-FY': 26,<br /> '93751-2015-FY': 23}</td><td>{'1026214-2010-FY': 94,<br /> '1026214-2011-FY': 93,<br /> '1026214-2011-Q1': 54,<br /> '1026214-2011-Q3': 64,<br /> '1026214-2012-FY': 87,<br /> '1026214-2012-Q1': 43,<br /> '1026214-2012-Q2': 42,<br /> '1026214-2012-Q3': 45,<br /> '1026214-2013-Q1': 41,<br /> '1026214-2013-Q2': 43,<br /> '1026214-2013-Q3': 60,<br /> '1026214-2014-FY': 97,<br /> '1026214-2014-Q1': 50,<br /> '1026214-2014-Q2': 48,<br /> '1026214-2014-Q3': 55,<br /> '1026214-2015-FY': 88,<br /> '1026214-2015-Q1': 47,<br /> '1026214-2015-Q2': 45,<br /> '1026214-2015-Q3': 40,<br /> '1026214-2016-Q1': 23,<br /> '1026214-2016-Q2': 19,<br /> '1026214-2016-Q3': 21,<br /> '109380-2010-FY': 4,<br /> '109380-2011-FY': 8,<br /> '109380-2012-FY': 11,<br /> '109380-2012-Q3': 2,<br /> '109380-2013-FY': 11,<br /> '109380-2013-Q1': 3,<br /> '109380-2013-Q2': 2,<br /> '109380-2013-Q3': 3,<br /> '109380-2014-Q1': 4,<br /> '109380-2014-Q2': 2,<br /> '109380-2014-Q3': 1,<br /> '109380-2015-FY': 10,<br /> '109380-2016-Q2': 1,<br /> '1390777-2010-FY': 4,<br /> '1390777-2011-FY': 7,<br /> '1390777-2011-Q1': 16,<br /> '1390777-2011-Q2': 18,<br /> '1390777-2011-Q3': 22,<br /> '1390777-2012-FY': 1,<br /> '1390777-2012-Q1': 16,<br /> '1390777-2012-Q2': 17,<br /> '1390777-2012-Q3': 16,<br /> '1390777-2013-FY': 3,<br /> '1390777-2013-Q1': 18,<br /> '1390777-2013-Q2': 17,<br /> '1390777-2013-Q3': 17,<br /> '1390777-2014-FY': 6,<br /> '1390777-2014-Q1': 26,<br /> '1390777-2014-Q2': 26,<br /> '1390777-2014-Q3': 27,<br /> '1390777-2015-FY': 6,<br /> '1390777-2015-Q2': 22,<br /> '1390777-2015-Q3': 20,<br /> '1390777-2016-Q1': 20,<br /> '1390777-2016-Q2': 19,<br /> '1390777-2016-Q3': 21,<br /> '1393612-2011-FY': 46,<br /> '1393612-2011-Q1': 5,<br /> '1393612-2011-Q2': 8,<br /> '1393612-2011-Q3': 8,<br /> '1393612-2012-FY': 38,<br /> '1393612-2012-Q1': 5,<br /> '1393612-2012-Q2': 4,<br /> '1393612-2012-Q3': 4,<br /> '1393612-2013-Q1': 4,<br /> '1393612-2013-Q2': 4,<br /> '1393612-2013-Q3': 8,<br /> '1393612-2014-FY': 43,<br /> '1393612-2014-Q1': 8,<br /> '1393612-2014-Q2': 6,<br /> '1393612-2014-Q3': 8,<br /> '1393612-2015-FY': 44,<br /> '1393612-2015-Q1': 5,<br /> '1393612-2015-Q2': 7,<br /> '1393612-2015-Q3': 5,<br /> '1393612-2016-Q1': 7,<br /> '1393612-2016-Q2': 7,<br /> '1393612-2016-Q3': 10,<br /> '19617-2010-FY': 52,<br /> '19617-2011-Q1': 35,<br /> '19617-2011-Q2': 37,<br /> '19617-2011-Q3': 46,<br /> '19617-2012-FY': 52,<br /> '19617-2012-Q1': 39,<br /> '19617-2012-Q2': 37,<br /> '19617-2012-Q3': 36,<br /> '19617-2013-FY': 65,<br /> '19617-2013-Q1': 37,<br /> '19617-2013-Q2': 36,<br /> '19617-2013-Q3': 38,<br /> '19617-2014-FY': 49,<br /> '19617-2014-Q1': 38,<br /> '19617-2014-Q2': 37,<br /> '19617-2014-Q3': 38,<br /> '19617-2015-FY': 23,<br /> '19617-2015-Q1': 37,<br /> '19617-2015-Q2': 38,<br /> '19617-2015-Q3': 40,<br /> '19617-2016-Q1': 42,<br /> '19617-2016-Q2': 33,<br /> '19617-2016-Q3': 31,<br /> '28412-2010-FY': 7,<br /> '28412-2011-FY': 10,<br /> '28412-2013-FY': 10,<br /> '28412-2014-FY': 8,<br /> '28412-2015-FY': 2,<br /> '28412-2015-Q1': 1,<br /> '28412-2015-Q3': 10,<br /> '28412-2016-Q2': 1,<br /> '28412-2016-Q3': 1,<br /> '310522-2010-FY': 46,<br /> '310522-2011-FY': 65,<br /> '310522-2011-Q1': 9,<br /> '310522-2011-Q2': 9,<br /> '310522-2011-Q3': 21,<br /> '310522-2012-Q1': 20,<br /> '310522-2012-Q2': 24,<br /> '310522-2012-Q3': 26,<br /> '310522-2013-FY': 56,<br /> '310522-2013-Q1': 26,<br /> '310522-2013-Q2': 11,<br /> '310522-2013-Q3': 30,<br /> '310522-2014-FY': 47,<br /> '310522-2014-Q1': 19,<br /> '310522-2014-Q2': 20,<br /> '310522-2014-Q3': 20,<br /> '310522-2015-FY': 48,<br /> '310522-2015-Q1': 22,<br /> '310522-2015-Q2': 24,<br /> '310522-2015-Q3': 25,<br /> '310522-2016-Q1': 19,<br /> '310522-2016-Q2': 19,<br /> '310522-2016-Q3': 20,<br /> '316709-2010-FY': 17,<br /> '316709-2011-FY': 17,<br /> '316709-2011-Q1': 1,<br /> '316709-2011-Q2': 1,<br /> '316709-2011-Q3': 1,<br /> '316709-2012-FY': 18,<br /> '316709-2012-Q1': 1,<br /> '316709-2012-Q2': 1,<br /> '316709-2012-Q3': 1,<br /> '316709-2013-FY': 16,<br /> '316709-2013-Q1': 1,<br /> '316709-2013-Q2': 1,<br /> '316709-2013-Q3': 1,<br /> '316709-2014-FY': 18,<br /> '316709-2014-Q1': 1,<br /> '316709-2014-Q2': 1,<br /> '316709-2014-Q3': 1,<br /> '316709-2015-Q1': 2,<br /> '316709-2015-Q2': 2,<br /> '316709-2015-Q3': 2,<br /> '316709-2016-Q1': 1,<br /> '316709-2016-Q2': 1,<br /> '316709-2016-Q3': 1,<br /> '35527-2010-FY': 56,<br /> '35527-2011-FY': 52,<br /> '35527-2011-Q1': 11,<br /> '35527-2011-Q2': 5,<br /> '35527-2011-Q3': 5,<br /> '35527-2012-FY': 40,<br /> '35527-2012-Q1': 9,<br /> '35527-2012-Q2': 11,<br /> '35527-2012-Q3': 4,<br /> '35527-2013-FY': 5,<br /> '35527-2013-Q1': 1,<br /> '35527-2013-Q2': 3,<br /> '35527-2014-Q1': 4,<br /> '35527-2014-Q2': 3,<br /> '35527-2015-Q2': 1,<br /> '35527-2015-Q3': 6,<br /> '35527-2016-Q2': 2,<br /> '35527-2016-Q3': 4,<br /> '36104-_2010-FY': 4,<br /> '36104-_2011-FY': 3,<br /> '36104-_2011-Q1': 2,<br /> '36104-_2011-Q2': 2,<br /> '36104-_2011-Q3': 2,<br /> '36104-_2012-FY': 3,<br /> '36104-_2012-Q1': 5,<br /> '36104-_2012-Q2': 6,<br /> '36104-_2012-Q3': 5,<br /> '36104-_2013-FY': 5,<br /> '36104-_2013-Q1': 5,<br /> '36104-_2013-Q2': 3,<br /> '36104-_2013-Q3': 2,<br /> '36104-_2014-FY': 4,<br /> '36104-_2014-Q1': 4,<br /> '36104-_2014-Q2': 3,<br /> '36104-_2014-Q3': 3,<br /> '36104-_2015-Q1': 6,<br /> '36104-_2015-Q2': 4,<br /> '36104-_2015-Q3': 6,<br /> '36104-_2016-Q1': 7,<br /> '36104-_2016-Q2': 4,<br /> '36104-_2016-Q3': 8,<br /> '36270-2011-Q1': 6,<br /> '36270-2011-Q2': 11,<br /> '36270-2011-Q3': 11,<br /> '36270-2012-Q1': 8,<br /> '36270-2012-Q2': 8,<br /> '36270-2012-Q3': 7,<br /> '36270-2013-Q1': 6,<br /> '36270-2013-Q2': 6,<br /> '36270-2013-Q3': 6,<br /> '36270-2014-Q1': 5,<br /> '36270-2014-Q2': 5,<br /> '36270-2014-Q3': 5,<br /> '36270-2015-FY': 19,<br /> '36270-2015-Q1': 5,<br /> '36270-2015-Q2': 5,<br /> '36270-2015-Q3': 5,<br /> '36270-2016-Q1': 5,<br /> '36270-2016-Q2': 5,<br /> '36270-2016-Q3': 5,<br /> '36270-m&t-2011': 24,<br /> '36270-m&t-2012': 1,<br /> '36270-m&t-2013': 21,<br /> '40545-2010-FY': 23,<br /> '40545-2011-FY': 17,<br /> '40545-2012-Q2': 9,<br /> '40545-2012-Q3': 3,<br /> '40545-2013-FY': 13,<br /> '40545-2013-Q1': 6,<br /> '40545-2013-Q2': 2,<br /> '40545-2013-Q3': 4,<br /> '40545-2014-FY': 16,<br /> '40545-2014-Q1': 2,<br /> '40545-2014-Q2': 2,<br /> '40545-2014-Q3': 7,<br /> '40545-2015-Q1': 2,<br /> '40545-2015-Q2': 4,<br /> '40545-2015-Q3': 5,<br /> '40545-2016-Q1': 5,<br /> '40545-2016-Q2': 6,<br /> '40545-2016-Q3': 6,<br /> '40729-2010-FY': 38,<br /> '40729-2011-FY': 81,<br /> '40729-2011-Q1': 11,<br /> '40729-2011-Q2': 17,<br /> '40729-2011-Q3': 16,<br /> '40729-2012-FY': 65,<br /> '40729-2012-Q1': 19,<br /> '40729-2012-Q2': 24,<br /> '40729-2013-Q1': 20,<br /> '40729-2013-Q2': 15,<br /> '40729-2013-Q3': 16,<br /> '40729-2014-FY': 40,<br /> '40729-2014-Q1': 10,<br /> '40729-2014-Q2': 13,<br /> '40729-2014-Q3': 13,<br /> '40729-2015-FY': 13,<br /> '40729-2015-Q1': 10,<br /> '40729-2015-Q2': 12,<br /> '40729-2015-Q3': 13,<br /> '40729-2016-Q1': 12,<br /> '40729-2016-Q2': 13,<br /> '40729-2016-Q3': 12,<br /> '49196-2010-FY': 2,<br /> '49196-2011-FY': 4,<br /> '49196-2012-FY': 4,<br /> '49196-2012-Q1': 2,<br /> '49196-2013-FY': 4,<br /> '49196-2014-FY': 4,<br /> '49196-2015-FY': 7,<br /> '4962-2010-FY': 15,<br /> '4962-2011-FY': 15,<br /> '4962-2011-Q2': 1,<br /> '4962-2011-Q3': 1,<br /> '4962-2012-FY': 16,<br /> '4962-2012-Q1': 1,<br /> '4962-2012-Q2': 1,<br /> '4962-2012-Q3': 2,<br /> '4962-2013-FY': 15,<br /> '4962-2013-Q1': 2,<br /> '4962-2013-Q2': 3,<br /> '4962-2013-Q3': 3,<br /> '4962-2014-FY': 2,<br /> '4962-2014-Q1': 4,<br /> '4962-2014-Q2': 5,<br /> '4962-2014-Q3': 4,<br /> '4962-2015-Q1': 2,<br /> '4962-2015-Q2': 2,<br /> '4962-2015-Q3': 3,<br /> '4962-2016-Q1': 3,<br /> '4962-2016-Q2': 3,<br /> '4962-2016-Q3': 2,<br /> '70858-2010-FY': 50,<br /> '70858-2011-FY': 87,<br /> '70858-2011-Q1': 20,<br /> '70858-2011-Q2': 62,<br /> '70858-2011-Q3': 77,<br /> '70858-2012-Q1': 36,<br /> '70858-2012-Q2': 35,<br /> '70858-2012-Q3': 44,<br /> '70858-2013-Q1': 38,<br /> '70858-2013-Q2': 34,<br /> '70858-2013-Q3': 25,<br /> '70858-2014-Q1': 36,<br /> '70858-2014-Q2': 32,<br /> '70858-2014-Q3': 36,<br /> '70858-2015-FY': 57,<br /> '70858-2015-Q1': 16,<br /> '70858-2015-Q2': 16,<br /> '70858-2015-Q3': 15,<br /> '70858-2016-Q1': 17,<br /> '70858-2016-Q2': 16,<br /> '70858-2016-Q3': 13,<br /> '713676-2010-FY': 30,<br /> '713676-2011-FY': 31,<br /> '713676-2011-Q1': 5,<br /> '713676-2011-Q2': 7,<br /> '713676-2011-Q3': 4,<br /> '713676-2012-FY': 37,<br /> '713676-2012-Q1': 6,<br /> '713676-2012-Q2': 9,<br /> '713676-2012-Q3': 10,<br /> '713676-2013-FY': 2,<br /> '713676-2013-Q1': 9,<br /> '713676-2013-Q2': 8,<br /> '713676-2013-Q3': 8,<br /> '713676-2014-Q1': 9,<br /> '713676-2014-Q2': 13,<br /> '713676-2014-Q3': 7,<br /> '713676-2015-Q1': 6,<br /> '713676-2015-Q2': 6,<br /> '713676-2015-Q3': 6,<br /> '713676-2016-Q1': 2,<br /> '713676-2016-Q2': 2,<br /> '713676-2016-Q3': 3,<br /> '72971-2011-FY': 2,<br /> '72971-2011-Q1': 5,<br /> '72971-2011-Q2': 7,<br /> '72971-2011-Q3': 8,<br /> '72971-2012-FY': 1,<br /> '72971-2012-Q1': 8,<br /> '72971-2012-Q2': 9,<br /> '72971-2012-Q3': 10,<br /> '72971-2013-Q1': 9,<br /> '72971-2013-Q2': 9,<br /> '72971-2013-Q3': 9,<br /> '72971-2014-FY': 1,<br /> '72971-2014-Q1': 7,<br /> '72971-2014-Q2': 9,<br /> '72971-2014-Q3': 7,<br /> '72971-2015-FY': 1,<br /> '72971-2015-Q1': 8,<br /> '72971-2015-Q2': 9,<br /> '72971-2015-Q3': 8,<br /> '72971-2016-Q1': 8,<br /> '72971-2016-Q2': 8,<br /> '72971-2016-Q3': 8,<br /> '73124-2010-FY': 18,<br /> '73124-2011-FY': 18,<br /> '73124-2011-Q1': 10,<br /> '73124-2011-Q2': 10,<br /> '73124-2011-Q3': 8,<br /> '73124-2012-Q1': 8,<br /> '73124-2012-Q2': 9,<br /> '73124-2012-Q3': 7,<br /> '73124-2013-FY': 19,<br /> '73124-2013-Q1': 10,<br /> '73124-2013-Q2': 10,<br /> '73124-2013-Q3': 7,<br /> '73124-2014-FY': 22,<br /> '73124-2014-Q1': 7,<br /> '73124-2014-Q2': 7,<br /> '73124-2014-Q3': 7,<br /> '73124-2015-Q1': 5,<br /> '73124-2015-Q2': 7,<br /> '73124-2015-Q3': 12,<br /> '73124-2016-Q1': 9,<br /> '73124-2016-Q2': 9,<br /> '73124-2016-Q3': 9,<br /> '750556-2010-FY': 12,<br /> '750556-2011-FY': 35,<br /> '750556-2011-Q1': 5,<br /> '750556-2011-Q2': 10,<br /> '750556-2011-Q3': 11,<br /> '750556-2012-FY': 23,<br /> '750556-2012-Q1': 10,<br /> '750556-2012-Q2': 10,<br /> '750556-2012-Q3': 13,<br /> '750556-2013-FY': 35,<br /> '750556-2013-Q1': 9,<br /> '750556-2013-Q2': 10,<br /> '750556-2013-Q3': 10,<br /> '750556-2014-FY': 40,<br /> '750556-2014-Q1': 11,<br /> '750556-2014-Q2': 11,<br /> '750556-2014-Q3': 11,<br /> '750556-2015-Q1': 11,<br /> '750556-2015-Q2': 11,<br /> '750556-2015-Q3': 7,<br /> '750556-2016-Q1': 5,<br /> '750556-2016-Q2': 5,<br /> '750556-2016-Q3': 5,<br /> '831001-2010-FY': 39,<br /> '831001-2011-Q1': 18,<br /> '831001-2011-Q2': 14,<br /> '831001-2011-Q3': 11,<br /> '831001-2012-FY': 31,<br /> '831001-2012-Q1': 10,<br /> '831001-2012-Q2': 8,<br /> '831001-2012-Q3': 8,<br /> '831001-2013-FY': 38,<br /> '831001-2013-Q1': 7,<br /> '831001-2013-Q2': 6,<br /> '831001-2013-Q3': 7,<br /> '831001-2014-FY': 60,<br /> '831001-2014-Q1': 17,<br /> '831001-2014-Q2': 19,<br /> '831001-2014-Q3': 12,<br /> '831001-2015-FY': 22,<br /> '831001-2015-Q1': 10,<br /> '831001-2015-Q2': 14,<br /> '831001-2015-Q3': 13,<br /> '831001-2016-Q1': 10,<br /> '831001-2016-Q2': 10,<br /> '831001-2016-Q3': 16,<br /> '886982-2010-FY': 21,<br /> '886982-2011-FY': 33,<br /> '886982-2011-Q1': 12,<br /> '886982-2011-Q2': 12,<br /> '886982-2011-Q3': 17,<br /> '886982-2012-FY': 39,<br /> '886982-2012-Q1': 11,<br /> '886982-2012-Q2': 11,<br /> '886982-2012-Q3': 15,<br /> '886982-2013-Q1': 21,<br /> '886982-2013-Q2': 17,<br /> '886982-2013-Q3': 20,<br /> '886982-2014-FY': 44,<br /> '886982-2014-Q1': 26,<br /> '886982-2014-Q2': 16,<br /> '886982-2014-Q3': 24,<br /> '886982-2015-FY': 29,<br /> '886982-2015-Q1': 23,<br /> '886982-2015-Q2': 17,<br /> '886982-2015-Q3': 10,<br /> '886982-2016-Q1': 5,<br /> '886982-2016-Q2': 7,<br /> '886982-2016-Q3': 4,<br /> '895421-2010-FY': 49,<br /> '895421-2011-FY': 55,<br /> '895421-2011-Q1': 3,<br /> '895421-2011-Q2': 3,<br /> '895421-2011-Q3': 10,<br /> '895421-2012-FY': 89,<br /> '895421-2012-Q1': 11,<br /> '895421-2012-Q2': 3,<br /> '895421-2012-Q3': 23,<br /> '895421-2013-FY': 106,<br /> '895421-2013-Q1': 10,<br /> '895421-2013-Q2': 25,<br /> '895421-2013-Q3': 19,<br /> '895421-2014-FY': 1,<br /> '895421-2014-Q1': 15,<br /> '895421-2014-Q2': 13,<br /> '895421-2014-Q3': 32,<br /> '895421-2015-Q1': 24,<br /> '895421-2015-Q2': 25,<br /> '895421-2015-Q3': 44,<br /> '895421-2016-Q1': 48,<br /> '895421-2016-Q2': 40,<br /> '895421-2016-Q3': 40,<br /> '91576-2010-FY': 7,<br /> '91576-2011-FY': 10,<br /> '91576-2011-Q2': 8,<br /> '91576-2011-Q3': 6,<br /> '91576-2012-Q1': 2,<br /> '91576-2012-Q2': 2,<br /> '91576-2012-Q3': 2,<br /> '91576-2013-FY': 8,<br /> '91576-2013-Q1': 1,<br /> '91576-2013-Q2': 5,<br /> '91576-2013-Q3': 1,<br /> '91576-2014-FY': 6,<br /> '91576-2014-Q1': 2,<br /> '91576-2014-Q2': 2,<br /> '91576-2014-Q3': 2,<br /> '91576-2015-FY': 4,<br /> '91576-2015-Q1': 1,<br /> '91576-2015-Q2': 1,<br /> '91576-2015-Q3': 1,<br /> '91576-2016-Q1': 1,<br /> '91576-2016-Q2': 1,<br /> '91576-2016-Q3': 3,<br /> '92230-2011-FY': 19,<br /> '92230-2011-Q2': 4,<br /> '92230-2011-Q3': 4,<br /> '92230-2012-FY': 15,<br /> '92230-2012-Q1': 3,<br /> '92230-2012-Q2': 7,<br /> '92230-2012-Q3': 3,<br /> '92230-2014-FY': 14,<br /> '92230-2015-FY': 14,<br /> '92230-2016-Q1': 1,<br /> '92230-2016-Q2': 1,<br /> '92230-2016-Q3': 1,<br /> '927628-2011-FY': 23,<br /> '927628-2011-Q1': 17,<br /> '927628-2011-Q2': 18,<br /> '927628-2011-Q3': 18,<br /> '927628-2012-Q1': 19,<br /> '927628-2012-Q2': 18,<br /> '927628-2012-Q3': 20,<br /> '927628-2013-FY': 16,<br /> '927628-2013-Q1': 5,<br /> '927628-2013-Q2': 10,<br /> '927628-2013-Q3': 10,<br /> '927628-2014-FY': 14,<br /> '927628-2014-Q1': 7,<br /> '927628-2014-Q2': 8,<br /> '927628-2014-Q3': 15,<br /> '927628-2015-FY': 13,<br /> '927628-2015-Q1': 9,<br /> '927628-2015-Q2': 13,<br /> '927628-2015-Q3': 11,<br /> '927628-2016-Q1': 8,<br /> '927628-2016-Q2': 4,<br /> '927628-2016-Q3': 4,<br /> '93751-2011-FY': 24,<br /> '93751-2011-Q1': 6,<br /> '93751-2011-Q2': 9,<br /> '93751-2011-Q3': 11,<br /> '93751-2012-FY': 30,<br /> '93751-2012-Q1': 10,<br /> '93751-2012-Q2': 11,<br /> '93751-2012-Q3': 12,<br /> '93751-2013-FY': 7,<br /> '93751-2013-Q1': 12,<br /> '93751-2013-Q2': 13,<br /> '93751-2013-Q3': 13,<br /> '93751-2014-FY': 24,<br /> '93751-2014-Q1': 15,<br /> '93751-2014-Q2': 15,<br /> '93751-2014-Q3': 13,<br /> '93751-2015-FY': 6,<br /> '93751-2015-Q1': 9,<br /> '93751-2015-Q2': 10,<br /> '93751-2015-Q3': 10,<br /> '93751-2016-Q1': 10,<br /> '93751-2016-Q2': 10,<br /> '93751-2016-Q3': 10}</td></tr><tr><td>COMPANIES</td><td>{'AMERICAN EXPRESS CO': 30,<br /> 'Ally Financial Inc': 107,<br /> 'BANK OF AMERICA CORP': 243,<br /> 'BB&T CORP': 35,<br /> 'Bank of New York Mellon Corp': 26,<br /> 'CAPITAL ONE FINANCIAL CORP': 45,<br /> 'CITIGROUP INC': 139,<br /> 'COMERICA INC': 22,<br /> 'Discover Financial Services': 52,<br /> 'FEDERAL_HOME_LOAN_MORTGAGE_CORP': 163,<br /> 'FEDERAL_NATIONAL_MORTGAGE_ASSOCIATION_FANNIE_MAE': 80,<br /> 'FIFTH THIRD BANCORP': 82,<br /> 'GENERAL ELECTRIC CO': 48,<br /> 'GOLDMAN SACHS GROUP INC': 36,<br /> 'JPMORGAN CHASE & CO': 119,<br /> 'KEYCORP': 10,<br /> 'M&T BANK CORP': 60,<br /> 'MORGAN STANLEY': 262,<br /> 'NORTHERN TRUST CORP': 46,<br /> 'PNC FINANCIAL SERVICES GROUP INC': 104,<br /> 'SCHWAB_CHARLES_CORP ': 14,<br /> 'STATE STREET CORP': 69,<br /> 'SUNTRUST BANKS INC': 62,<br /> 'US_BANCORP': 8,<br /> 'ZIONS BANCORPORATION': 13}</td><td>{'AMERICAN EXPRESS CO': 105,<br /> 'Ally Financial Inc': 483,<br /> 'BANK OF AMERICA CORP': 762,<br /> 'BB&T CORP': 86,<br /> 'Bank of New York Mellon Corp': 365,<br /> 'CAPITAL ONE FINANCIAL CORP': 280,<br /> 'CITIGROUP INC': 400,<br /> 'COMERICA INC': 50,<br /> 'Discover Financial Services': 284,<br /> 'FEDERAL_HOME_LOAN_MORTGAGE_CORP': 1199,<br /> 'FEDERAL_NATIONAL_MORTGAGE_ASSOCIATION_FANNIE_MAE': 626,<br /> 'FIFTH THIRD BANCORP': 222,<br /> 'GENERAL ELECTRIC CO': 132,<br /> 'GOLDMAN SACHS GROUP INC': 434,<br /> 'HUNTINGTON BANCSHARES INC': 27,<br /> 'JPMORGAN CHASE & CO': 916,<br /> 'KEYCORP': 76,<br /> 'M&T BANK CORP': 179,<br /> 'MORGAN STANLEY': 688,<br /> 'NORTHERN TRUST CORP': 228,<br /> 'PNC FINANCIAL SERVICES GROUP INC': 220,<br /> 'SCHWAB_CHARLES_CORP ': 107,<br /> 'STATE STREET CORP': 290,<br /> 'SUNTRUST BANKS INC': 310,<br /> 'US_BANCORP': 96,<br /> 'WELLS FARGO & COMPANY': 151,<br /> 'ZIONS BANCORPORATION': 62}</td></tr><tr><td>Expert ratings</td><td>ALL: [H]_604 [R]_652 [N]_449 [I]_170<br/>[H]149 [R]_93 [N]139 [I]_28 [x]1466 (RATING_EXPERT_1)<br/>[H]101 [R]_35 [N]_83 [I]__7 [x]1649 (RATING_EXPERT_1.1)<br/>[H]__0 [R]_20 [N]__0 [I]__0 [x]1855 (RATING_EXPERT_10)<br/>[H]_58 [R]163 [N]131 [I]_14 [x]1509 (RATING_EXPERT_2)<br/>[H]__8 [R]__8 [N]_15 [I]_29 [x]1815 (RATING_EXPERT_3)<br/>[H]__5 [R]_23 [N]__5 [I]__2 [x]1840 (RATING_EXPERT_4)<br/>[H]__7 [R]_29 [N]_41 [I]__3 [x]1795 (RATING_EXPERT_5)<br/>[H]_10 [R]_34 [N]__6 [I]_10 [x]1815 (RATING_EXPERT_6)<br/>[H]_10 [R]_27 [N]_36 [I]__2 [x]1800 (RATING_EXPERT_7)<br/>[H]_62 [R]__1 [N]__1 [I]_10 [x]1801 (RATING_EXPERT_9)<br/></td><td>ALL: [H]___0 [R]___0 [N]___0 [I]8778<br/></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from code.feiii_experiment import evaluate, kfold\n",
    "from code.feiii_data import DataHolder\n",
    "from code.feiii_pipeline import FeiiiPipeline\n",
    "\n",
    "\n",
    "def pp(obj):\n",
    "    return pprint.pformat(obj).replace('\\n','<br />')\n",
    "\n",
    "data = DataHolder(eval_docs=2, combinetraintest=True)\n",
    "print('done')\n",
    "\n",
    "out = '<table><tr><th></th><th>TRAINING</th><th>TESTING SET</th></tr>'\n",
    "out+= '<tr><td>#ROWS</td><td>'+str(len(data.train_full))+'</td><td>'+str(len(data.test))+'</td></tr>'\n",
    "out+= '<tr><td>ROLES</td><td>'+pp(dict(Counter(data.train_full['grp'])))+'</td><td>'+pp(dict(Counter(data.test['grp'])))+'</td></tr>'\n",
    "out+= '<tr><td>DOCUMENTS</td><td>'+pp(dict(Counter(data.train_full['SOURCE'])))+'</td><td>'+pp(dict(Counter(data.test['SOURCE'])))+'</td></tr>'\n",
    "out+= '<tr><td>COMPANIES</td><td>'+pp(dict(Counter(data.train_full['FILER_NAME'])))+'</td><td>'+pp(dict(Counter(data.test['FILER_NAME'])))+'</td></tr>'\n",
    "out+= '<tr><td>Expert ratings</td><td>'\n",
    "tmp = Counter(data.train_full['rating'])\n",
    "out+= \"ALL: [H]{:_>4d} [R]{:_>4d} [N]{:_>4d} [I]{:_>4d}<br/>\".format(\n",
    "    tmp.get('highly',0),tmp.get('relevant',0),tmp.get('neutral',0),tmp.get('irrelevant',0))\n",
    "for c in data.train_full.filter(regex=(\"RATING\")):\n",
    "    tmp = Counter(data.train_full[c])\n",
    "    out+=\"[H]{:_>3d} [R]{:_>3d} [N]{:_>3d} [I]{:_>3d} [x]{:_>3d} ({})<br/>\".format(\n",
    "        tmp.get(\"Highly relevant\", 0), tmp.get(\"Relevant\", 0),\n",
    "        tmp.get(\"Neutral\", 0),tmp.get(\"Irrelevant\", 0),\n",
    "        tmp.get(np.nan, 0), c)\n",
    "out+= '</td><td>'\n",
    "tmp = Counter(data.test['rating'])\n",
    "out+= \"ALL: [H]{:_>4d} [R]{:_>4d} [N]{:_>4d} [I]{:_>4d}<br/>\".format(\n",
    "    tmp.get('highly',0),tmp.get('relevant',0),tmp.get('neutral',0),tmp.get('irrelevant',0))\n",
    "out+= '</td></tr>'\n",
    "out+= '</table>'\n",
    "\n",
    "HTML(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "import code.feiii_transformers as ft\n",
    "\n",
    "params_ = {\n",
    "    'cv': {\n",
    "        'ngram_range': (1, 3),\n",
    "        'min_df': 0.4,\n",
    "        'max_df': 0.6,\n",
    "        'stop_words': 'english'\n",
    "    },\n",
    "    'tt': {\n",
    "        'use_idf': True,\n",
    "        'sublinear_tf': True,\n",
    "    },\n",
    "    'emb': {\n",
    "        'num_files': 30,\n",
    "        'num_epoch': 20\n",
    "    },\n",
    "    'logit': {\n",
    "        'loss': 'log',  # ['hinge', 'log', 'perceptron','huber'] # for pred_proba: log or modified_huber\n",
    "        'penalty': 'l2',\n",
    "        'shuffle': True,\n",
    "        'alpha': 1e-4,\n",
    "        'n_iter': 15,\n",
    "        'random_state': 42,\n",
    "        'class_weight': 'balanced'\n",
    "    },\n",
    "    'rf': {\n",
    "        'n_estimators': 20,\n",
    "        'criterion': 'gini',  # gini or entropy\n",
    "        'max_features': 'auto',  # int, float, auto, sqrt, log2, None\n",
    "        'random_state': 42,\n",
    "        'class_weight': 'balanced'\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': 1.0,\n",
    "        'kernel': 'sigmoid',  # linear’, ‘poly’, ‘rbf’, ‘sigmoid’\n",
    "        'probability': True,\n",
    "        'class_weight': 'balanced',\n",
    "        'decision_function_shape': 'ovr',  # ovo, ovr\n",
    "        'random_state': 42\n",
    "    }\n",
    "}\n",
    "\n",
    "def pipeline():\n",
    "    pipln = 'emb'\n",
    "    if pipln == 'all_union':\n",
    "        line = [\n",
    "            ('union', FeatureUnion(\n",
    "                transformer_list=[\n",
    "                    ('emb', Pipeline([\n",
    "                        ('emb', ft.Embedder(embedding))\n",
    "                    ])),\n",
    "                    ('syntax', Pipeline([\n",
    "                        ('feats', ft.SyntaxFeatures()),\n",
    "                    ])),\n",
    "                    ('bow', Pipeline([\n",
    "                        ('lem', ft.Lemmatiser()),\n",
    "                        ('vect', CountVectorizer(**params_['cv'])),\n",
    "                        ('tfidf', TfidfTransformer(**params_['tt']))\n",
    "                    ]))\n",
    "                ],\n",
    "                transformer_weights={\n",
    "                    'syntax': 1,\n",
    "                    'bow': 1\n",
    "                },\n",
    "            )),\n",
    "            # ('clf', SGDClassifier(**params_['svm']))\n",
    "            ('clf', SGDClassifier(**params_['logit']))\n",
    "            # ('clf', RandomForestClassifier(**params_['rf']))\n",
    "        ]\n",
    "    elif pipln == 'all_vote':\n",
    "        line = [\n",
    "            ('clf', VotingClassifier(\n",
    "                voting='soft',  # hard, soft\n",
    "                # weights=[2,1,2],\n",
    "                estimators=[\n",
    "                    ('syn', Pipeline([\n",
    "                        ('feats', ft.SyntaxFeatures()),\n",
    "                        ('rf', RandomForestClassifier(**params_['rf'])),\n",
    "                        # ('svc', SVC(**params_['svm']))\n",
    "                    ])),\n",
    "                    ('emb', Pipeline([\n",
    "                        ('emb', ft.Embedder(embedding)),\n",
    "                        ('svc', SVC(**params_['svm']))\n",
    "                    ])),\n",
    "                    ('bow', Pipeline([\n",
    "                        ('lem', ft.Lemmatiser()),\n",
    "                        ('vect', CountVectorizer(**params_['cv'])),\n",
    "                        ('tfidf', TfidfTransformer(**params_['tt'])),\n",
    "                        ('svc', SVC(**params_['svm']))\n",
    "                        # ('bclf', SGDClassifier(**params_['logit']))\n",
    "                    ]))\n",
    "                ]))]\n",
    "    elif pipln == 'syn':\n",
    "        line = [\n",
    "            ('feats', ft.SyntaxFeatures()),\n",
    "            ('clf', RandomForestClassifier(**params_['rf']))\n",
    "        ]\n",
    "    elif pipln == 'emb':\n",
    "        line = [\n",
    "            ('emb', ft.Embedder(embedding)),\n",
    "            ('clf', SVC(**params_['svm']))\n",
    "           # ('bclf', SGDClassifier(**params_['logit']))\n",
    "        ]\n",
    "    else:  # pipln == 'bow'\n",
    "        line = [\n",
    "            ('lem', ft.Lemmatiser()),\n",
    "            ('vect', CountVectorizer(**params_['cv'])),\n",
    "            ('tfidf', TfidfTransformer(**params_['tt'])),\n",
    "            ('svc', SVC(**params_['svm']))\n",
    "            #('clf', SGDClassifier(**params_['logit']))\n",
    "        ]\n",
    "    return FeiiiPipeline(line=line, embedding=embedding)\n",
    "\n",
    "def score_func(x):\n",
    "    kind = 'mulmax'\n",
    "    if kind=='last':\n",
    "        xx = x[-1]\n",
    "    elif kind=='mulmax':\n",
    "        xx = (x.argmax(axis=1)+1)*x.max(axis=1)\n",
    "    elif kind=='maxadjust':\n",
    "        xx = np.array([ai-xi[ai:ai+1].sum()+xi[ai+1:].sum() for xi,ai in zip(x,x.argmax(axis=1))])\n",
    "    elif kind == '1234':\n",
    "        xx = np.sum(x * np.array([1, 2, 3, 4]), axis=1)\n",
    "    elif kind == '1246':\n",
    "        xx = np.sum(x * np.array([1, 2, 4, 6]), axis=1)\n",
    "    elif kind=='1245':\n",
    "        xx = np.sum(x * np.array([1, 2, 4, 5]), axis=1)\n",
    "    else:\n",
    "        raise AttributeError('meh.')\n",
    "        \n",
    "    return (xx-xx.min())/(xx-xx.min()).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following line in case of errors during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.shuffle_train_eval(n_docs_eval=9, max_tries=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items in training set: 1575 (84.00%)\n",
      "Items in eval set: 300\n",
      "Items in test set: 8778\n",
      " = 10653\n",
      "Number of source documents: 598 total, 41 train, 9 eval 548 test\n",
      "Absolute (training): IR 62.00, N 250.00, R 247.00, HR 282.00\n",
      "Relative (training): IR 0.07, N 0.30, R 0.29, HR 0.34\n",
      "Absolute (eval): IR 28.00, N 57.00, R 36.00, HR 13.00\n",
      "Relative (eval): IR 0.21, N 0.43, R 0.27, HR 0.10\n",
      "Role samples for ISSUER in train: 191, eval: 36, test: 1106\n",
      "Role samples for AFFILIATE in train: 243, eval: 72, test: 1145\n",
      "Role samples for AGENT in train: 84, eval: 17, test: 508\n",
      "Role samples for TRUSTEE in train: 623, eval: 101, test: 2399\n",
      "Role samples for INSURER in train: 55, eval: 11, test: 473\n",
      "Role samples for UNDERWRITER in train: 52, eval: 9, test: 559\n",
      "Role samples for SELLER in train: 64, eval: 5, test: 520\n",
      "Role samples for GUARANTOR in train: 44, eval: 18, test: 335\n",
      "Role samples for SERVICER in train: 69, eval: 9, test: 614\n",
      "Role samples for COUNTERPART in train: 150, eval: 22, test: 1119\n",
      "=== AGENT ======\n",
      "Items in training set: 84 (83.17%)\n",
      "Items in eval set: 17\n",
      "Items in test set: 508\n",
      " = 609\n",
      "Number of source documents: 185 total, 21 train, 4 eval 160 test\n",
      "Absolute (training): IR 1.00, N 27.00, R 14.00, HR 5.00\n",
      "Relative (training): IR 0.02, N 0.57, R 0.30, HR 0.11\n",
      "Absolute (eval): IR 2.00, N 8.00, R 4.00, HR 0.00\n",
      "Relative (eval): IR 0.14, N 0.57, R 0.29, HR 0.00\n",
      "Role samples for AGENT in train: 84, eval: 17, test: 508\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.836296277985 | std = 0.0601593040824\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.631492381287\n",
      "Accuracy | role : 0.352941176471\n",
      "Accuracy | full : 0.294117647059\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         2\n",
      "    neutral       0.00      0.00      0.00         9\n",
      "   relevant       0.31      0.83      0.45         6\n",
      "     highly       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.11      0.29      0.16        17\n",
      "\n",
      "[[0 0 2 0]\n",
      " [0 0 9 0]\n",
      " [0 0 5 1]\n",
      " [0 0 0 0]]\n",
      "> NDCG Score | role | categ  | 0.90802\n",
      "> NDCG Score | role | proba* | 0.84473\n",
      "> NDCG Score | full | categ  | 0.90802\n",
      "> NDCG Score | full | proba* | 0.85533\n",
      "=== TRUSTEE ======\n",
      "Items in training set: 623 (86.05%)\n",
      "Items in eval set: 101\n",
      "Items in test set: 2399\n",
      " = 3123\n",
      "Number of source documents: 306 total, 34 train, 6 eval 266 test\n",
      "Absolute (training): IR 5.00, N 131.00, R 79.00, HR 156.00\n",
      "Relative (training): IR 0.01, N 0.35, R 0.21, HR 0.42\n",
      "Absolute (eval): IR 16.00, N 33.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR 0.33, N 0.67, R 0.00, HR 0.00\n",
      "Role samples for TRUSTEE in train: 623, eval: 101, test: 2399\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.825117217333 | std = 0.0304181218259\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.634923724845\n",
      "Accuracy | role : 0.475247524752\n",
      "Accuracy | full : 0.227722772277\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00        17\n",
      "    neutral       0.00      0.00      0.00        39\n",
      "   relevant       0.07      0.38      0.12        13\n",
      "     highly       0.55      0.56      0.55        32\n",
      "\n",
      "avg / total       0.18      0.23      0.19       101\n",
      "\n",
      "[[ 0  0 15  2]\n",
      " [ 0  0 34  5]\n",
      " [ 0  0  5  8]\n",
      " [ 0  0 14 18]]\n",
      "> NDCG Score | role | categ  | 0.87121\n",
      "> NDCG Score | role | proba* | 0.90543\n",
      "> NDCG Score | full | categ  | 0.90622\n",
      "> NDCG Score | full | proba* | 0.94106\n",
      "=== SERVICER ======\n",
      "Items in training set: 69 (88.46%)\n",
      "Items in eval set: 9\n",
      "Items in test set: 614\n",
      " = 692\n",
      "Number of source documents: 202 total, 14 train, 4 eval 184 test\n",
      "Absolute (training): IR 0.00, N 4.00, R 6.00, HR 9.00\n",
      "Relative (training): IR 0.00, N 0.21, R 0.32, HR 0.47\n",
      "Absolute (eval): IR 0.00, N 2.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 1.00, R 0.00, HR 0.00\n",
      "Role samples for SERVICER in train: 69, eval: 9, test: 614\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.829641834728 | std = 0.0728221557415\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.682145835421\n",
      "Accuracy | role : 0.333333333333\n",
      "Accuracy | full : 0.0\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         1\n",
      "    neutral       0.00      0.00      0.00         5\n",
      "   relevant       0.00      0.00      0.00         3\n",
      "     highly       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.00      0.00      0.00         9\n",
      "\n",
      "[[0 0 0 1]\n",
      " [0 0 4 1]\n",
      " [0 0 0 3]\n",
      " [0 0 0 0]]\n",
      "> NDCG Score | role | categ  | 0.77674\n",
      "> NDCG Score | role | proba* | 0.69554\n",
      "> NDCG Score | full | categ  | 0.86027\n",
      "> NDCG Score | full | proba* | 0.89629\n",
      "=== COUNTERPART ======\n",
      "Items in training set: 150 (87.21%)\n",
      "Items in eval set: 22\n",
      "Items in test set: 1119\n",
      " = 1291\n",
      "Number of source documents: 305 total, 26 train, 5 eval 274 test\n",
      "Absolute (training): IR 4.00, N 6.00, R 25.00, HR 12.00\n",
      "Relative (training): IR 0.09, N 0.13, R 0.53, HR 0.26\n",
      "Absolute (eval): IR 2.00, N 6.00, R 5.00, HR 4.00\n",
      "Relative (eval): IR 0.12, N 0.35, R 0.29, HR 0.24\n",
      "Role samples for COUNTERPART in train: 150, eval: 22, test: 1119\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.775861575346 | std = 0.0667236512272\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.547873185578\n",
      "Accuracy | role : 0.227272727273\n",
      "Accuracy | full : 0.181818181818\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.00      0.00      0.00         7\n",
      "   relevant       0.27      0.50      0.35         6\n",
      "     highly       0.09      0.20      0.13         5\n",
      "\n",
      "avg / total       0.10      0.18      0.12        22\n",
      "\n",
      "[[0 0 1 3]\n",
      " [0 0 3 4]\n",
      " [0 0 3 3]\n",
      " [0 0 4 1]]\n",
      "> NDCG Score | role | categ  | 0.78918\n",
      "> NDCG Score | role | proba* | 0.66215\n",
      "> NDCG Score | full | categ  | 0.67180\n",
      "> NDCG Score | full | proba* | 0.68962\n",
      "=== ISSUER ======\n",
      "Items in training set: 191 (84.14%)\n",
      "Items in eval set: 36\n",
      "Items in test set: 1106\n",
      " = 1333\n",
      "Number of source documents: 415 total, 38 train, 8 eval 369 test\n",
      "Absolute (training): IR 28.00, N 42.00, R 26.00, HR 20.00\n",
      "Relative (training): IR 0.24, N 0.36, R 0.22, HR 0.17\n",
      "Absolute (eval): IR 4.00, N 3.00, R 4.00, HR 2.00\n",
      "Relative (eval): IR 0.31, N 0.23, R 0.31, HR 0.15\n",
      "Role samples for ISSUER in train: 191, eval: 36, test: 1106\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.781375655655 | std = 0.0492655134238\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.560605280506\n",
      "Accuracy | role : 0.416666666667\n",
      "Accuracy | full : 0.305555555556\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         8\n",
      "    neutral       0.00      0.00      0.00        11\n",
      "   relevant       0.40      0.67      0.50        15\n",
      "     highly       0.09      0.50      0.15         2\n",
      "\n",
      "avg / total       0.17      0.31      0.22        36\n",
      "\n",
      "[[ 0  0  5  3]\n",
      " [ 0  0  9  2]\n",
      " [ 0  0 10  5]\n",
      " [ 0  0  1  1]]\n",
      "> NDCG Score | role | categ  | 0.75673\n",
      "> NDCG Score | role | proba* | 0.73641\n",
      "> NDCG Score | full | categ  | 0.77854\n",
      "> NDCG Score | full | proba* | 0.88119\n",
      "=== AFFILIATE ======\n",
      "Items in training set: 243 (77.14%)\n",
      "Items in eval set: 72\n",
      "Items in test set: 1145\n",
      " = 1460\n",
      "Number of source documents: 308 total, 32 train, 9 eval 267 test\n",
      "Absolute (training): IR 23.00, N 36.00, R 47.00, HR 51.00\n",
      "Relative (training): IR 0.15, N 0.23, R 0.30, HR 0.32\n",
      "Absolute (eval): IR 3.00, N 4.00, R 15.00, HR 7.00\n",
      "Relative (eval): IR 0.10, N 0.14, R 0.52, HR 0.24\n",
      "Role samples for AFFILIATE in train: 243, eval: 72, test: 1145\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.871688834759 | std = 0.0351704798515\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.675933237451\n",
      "Accuracy | role : 0.388888888889\n",
      "Accuracy | full : 0.472222222222\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00        11\n",
      "    neutral       0.00      0.00      0.00         7\n",
      "   relevant       0.47      0.65      0.55        26\n",
      "     highly       0.47      0.61      0.53        28\n",
      "\n",
      "avg / total       0.35      0.47      0.40        72\n",
      "\n",
      "[[ 0  0  5  6]\n",
      " [ 0  0  3  4]\n",
      " [ 0  0 17  9]\n",
      " [ 0  0 11 17]]\n",
      "> NDCG Score | role | categ  | 0.93219\n",
      "> NDCG Score | role | proba* | 0.84750\n",
      "> NDCG Score | full | categ  | 0.88457\n",
      "> NDCG Score | full | proba* | 0.92944\n",
      "=== INSURER ======\n",
      "Items in training set: 55 (83.33%)\n",
      "Items in eval set: 11\n",
      "Items in test set: 473\n",
      " = 539\n",
      "Number of source documents: 128 total, 12 train, 3 eval 113 test\n",
      "Absolute (training): IR 1.00, N 1.00, R 7.00, HR 9.00\n",
      "Relative (training): IR 0.06, N 0.06, R 0.39, HR 0.50\n",
      "Absolute (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for INSURER in train: 55, eval: 11, test: 473\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.835676350134 | std = 0.0609272690787\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.644929467535\n",
      "Accuracy | role : 0.363636363636\n",
      "Accuracy | full : 0.181818181818\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         1\n",
      "    neutral       0.00      0.00      0.00         3\n",
      "   relevant       0.20      0.25      0.22         4\n",
      "     highly       0.17      0.33      0.22         3\n",
      "\n",
      "avg / total       0.12      0.18      0.14        11\n",
      "\n",
      "[[0 0 1 0]\n",
      " [0 0 1 2]\n",
      " [0 0 1 3]\n",
      " [0 0 2 1]]\n",
      "> NDCG Score | role | categ  | 0.76930\n",
      "> NDCG Score | role | proba* | 0.93335\n",
      "> NDCG Score | full | categ  | 0.78798\n",
      "> NDCG Score | full | proba* | 0.82881\n",
      "=== UNDERWRITER ======\n",
      "Items in training set: 52 (85.25%)\n",
      "Items in eval set: 9\n",
      "Items in test set: 559\n",
      " = 620\n",
      "Number of source documents: 167 total, 13 train, 3 eval 151 test\n",
      "Absolute (training): IR 0.00, N 0.00, R 9.00, HR 7.00\n",
      "Relative (training): IR 0.00, N 0.00, R 0.56, HR 0.44\n",
      "Absolute (eval): IR 0.00, N 0.00, R 5.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for UNDERWRITER in train: 52, eval: 9, test: 559\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.836171991343 | std = 0.084287845404\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.61792079503\n",
      "Accuracy | role : 0.555555555556\n",
      "Accuracy | full : 0.111111111111\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         2\n",
      "    neutral       0.00      0.00      0.00         0\n",
      "   relevant       0.20      0.20      0.20         5\n",
      "     highly       0.00      0.00      0.00         2\n",
      "\n",
      "avg / total       0.11      0.11      0.11         9\n",
      "\n",
      "[[0 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 0 1 4]\n",
      " [0 0 2 0]]\n",
      "> NDCG Score | role | categ  | 0.88153\n",
      "> NDCG Score | role | proba* | 0.82235\n",
      "> NDCG Score | full | categ  | 0.88153\n",
      "> NDCG Score | full | proba* | 0.88086\n",
      "=== SELLER ======\n",
      "Items in training set: 64 (92.75%)\n",
      "Items in eval set: 5\n",
      "Items in test set: 520\n",
      " = 589\n",
      "Number of source documents: 169 total, 17 train, 2 eval 150 test\n",
      "Absolute (training): IR 0.00, N 1.00, R 12.00, HR 5.00\n",
      "Relative (training): IR 0.00, N 0.06, R 0.67, HR 0.28\n",
      "Absolute (eval): IR 0.00, N 0.00, R 2.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for SELLER in train: 64, eval: 5, test: 520\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.937963351088 | std = 0.0312509343507\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.882790910825\n",
      "Accuracy | role : 0.6\n",
      "Accuracy | full : 0.2\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         0\n",
      "    neutral       0.00      0.00      0.00         0\n",
      "   relevant       0.33      0.33      0.33         3\n",
      "     highly       0.00      0.00      0.00         2\n",
      "\n",
      "avg / total       0.20      0.20      0.20         5\n",
      "\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 1 2]\n",
      " [0 0 2 0]]\n",
      "> NDCG Score | role | categ  | 0.90474\n",
      "> NDCG Score | role | proba* | 0.89714\n",
      "> NDCG Score | full | categ  | 0.90474\n",
      "> NDCG Score | full | proba* | 0.88279\n",
      "=== GUARANTOR ======\n",
      "Items in training set: 44 (70.97%)\n",
      "Items in eval set: 18\n",
      "Items in test set: 335\n",
      " = 397\n",
      "Number of source documents: 96 total, 13 train, 2 eval 81 test\n",
      "Absolute (training): IR 0.00, N 2.00, R 22.00, HR 8.00\n",
      "Relative (training): IR 0.00, N 0.06, R 0.69, HR 0.25\n",
      "Absolute (eval): IR 1.00, N 1.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR 0.50, N 0.50, R 0.00, HR 0.00\n",
      "Role samples for GUARANTOR in train: 44, eval: 18, test: 335\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.898692707011 | std = 0.0336037714221\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.762561581099\n",
      "Accuracy | role : 0.777777777778\n",
      "Accuracy | full : 0.722222222222\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         1\n",
      "    neutral       0.00      0.00      0.00         1\n",
      "   relevant       0.76      0.93      0.84        14\n",
      "     highly       0.00      0.00      0.00         2\n",
      "\n",
      "avg / total       0.59      0.72      0.65        18\n",
      "\n",
      "[[ 0  0  1  0]\n",
      " [ 0  0  1  0]\n",
      " [ 0  0 13  1]\n",
      " [ 0  0  2  0]]\n",
      "> NDCG Score | role | categ  | 0.88798\n",
      "> NDCG Score | role | proba* | 0.92330\n",
      "> NDCG Score | full | categ  | 0.88798\n",
      "> NDCG Score | full | proba* | 0.90973\n",
      "TOTAL NDCG | role | categ  | 0.92218\n",
      "TOTAL NDCG | role | proba* | 0.86977\n",
      "TOTAL NDCG | full | categ  | 0.93128\n",
      "TOTAL NDCG | full | proba* | 0.90661\n"
     ]
    }
   ],
   "source": [
    "fm, models, res, macro_res, conf_matrix_role, conf_matrix_full = evaluate(data, pipeline, score_func=score_func,predict_on='eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaving 3 docs out per fold\n",
      "\n",
      "\n",
      "==========================================================================\n",
      "===                      CROSSEVAL ITERATION 1/5                     =====\n",
      "==========================================================================\n",
      "\n",
      "\n",
      "Items in training set: 774 (79.38%)\n",
      "Items in eval set: 201\n",
      "Items in test set: 900\n",
      " = 1875\n",
      "Number of source documents: 50 total, 22 train, 3 eval 25 test\n",
      "Absolute (training): IR 84.00, N 245.00, R 248.00, HR 197.00\n",
      "Relative (training): IR 0.11, N 0.32, R 0.32, HR 0.25\n",
      "Absolute (eval): IR 6.00, N 62.00, R 35.00, HR 98.00\n",
      "Relative (eval): IR 0.03, N 0.31, R 0.17, HR 0.49\n",
      "Role samples for TRUSTEE in train: 291, eval: 129, test: 304\n",
      "Role samples for COUNTERPART in train: 63, eval: 1, test: 108\n",
      "Role samples for SELLER in train: 17, eval: 3, test: 49\n",
      "Role samples for GUARANTOR in train: 34, eval: 0, test: 28\n",
      "Role samples for ISSUER in train: 109, eval: 20, test: 98\n",
      "Role samples for AGENT in train: 49, eval: 12, test: 40\n",
      "Role samples for UNDERWRITER in train: 19, eval: 2, test: 40\n",
      "Role samples for SERVICER in train: 19, eval: 2, test: 57\n",
      "Role samples for AFFILIATE in train: 155, eval: 31, test: 129\n",
      "Role samples for INSURER in train: 18, eval: 1, test: 47\n",
      "=== GUARANTOR ======\n",
      "Items in training set: 34 (100.00%)\n",
      "Items in eval set: 0\n",
      "Items in test set: 28\n",
      " = 62\n",
      "Number of source documents: 15 total, 6 train, 0 eval 9 test\n",
      "Absolute (training): IR 1.00, N 3.00, R 22.00, HR 8.00\n",
      "Relative (training): IR 0.03, N 0.09, R 0.65, HR 0.24\n",
      "Absolute (eval): IR 0.00, N 0.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR nan, N nan, R nan, HR nan\n",
      "Role samples for GUARANTOR in train: 34, eval: 0, test: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/Uni/HPI/workspace/FEII/code/feiii_data.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  rating_agg.sum()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.904693093232 | std = 0.0224933107974\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.810440241115\n",
      "Accuracy | role : 0.785714285714\n",
      "Accuracy | full : 0.535714285714\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         0\n",
      "    neutral       0.17      0.33      0.22         3\n",
      "   relevant       0.85      0.50      0.63        22\n",
      "     highly       0.33      1.00      0.50         3\n",
      "\n",
      "avg / total       0.72      0.54      0.57        28\n",
      "\n",
      "[[ 0  0  0  0]\n",
      " [ 0  1  2  0]\n",
      " [ 0  5 11  6]\n",
      " [ 0  0  0  3]]\n",
      "> NDCG Score | role | categ  | 0.84067\n",
      "> NDCG Score | role | proba* | 0.90207\n",
      "> NDCG Score | full | categ  | 0.95140\n",
      "> NDCG Score | full | proba* | 0.95381\n",
      "=== ISSUER ======\n",
      "Items in training set: 109 (84.50%)\n",
      "Items in eval set: 20\n",
      "Items in test set: 98\n",
      " = 227\n",
      "Number of source documents: 46 total, 21 train, 3 eval 22 test\n",
      "Absolute (training): IR 29.00, N 43.00, R 23.00, HR 14.00\n",
      "Relative (training): IR 0.27, N 0.39, R 0.21, HR 0.13\n",
      "Absolute (eval): IR 3.00, N 2.00, R 7.00, HR 8.00\n",
      "Relative (eval): IR 0.15, N 0.10, R 0.35, HR 0.40\n",
      "Role samples for ISSUER in train: 109, eval: 20, test: 98\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.817504328601 | std = 0.0327401170005\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.621677771913\n",
      "Accuracy | role : 0.30612244898\n",
      "Accuracy | full : 0.316326530612\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.71      0.48      0.57        21\n",
      "    neutral       0.33      0.15      0.21        27\n",
      "   relevant       0.22      0.48      0.30        27\n",
      "     highly       0.31      0.17      0.22        23\n",
      "\n",
      "avg / total       0.38      0.32      0.31        98\n",
      "\n",
      "[[10  0 11  0]\n",
      " [ 2  4 18  3]\n",
      " [ 2  6 13  6]\n",
      " [ 0  2 17  4]]\n",
      "> NDCG Score | role | categ  | 0.84903\n",
      "> NDCG Score | role | proba* | 0.83336\n",
      "> NDCG Score | full | categ  | 0.85368\n",
      "> NDCG Score | full | proba* | 0.87541\n",
      "=== UNDERWRITER ======\n",
      "Items in training set: 19 (90.48%)\n",
      "Items in eval set: 2\n",
      "Items in test set: 40\n",
      " = 61\n",
      "Number of source documents: 16 total, 6 train, 1 eval 9 test\n",
      "Absolute (training): IR 0.00, N 0.00, R 12.00, HR 7.00\n",
      "Relative (training): IR 0.00, N 0.00, R 0.63, HR 0.37\n",
      "Absolute (eval): IR 0.00, N 0.00, R 2.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for UNDERWRITER in train: 19, eval: 2, test: 40\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.908006997794 | std = 0.0377869477764\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.721180267881\n",
      "Accuracy | role : 0.375\n",
      "Accuracy | full : 0.525\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.17      1.00      0.29         2\n",
      "   relevant       0.53      0.67      0.59        15\n",
      "     highly       1.00      0.45      0.62        20\n",
      "\n",
      "avg / total       0.71      0.53      0.55        40\n",
      "\n",
      "[[ 0  1  2  0]\n",
      " [ 0  2  0  0]\n",
      " [ 0  5 10  0]\n",
      " [ 0  4  7  9]]\n",
      "> NDCG Score | role | categ  | 0.86908\n",
      "> NDCG Score | role | proba* | 0.84526\n",
      "> NDCG Score | full | categ  | 0.97421\n",
      "> NDCG Score | full | proba* | 0.98289\n",
      "=== SERVICER ======\n",
      "Items in training set: 19 (90.48%)\n",
      "Items in eval set: 2\n",
      "Items in test set: 57\n",
      " = 78\n",
      "Number of source documents: 18 total, 7 train, 1 eval 10 test\n",
      "Absolute (training): IR 0.00, N 4.00, R 6.00, HR 9.00\n",
      "Relative (training): IR 0.00, N 0.21, R 0.32, HR 0.47\n",
      "Absolute (eval): IR 0.00, N 2.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 1.00, R 0.00, HR 0.00\n",
      "Role samples for SERVICER in train: 19, eval: 2, test: 57\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.882676121833 | std = 0.0261045537872\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.717524169319\n",
      "Accuracy | role : 0.350877192982\n",
      "Accuracy | full : 0.491228070175\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.20      0.11      0.14         9\n",
      "   relevant       0.62      0.66      0.64        38\n",
      "     highly       0.17      0.29      0.21         7\n",
      "\n",
      "avg / total       0.47      0.49      0.48        57\n",
      "\n",
      "[[ 0  0  3  0]\n",
      " [ 0  1  8  0]\n",
      " [ 0  3 25 10]\n",
      " [ 0  1  4  2]]\n",
      "> NDCG Score | role | categ  | 0.88610\n",
      "> NDCG Score | role | proba* | 0.92632\n",
      "> NDCG Score | full | categ  | 0.89569\n",
      "> NDCG Score | full | proba* | 0.88326\n",
      "=== TRUSTEE ======\n",
      "Items in training set: 291 (69.29%)\n",
      "Items in eval set: 129\n",
      "Items in test set: 304\n",
      " = 724\n",
      "Number of source documents: 40 total, 18 train, 3 eval 19 test\n",
      "Absolute (training): IR 21.00, N 115.00, R 70.00, HR 85.00\n",
      "Relative (training): IR 0.07, N 0.40, R 0.24, HR 0.29\n",
      "Absolute (eval): IR 0.00, N 49.00, R 9.00, HR 71.00\n",
      "Relative (eval): IR 0.00, N 0.38, R 0.07, HR 0.55\n",
      "Role samples for TRUSTEE in train: 291, eval: 129, test: 304\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.931202213822 | std = 0.0111950710907\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.814144820697\n",
      "Accuracy | role : 0.394736842105\n",
      "Accuracy | full : 0.516447368421\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         9\n",
      "    neutral       0.16      0.36      0.23        47\n",
      "   relevant       0.45      0.23      0.31       124\n",
      "     highly       0.83      0.90      0.86       124\n",
      "\n",
      "avg / total       0.55      0.52      0.51       304\n",
      "\n",
      "[[  0   1   5   3]\n",
      " [  2  17  26   2]\n",
      " [  0  77  29  18]\n",
      " [  0   9   4 111]]\n",
      "> NDCG Score | role | categ  | 0.95338\n",
      "> NDCG Score | role | proba* | 0.95031\n",
      "> NDCG Score | full | categ  | 0.98597\n",
      "> NDCG Score | full | proba* | 0.97990\n",
      "=== COUNTERPART ======\n",
      "Items in training set: 63 (98.44%)\n",
      "Items in eval set: 1\n",
      "Items in test set: 108\n",
      " = 172\n",
      "Number of source documents: 31 total, 16 train, 1 eval 14 test\n",
      "Absolute (training): IR 6.00, N 12.00, R 29.00, HR 16.00\n",
      "Relative (training): IR 0.10, N 0.19, R 0.46, HR 0.25\n",
      "Absolute (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for COUNTERPART in train: 63, eval: 1, test: 108\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.860436553498 | std = 0.0292564668944\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.677182649456\n",
      "Accuracy | role : 0.388888888889\n",
      "Accuracy | full : 0.398148148148\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       1.00      0.06      0.11        18\n",
      "    neutral       0.10      0.06      0.07        17\n",
      "   relevant       0.33      0.58      0.42        38\n",
      "     highly       0.61      0.54      0.58        35\n",
      "\n",
      "avg / total       0.50      0.40      0.36       108\n",
      "\n",
      "[[ 1  1 16  0]\n",
      " [ 0  1 15  1]\n",
      " [ 0  5 22 11]\n",
      " [ 0  3 13 19]]\n",
      "> NDCG Score | role | categ  | 0.94220\n",
      "> NDCG Score | role | proba* | 0.85121\n",
      "> NDCG Score | full | categ  | 0.93072\n",
      "> NDCG Score | full | proba* | 0.93995\n",
      "=== SELLER ======\n",
      "Items in training set: 17 (85.00%)\n",
      "Items in eval set: 3\n",
      "Items in test set: 49\n",
      " = 69\n",
      "Number of source documents: 19 total, 9 train, 1 eval 9 test\n",
      "Absolute (training): IR 0.00, N 1.00, R 11.00, HR 5.00\n",
      "Relative (training): IR 0.00, N 0.06, R 0.65, HR 0.29\n",
      "Absolute (eval): IR 0.00, N 0.00, R 3.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for SELLER in train: 17, eval: 3, test: 49\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.866089351788 | std = 0.0397533310384\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.674252161736\n",
      "Accuracy | role : 0.34693877551\n",
      "Accuracy | full : 0.510204081633\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.00      0.00      0.00        10\n",
      "   relevant       0.47      0.88      0.61        17\n",
      "     highly       0.83      0.56      0.67        18\n",
      "\n",
      "avg / total       0.47      0.51      0.46        49\n",
      "\n",
      "[[ 0  0  4  0]\n",
      " [ 0  0  9  1]\n",
      " [ 0  1 15  1]\n",
      " [ 0  4  4 10]]\n",
      "> NDCG Score | role | categ  | 0.80424\n",
      "> NDCG Score | role | proba* | 0.87101\n",
      "> NDCG Score | full | categ  | 0.95394\n",
      "> NDCG Score | full | proba* | 0.91708\n",
      "=== AGENT ======\n",
      "Items in training set: 49 (80.33%)\n",
      "Items in eval set: 12\n",
      "Items in test set: 40\n",
      " = 101\n",
      "Number of source documents: 25 total, 13 train, 3 eval 9 test\n",
      "Absolute (training): IR 3.00, N 29.00, R 14.00, HR 3.00\n",
      "Relative (training): IR 0.06, N 0.59, R 0.29, HR 0.06\n",
      "Absolute (eval): IR 0.00, N 6.00, R 4.00, HR 2.00\n",
      "Relative (eval): IR 0.00, N 0.50, R 0.33, HR 0.17\n",
      "Role samples for AGENT in train: 49, eval: 12, test: 40\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.854804552428 | std = 0.0373773782604\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.650212572736\n",
      "Accuracy | role : 0.2\n",
      "Accuracy | full : 0.425\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.18      0.25      0.21         8\n",
      "   relevant       0.46      0.73      0.56        15\n",
      "     highly       0.80      0.31      0.44        13\n",
      "\n",
      "avg / total       0.47      0.42      0.40        40\n",
      "\n",
      "[[ 0  3  0  1]\n",
      " [ 0  2  6  0]\n",
      " [ 0  4 11  0]\n",
      " [ 0  2  7  4]]\n",
      "> NDCG Score | role | categ  | 0.83777\n",
      "> NDCG Score | role | proba* | 0.81160\n",
      "> NDCG Score | full | categ  | 0.88626\n",
      "> NDCG Score | full | proba* | 0.93328\n",
      "=== AFFILIATE ======\n",
      "Items in training set: 155 (83.33%)\n",
      "Items in eval set: 31\n",
      "Items in test set: 129\n",
      " = 315\n",
      "Number of source documents: 41 total, 19 train, 3 eval 19 test\n",
      "Absolute (training): IR 23.00, N 37.00, R 54.00, HR 41.00\n",
      "Relative (training): IR 0.15, N 0.24, R 0.35, HR 0.26\n",
      "Absolute (eval): IR 3.00, N 3.00, R 8.00, HR 17.00\n",
      "Relative (eval): IR 0.10, N 0.10, R 0.26, HR 0.55\n",
      "Role samples for AFFILIATE in train: 155, eval: 31, test: 129\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.892312480085 | std = 0.0231252432616\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.720304562167\n",
      "Accuracy | role : 0.488372093023\n",
      "Accuracy | full : 0.496124031008\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.20      0.06      0.09        17\n",
      "    neutral       0.07      0.08      0.08        12\n",
      "   relevant       0.49      0.69      0.57        54\n",
      "     highly       0.71      0.54      0.62        46\n",
      "\n",
      "avg / total       0.49      0.50      0.48       129\n",
      "\n",
      "[[ 1  1 15  0]\n",
      " [ 1  1  9  1]\n",
      " [ 3  5 37  9]\n",
      " [ 0  7 14 25]]\n",
      "> NDCG Score | role | categ  | 0.95093\n",
      "> NDCG Score | role | proba* | 0.97226\n",
      "> NDCG Score | full | categ  | 0.93319\n",
      "> NDCG Score | full | proba* | 0.96299\n",
      "=== INSURER ======\n",
      "Items in training set: 18 (94.74%)\n",
      "Items in eval set: 1\n",
      "Items in test set: 47\n",
      " = 66\n",
      "Number of source documents: 15 total, 7 train, 1 eval 7 test\n",
      "Absolute (training): IR 1.00, N 1.00, R 7.00, HR 9.00\n",
      "Relative (training): IR 0.06, N 0.06, R 0.39, HR 0.50\n",
      "Absolute (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for INSURER in train: 18, eval: 1, test: 47\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.906263959845 | std = 0.0259821026274\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.755505248883\n",
      "Accuracy | role : 0.425531914894\n",
      "Accuracy | full : 0.68085106383\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         1\n",
      "    neutral       1.00      0.14      0.25         7\n",
      "   relevant       0.62      0.68      0.65        19\n",
      "     highly       0.72      0.90      0.80        20\n",
      "\n",
      "avg / total       0.71      0.68      0.64        47\n",
      "\n",
      "[[ 0  0  1  0]\n",
      " [ 0  1  5  1]\n",
      " [ 0  0 13  6]\n",
      " [ 0  0  2 18]]\n",
      "> NDCG Score | role | categ  | 0.89620\n",
      "> NDCG Score | role | proba* | 0.91170\n",
      "> NDCG Score | full | categ  | 0.93389\n",
      "> NDCG Score | full | proba* | 0.92517\n",
      "TOTAL NDCG | role | categ  | 0.96185\n",
      "TOTAL NDCG | role | proba* | 0.93688\n",
      "TOTAL NDCG | full | categ  | 0.97525\n",
      "TOTAL NDCG | full | proba* | 0.96588\n",
      "baseline_rand 10\n",
      "baseline_worst 10\n",
      "ndcg_role 10\n",
      "ndcg_full 10\n",
      "ndcg_role_proba 10\n",
      "ndcg_full_proba 10\n",
      "acc_role 10\n",
      "acc_full 10\n",
      "f1_role 10\n",
      "f1_full 10\n",
      "\n",
      "\n",
      "==========================================================================\n",
      "===                      CROSSEVAL ITERATION 2/5                     =====\n",
      "==========================================================================\n",
      "\n",
      "\n",
      "Items in training set: 787 (80.72%)\n",
      "Items in eval set: 188\n",
      "Items in test set: 900\n",
      " = 1875\n",
      "Number of source documents: 50 total, 22 train, 3 eval 25 test\n",
      "Absolute (training): IR 77.00, N 295.00, R 257.00, HR 158.00\n",
      "Relative (training): IR 0.10, N 0.37, R 0.33, HR 0.20\n",
      "Absolute (eval): IR 13.00, N 12.00, R 26.00, HR 137.00\n",
      "Relative (eval): IR 0.07, N 0.06, R 0.14, HR 0.73\n",
      "Role samples for TRUSTEE in train: 328, eval: 92, test: 304\n",
      "Role samples for COUNTERPART in train: 43, eval: 21, test: 108\n",
      "Role samples for SELLER in train: 18, eval: 2, test: 49\n",
      "Role samples for GUARANTOR in train: 34, eval: 0, test: 28\n",
      "Role samples for ISSUER in train: 119, eval: 10, test: 98\n",
      "Role samples for AGENT in train: 57, eval: 4, test: 40\n",
      "Role samples for UNDERWRITER in train: 17, eval: 4, test: 40\n",
      "Role samples for SERVICER in train: 14, eval: 7, test: 57\n",
      "Role samples for AFFILIATE in train: 149, eval: 37, test: 129\n",
      "Role samples for INSURER in train: 8, eval: 11, test: 47\n",
      "=== GUARANTOR ======\n",
      "Items in training set: 34 (100.00%)\n",
      "Items in eval set: 0\n",
      "Items in test set: 28\n",
      " = 62\n",
      "Number of source documents: 15 total, 6 train, 0 eval 9 test\n",
      "Absolute (training): IR 1.00, N 3.00, R 22.00, HR 8.00\n",
      "Relative (training): IR 0.03, N 0.09, R 0.65, HR 0.24\n",
      "Absolute (eval): IR 0.00, N 0.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR nan, N nan, R nan, HR nan\n",
      "Role samples for GUARANTOR in train: 34, eval: 0, test: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/Uni/HPI/workspace/FEII/code/feiii_data.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  rating_agg.sum()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.905324891457 | std = 0.0255478279368\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.810440241115\n",
      "Accuracy | role : 0.785714285714\n",
      "Accuracy | full : 0.5\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         0\n",
      "    neutral       0.17      0.33      0.22         3\n",
      "   relevant       0.79      0.50      0.61        22\n",
      "     highly       0.25      0.67      0.36         3\n",
      "\n",
      "avg / total       0.66      0.50      0.54        28\n",
      "\n",
      "[[ 0  0  0  0]\n",
      " [ 0  1  2  0]\n",
      " [ 0  5 11  6]\n",
      " [ 0  0  1  2]]\n",
      "> NDCG Score | role | categ  | 0.84067\n",
      "> NDCG Score | role | proba* | 0.90207\n",
      "> NDCG Score | full | categ  | 0.91938\n",
      "> NDCG Score | full | proba* | 0.94848\n",
      "=== ISSUER ======\n",
      "Items in training set: 119 (92.25%)\n",
      "Items in eval set: 10\n",
      "Items in test set: 98\n",
      " = 227\n",
      "Number of source documents: 46 total, 21 train, 3 eval 22 test\n",
      "Absolute (training): IR 27.00, N 43.00, R 28.00, HR 21.00\n",
      "Relative (training): IR 0.23, N 0.36, R 0.24, HR 0.18\n",
      "Absolute (eval): IR 5.00, N 2.00, R 2.00, HR 1.00\n",
      "Relative (eval): IR 0.50, N 0.20, R 0.20, HR 0.10\n",
      "Role samples for ISSUER in train: 119, eval: 10, test: 98\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.807876625219 | std = 0.0341858477199\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.621677771913\n",
      "Accuracy | role : 0.34693877551\n",
      "Accuracy | full : 0.336734693878\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.80      0.57      0.67        21\n",
      "    neutral       0.24      0.15      0.18        27\n",
      "   relevant       0.25      0.56      0.34        27\n",
      "     highly       0.33      0.09      0.14        23\n",
      "\n",
      "avg / total       0.38      0.34      0.32        98\n",
      "\n",
      "[[12  2  7  0]\n",
      " [ 2  4 18  3]\n",
      " [ 1 10 15  1]\n",
      " [ 0  1 20  2]]\n",
      "> NDCG Score | role | categ  | 0.85727\n",
      "> NDCG Score | role | proba* | 0.88242\n",
      "> NDCG Score | full | categ  | 0.86582\n",
      "> NDCG Score | full | proba* | 0.85733\n",
      "=== UNDERWRITER ======\n",
      "Items in training set: 17 (80.95%)\n",
      "Items in eval set: 4\n",
      "Items in test set: 40\n",
      " = 61\n",
      "Number of source documents: 16 total, 5 train, 2 eval 9 test\n",
      "Absolute (training): IR 0.00, N 0.00, R 14.00, HR 3.00\n",
      "Relative (training): IR 0.00, N 0.00, R 0.82, HR 0.18\n",
      "Absolute (eval): IR 0.00, N 0.00, R 0.00, HR 4.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 0.00, HR 1.00\n",
      "Role samples for UNDERWRITER in train: 17, eval: 4, test: 40\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.906009045562 | std = 0.0353011538561\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.721180267881\n",
      "Accuracy | role : 0.375\n",
      "Accuracy | full : 0.675\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.00      0.00      0.00         2\n",
      "   relevant       0.56      1.00      0.71        15\n",
      "     highly       1.00      0.60      0.75        20\n",
      "\n",
      "avg / total       0.71      0.68      0.64        40\n",
      "\n",
      "[[ 0  1  2  0]\n",
      " [ 0  0  2  0]\n",
      " [ 0  0 15  0]\n",
      " [ 0  0  8 12]]\n",
      "> NDCG Score | role | categ  | 0.86908\n",
      "> NDCG Score | role | proba* | 0.87247\n",
      "> NDCG Score | full | categ  | 0.99169\n",
      "> NDCG Score | full | proba* | 0.98345\n",
      "=== SERVICER ======\n",
      "Items in training set: 14 (66.67%)\n",
      "Items in eval set: 7\n",
      "Items in test set: 57\n",
      " = 78\n",
      "Number of source documents: 18 total, 6 train, 2 eval 10 test\n",
      "Absolute (training): IR 0.00, N 6.00, R 6.00, HR 2.00\n",
      "Relative (training): IR 0.00, N 0.43, R 0.43, HR 0.14\n",
      "Absolute (eval): IR 0.00, N 0.00, R 0.00, HR 7.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 0.00, HR 1.00\n",
      "Role samples for SERVICER in train: 14, eval: 7, test: 57\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.883668324941 | std = 0.0244836575862\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.717524169319\n",
      "Accuracy | role : 0.175438596491\n",
      "Accuracy | full : 0.473684210526\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.50      0.22      0.31         9\n",
      "   relevant       0.60      0.63      0.62        38\n",
      "     highly       0.08      0.14      0.10         7\n",
      "\n",
      "avg / total       0.49      0.47      0.47        57\n",
      "\n",
      "[[ 0  0  3  0]\n",
      " [ 0  2  7  0]\n",
      " [ 0  2 24 12]\n",
      " [ 0  0  6  1]]\n",
      "> NDCG Score | role | categ  | 0.85663\n",
      "> NDCG Score | role | proba* | 0.91809\n",
      "> NDCG Score | full | categ  | 0.89858\n",
      "> NDCG Score | full | proba* | 0.84893\n",
      "=== TRUSTEE ======\n",
      "Items in training set: 328 (78.10%)\n",
      "Items in eval set: 92\n",
      "Items in test set: 304\n",
      " = 724\n",
      "Number of source documents: 40 total, 19 train, 2 eval 19 test\n",
      "Absolute (training): IR 17.00, N 160.00, R 79.00, HR 72.00\n",
      "Relative (training): IR 0.05, N 0.49, R 0.24, HR 0.22\n",
      "Absolute (eval): IR 4.00, N 4.00, R 0.00, HR 84.00\n",
      "Relative (eval): IR 0.04, N 0.04, R 0.00, HR 0.91\n",
      "Role samples for TRUSTEE in train: 328, eval: 92, test: 304\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.931369592619 | std = 0.00998041432334\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.814144820697\n",
      "Accuracy | role : 0.473684210526\n",
      "Accuracy | full : 0.486842105263\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         9\n",
      "    neutral       0.15      0.32      0.20        47\n",
      "   relevant       0.38      0.30      0.33       124\n",
      "     highly       0.92      0.77      0.84       124\n",
      "\n",
      "avg / total       0.55      0.49      0.51       304\n",
      "\n",
      "[[ 0  3  6  0]\n",
      " [ 0 15 29  3]\n",
      " [ 0 82 37  5]\n",
      " [ 0  3 25 96]]\n",
      "> NDCG Score | role | categ  | 0.97811\n",
      "> NDCG Score | role | proba* | 0.98735\n",
      "> NDCG Score | full | categ  | 0.98735\n",
      "> NDCG Score | full | proba* | 0.98927\n",
      "=== COUNTERPART ======\n",
      "Items in training set: 43 (67.19%)\n",
      "Items in eval set: 21\n",
      "Items in test set: 108\n",
      " = 172\n",
      "Number of source documents: 31 total, 14 train, 3 eval 14 test\n",
      "Absolute (training): IR 3.00, N 8.00, R 25.00, HR 7.00\n",
      "Relative (training): IR 0.07, N 0.19, R 0.58, HR 0.16\n",
      "Absolute (eval): IR 3.00, N 4.00, R 5.00, HR 9.00\n",
      "Relative (eval): IR 0.14, N 0.19, R 0.24, HR 0.43\n",
      "Role samples for COUNTERPART in train: 43, eval: 21, test: 108\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.867251272895 | std = 0.0301443685155\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.677182649456\n",
      "Accuracy | role : 0.240740740741\n",
      "Accuracy | full : 0.351851851852\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00        18\n",
      "    neutral       0.00      0.00      0.00        17\n",
      "   relevant       0.34      0.79      0.48        38\n",
      "     highly       0.50      0.23      0.31        35\n",
      "\n",
      "avg / total       0.28      0.35      0.27       108\n",
      "\n",
      "[[ 0  3 15  0]\n",
      " [ 0  0 16  1]\n",
      " [ 0  1 30  7]\n",
      " [ 0  1 26  8]]\n",
      "> NDCG Score | role | categ  | 0.80440\n",
      "> NDCG Score | role | proba* | 0.76460\n",
      "> NDCG Score | full | categ  | 0.91528\n",
      "> NDCG Score | full | proba* | 0.90318\n",
      "=== SELLER ======\n",
      "Items in training set: 18 (90.00%)\n",
      "Items in eval set: 2\n",
      "Items in test set: 49\n",
      " = 69\n",
      "Number of source documents: 19 total, 8 train, 2 eval 9 test\n",
      "Absolute (training): IR 0.00, N 0.00, R 14.00, HR 4.00\n",
      "Relative (training): IR 0.00, N 0.00, R 0.78, HR 0.22\n",
      "Absolute (eval): IR 0.00, N 1.00, R 0.00, HR 1.00\n",
      "Relative (eval): IR 0.00, N 0.50, R 0.00, HR 0.50\n",
      "Role samples for SELLER in train: 18, eval: 2, test: 49\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.868229496588 | std = 0.0342601876169\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.674252161736\n",
      "Accuracy | role : 0.34693877551\n",
      "Accuracy | full : 0.510204081633\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.00      0.00      0.00        10\n",
      "   relevant       0.40      0.82      0.54        17\n",
      "     highly       0.92      0.61      0.73        18\n",
      "\n",
      "avg / total       0.48      0.51      0.46        49\n",
      "\n",
      "[[ 0  0  4  0]\n",
      " [ 0  0 10  0]\n",
      " [ 0  2 14  1]\n",
      " [ 0  0  7 11]]\n",
      "> NDCG Score | role | categ  | 0.80424\n",
      "> NDCG Score | role | proba* | 0.90947\n",
      "> NDCG Score | full | categ  | 0.96264\n",
      "> NDCG Score | full | proba* | 0.95860\n",
      "=== AGENT ======\n",
      "Items in training set: 57 (93.44%)\n",
      "Items in eval set: 4\n",
      "Items in test set: 40\n",
      " = 101\n",
      "Number of source documents: 25 total, 15 train, 1 eval 9 test\n",
      "Absolute (training): IR 3.00, N 35.00, R 14.00, HR 5.00\n",
      "Relative (training): IR 0.05, N 0.61, R 0.25, HR 0.09\n",
      "Absolute (eval): IR 0.00, N 0.00, R 4.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for AGENT in train: 57, eval: 4, test: 40\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.85234044097 | std = 0.0362345197849\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.650212572736\n",
      "Accuracy | role : 0.2\n",
      "Accuracy | full : 0.35\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.20      0.25      0.22         8\n",
      "   relevant       0.38      0.73      0.50        15\n",
      "     highly       1.00      0.08      0.14        13\n",
      "\n",
      "avg / total       0.51      0.35      0.28        40\n",
      "\n",
      "[[ 0  3  1  0]\n",
      " [ 0  2  6  0]\n",
      " [ 0  4 11  0]\n",
      " [ 0  1 11  1]]\n",
      "> NDCG Score | role | categ  | 0.81118\n",
      "> NDCG Score | role | proba* | 0.76213\n",
      "> NDCG Score | full | categ  | 0.91181\n",
      "> NDCG Score | full | proba* | 0.85700\n",
      "=== AFFILIATE ======\n",
      "Items in training set: 149 (80.11%)\n",
      "Items in eval set: 37\n",
      "Items in test set: 129\n",
      " = 315\n",
      "Number of source documents: 41 total, 19 train, 3 eval 19 test\n",
      "Absolute (training): IR 26.00, N 40.00, R 48.00, HR 35.00\n",
      "Relative (training): IR 0.17, N 0.27, R 0.32, HR 0.23\n",
      "Absolute (eval): IR 0.00, N 0.00, R 14.00, HR 23.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 0.38, HR 0.62\n",
      "Role samples for AFFILIATE in train: 149, eval: 37, test: 129\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.892177647289 | std = 0.0210444053902\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.720304562167\n",
      "Accuracy | role : 0.542635658915\n",
      "Accuracy | full : 0.53488372093\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00        17\n",
      "    neutral       0.18      0.25      0.21        12\n",
      "   relevant       0.51      0.72      0.60        54\n",
      "     highly       0.79      0.59      0.68        46\n",
      "\n",
      "avg / total       0.51      0.53      0.51       129\n",
      "\n",
      "[[ 0  2 15  0]\n",
      " [ 0  3  9  0]\n",
      " [ 1  7 39  7]\n",
      " [ 0  5 14 27]]\n",
      "> NDCG Score | role | categ  | 0.97733\n",
      "> NDCG Score | role | proba* | 0.97030\n",
      "> NDCG Score | full | categ  | 0.95964\n",
      "> NDCG Score | full | proba* | 0.95343\n",
      "=== INSURER ======\n",
      "Items in training set: 8 (42.11%)\n",
      "Items in eval set: 11\n",
      "Items in test set: 47\n",
      " = 66\n",
      "Number of source documents: 15 total, 6 train, 2 eval 7 test\n",
      "Absolute (training): IR 0.00, N 0.00, R 7.00, HR 1.00\n",
      "Relative (training): IR 0.00, N 0.00, R 0.88, HR 0.12\n",
      "Absolute (eval): IR 1.00, N 1.00, R 1.00, HR 8.00\n",
      "Relative (eval): IR 0.09, N 0.09, R 0.09, HR 0.73\n",
      "Role samples for INSURER in train: 8, eval: 11, test: 47\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.910414417966 | std = 0.0292974869853\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.755505248883\n",
      "Accuracy | role : 0.404255319149\n",
      "Accuracy | full : 0.468085106383\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         1\n",
      "    neutral       0.00      0.00      0.00         7\n",
      "   relevant       0.39      0.58      0.47        19\n",
      "     highly       0.65      0.55      0.59        20\n",
      "\n",
      "avg / total       0.43      0.47      0.44        47\n",
      "\n",
      "[[ 0  0  1  0]\n",
      " [ 0  0  7  0]\n",
      " [ 0  2 11  6]\n",
      " [ 0  0  9 11]]\n",
      "> NDCG Score | role | categ  | 0.89620\n",
      "> NDCG Score | role | proba* | 0.88323\n",
      "> NDCG Score | full | categ  | 0.96415\n",
      "> NDCG Score | full | proba* | 0.97563\n",
      "TOTAL NDCG | role | categ  | 0.95489\n",
      "TOTAL NDCG | role | proba* | 0.94060\n",
      "TOTAL NDCG | full | categ  | 0.97355\n",
      "TOTAL NDCG | full | proba* | 0.95644\n",
      "baseline_rand 10\n",
      "baseline_worst 10\n",
      "ndcg_role 10\n",
      "ndcg_full 10\n",
      "ndcg_role_proba 10\n",
      "ndcg_full_proba 10\n",
      "acc_role 10\n",
      "acc_full 10\n",
      "f1_role 10\n",
      "f1_full 10\n",
      "\n",
      "\n",
      "==========================================================================\n",
      "===                      CROSSEVAL ITERATION 3/5                     =====\n",
      "==========================================================================\n",
      "\n",
      "\n",
      "Items in training set: 844 (86.56%)\n",
      "Items in eval set: 131\n",
      "Items in test set: 900\n",
      " = 1875\n",
      "Number of source documents: 50 total, 22 train, 3 eval 25 test\n",
      "Absolute (training): IR 63.00, N 249.00, R 255.00, HR 277.00\n",
      "Relative (training): IR 0.07, N 0.30, R 0.30, HR 0.33\n",
      "Absolute (eval): IR 27.00, N 58.00, R 28.00, HR 18.00\n",
      "Relative (eval): IR 0.21, N 0.44, R 0.21, HR 0.14\n",
      "Role samples for TRUSTEE in train: 394, eval: 26, test: 304\n",
      "Role samples for COUNTERPART in train: 52, eval: 12, test: 108\n",
      "Role samples for SELLER in train: 14, eval: 6, test: 49\n",
      "Role samples for GUARANTOR in train: 32, eval: 2, test: 28\n",
      "Role samples for ISSUER in train: 97, eval: 32, test: 98\n",
      "Role samples for AGENT in train: 52, eval: 9, test: 40\n",
      "Role samples for UNDERWRITER in train: 19, eval: 2, test: 40\n",
      "Role samples for SERVICER in train: 15, eval: 6, test: 57\n",
      "Role samples for AFFILIATE in train: 152, eval: 34, test: 129\n",
      "Role samples for INSURER in train: 17, eval: 2, test: 47\n",
      "=== GUARANTOR ======\n",
      "Items in training set: 32 (94.12%)\n",
      "Items in eval set: 2\n",
      "Items in test set: 28\n",
      " = 62\n",
      "Number of source documents: 15 total, 5 train, 1 eval 9 test\n",
      "Absolute (training): IR 0.00, N 2.00, R 22.00, HR 8.00\n",
      "Relative (training): IR 0.00, N 0.06, R 0.69, HR 0.25\n",
      "Absolute (eval): IR 1.00, N 1.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR 0.50, N 0.50, R 0.00, HR 0.00\n",
      "Role samples for GUARANTOR in train: 32, eval: 2, test: 28\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.906869289134 | std = 0.0255132135446\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.810440241115\n",
      "Accuracy | role : 0.714285714286\n",
      "Accuracy | full : 0.464285714286\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         0\n",
      "    neutral       0.00      0.00      0.00         3\n",
      "   relevant       0.77      0.45      0.57        22\n",
      "     highly       0.30      1.00      0.46         3\n",
      "\n",
      "avg / total       0.64      0.46      0.50        28\n",
      "\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  3  0]\n",
      " [ 0  5 10  7]\n",
      " [ 0  0  0  3]]\n",
      "> NDCG Score | role | categ  | 0.93792\n",
      "> NDCG Score | role | proba* | 0.91263\n",
      "> NDCG Score | full | categ  | 0.94810\n",
      "> NDCG Score | full | proba* | 0.92187\n",
      "=== ISSUER ======\n",
      "Items in training set: 97 (75.19%)\n",
      "Items in eval set: 32\n",
      "Items in test set: 98\n",
      " = 227\n",
      "Number of source documents: 46 total, 21 train, 3 eval 22 test\n",
      "Absolute (training): IR 30.00, N 25.00, R 22.00, HR 20.00\n",
      "Relative (training): IR 0.31, N 0.26, R 0.23, HR 0.21\n",
      "Absolute (eval): IR 2.00, N 20.00, R 8.00, HR 2.00\n",
      "Relative (eval): IR 0.06, N 0.62, R 0.25, HR 0.06\n",
      "Role samples for ISSUER in train: 97, eval: 32, test: 98\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.811261229913 | std = 0.0334767635479\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.621677771913\n",
      "Accuracy | role : 0.34693877551\n",
      "Accuracy | full : 0.295918367347\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.69      0.43      0.53        21\n",
      "    neutral       0.20      0.11      0.14        27\n",
      "   relevant       0.24      0.41      0.30        27\n",
      "     highly       0.25      0.26      0.26        23\n",
      "\n",
      "avg / total       0.33      0.30      0.30        98\n",
      "\n",
      "[[ 9  2  8  2]\n",
      " [ 2  3 12 10]\n",
      " [ 2  8 11  6]\n",
      " [ 0  2 15  6]]\n",
      "> NDCG Score | role | categ  | 0.83091\n",
      "> NDCG Score | role | proba* | 0.87381\n",
      "> NDCG Score | full | categ  | 0.85150\n",
      "> NDCG Score | full | proba* | 0.85186\n",
      "=== UNDERWRITER ======\n",
      "Items in training set: 19 (90.48%)\n",
      "Items in eval set: 2\n",
      "Items in test set: 40\n",
      " = 61\n",
      "Number of source documents: 16 total, 6 train, 1 eval 9 test\n",
      "Absolute (training): IR 0.00, N 0.00, R 14.00, HR 5.00\n",
      "Relative (training): IR 0.00, N 0.00, R 0.74, HR 0.26\n",
      "Absolute (eval): IR 0.00, N 0.00, R 0.00, HR 2.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 0.00, HR 1.00\n",
      "Role samples for UNDERWRITER in train: 19, eval: 2, test: 40\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.910469493423 | std = 0.0356695112374\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.721180267881\n",
      "Accuracy | role : 0.525\n",
      "Accuracy | full : 0.55\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.00      0.00      0.00         2\n",
      "   relevant       0.52      0.73      0.61        15\n",
      "     highly       0.65      0.55      0.59        20\n",
      "\n",
      "avg / total       0.52      0.55      0.53        40\n",
      "\n",
      "[[ 0  1  2  0]\n",
      " [ 0  0  0  2]\n",
      " [ 0  0 11  4]\n",
      " [ 0  1  8 11]]\n",
      "> NDCG Score | role | categ  | 0.93192\n",
      "> NDCG Score | role | proba* | 0.95056\n",
      "> NDCG Score | full | categ  | 0.94005\n",
      "> NDCG Score | full | proba* | 0.93860\n",
      "=== SERVICER ======\n",
      "Items in training set: 15 (71.43%)\n",
      "Items in eval set: 6\n",
      "Items in test set: 57\n",
      " = 78\n",
      "Number of source documents: 18 total, 5 train, 3 eval 10 test\n",
      "Absolute (training): IR 0.00, N 3.00, R 3.00, HR 9.00\n",
      "Relative (training): IR 0.00, N 0.20, R 0.20, HR 0.60\n",
      "Absolute (eval): IR 0.00, N 3.00, R 3.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.50, R 0.50, HR 0.00\n",
      "Role samples for SERVICER in train: 15, eval: 6, test: 57\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.878927360557 | std = 0.0243943259116\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.717524169319\n",
      "Accuracy | role : 0.122807017544\n",
      "Accuracy | full : 0.228070175439\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.29      0.22      0.25         9\n",
      "   relevant       0.38      0.21      0.27        38\n",
      "     highly       0.10      0.43      0.17         7\n",
      "\n",
      "avg / total       0.31      0.23      0.24        57\n",
      "\n",
      "[[ 0  0  2  1]\n",
      " [ 0  2  7  0]\n",
      " [ 0  5  8 25]\n",
      " [ 0  0  4  3]]\n",
      "> NDCG Score | role | categ  | 0.85663\n",
      "> NDCG Score | role | proba* | 0.83103\n",
      "> NDCG Score | full | categ  | 0.92354\n",
      "> NDCG Score | full | proba* | 0.85871\n",
      "=== TRUSTEE ======\n",
      "Items in training set: 394 (93.81%)\n",
      "Items in eval set: 26\n",
      "Items in test set: 304\n",
      " = 724\n",
      "Number of source documents: 40 total, 18 train, 3 eval 19 test\n",
      "Absolute (training): IR 5.00, N 154.00, R 79.00, HR 156.00\n",
      "Relative (training): IR 0.01, N 0.39, R 0.20, HR 0.40\n",
      "Absolute (eval): IR 16.00, N 10.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR 0.62, N 0.38, R 0.00, HR 0.00\n",
      "Role samples for TRUSTEE in train: 394, eval: 26, test: 304\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.930752530579 | std = 0.0098018586128\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.814144820697\n",
      "Accuracy | role : 0.480263157895\n",
      "Accuracy | full : 0.506578947368\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         9\n",
      "    neutral       0.16      0.28      0.20        47\n",
      "   relevant       0.53      0.22      0.31       124\n",
      "     highly       0.67      0.92      0.77       124\n",
      "\n",
      "avg / total       0.51      0.51      0.47       304\n",
      "\n",
      "[[  0   3   1   5]\n",
      " [  1  13  14  19]\n",
      " [  0  64  27  33]\n",
      " [  0   1   9 114]]\n",
      "> NDCG Score | role | categ  | 0.97351\n",
      "> NDCG Score | role | proba* | 0.98857\n",
      "> NDCG Score | full | categ  | 0.97343\n",
      "> NDCG Score | full | proba* | 0.99056\n",
      "=== COUNTERPART ======\n",
      "Items in training set: 52 (81.25%)\n",
      "Items in eval set: 12\n",
      "Items in test set: 108\n",
      " = 172\n",
      "Number of source documents: 31 total, 14 train, 3 eval 14 test\n",
      "Absolute (training): IR 4.00, N 10.00, R 24.00, HR 14.00\n",
      "Relative (training): IR 0.08, N 0.19, R 0.46, HR 0.27\n",
      "Absolute (eval): IR 2.00, N 2.00, R 6.00, HR 2.00\n",
      "Relative (eval): IR 0.17, N 0.17, R 0.50, HR 0.17\n",
      "Role samples for COUNTERPART in train: 52, eval: 12, test: 108\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.861172247415 | std = 0.028198091689\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.677182649456\n",
      "Accuracy | role : 0.342592592593\n",
      "Accuracy | full : 0.435185185185\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       1.00      0.06      0.11        18\n",
      "    neutral       0.50      0.24      0.32        17\n",
      "   relevant       0.35      0.50      0.41        38\n",
      "     highly       0.51      0.66      0.57        35\n",
      "\n",
      "avg / total       0.53      0.44      0.40       108\n",
      "\n",
      "[[ 1  2 14  1]\n",
      " [ 0  4  9  4]\n",
      " [ 0  2 19 17]\n",
      " [ 0  0 12 23]]\n",
      "> NDCG Score | role | categ  | 0.90525\n",
      "> NDCG Score | role | proba* | 0.78605\n",
      "> NDCG Score | full | categ  | 0.91764\n",
      "> NDCG Score | full | proba* | 0.94644\n",
      "=== SELLER ======\n",
      "Items in training set: 14 (70.00%)\n",
      "Items in eval set: 6\n",
      "Items in test set: 49\n",
      " = 69\n",
      "Number of source documents: 19 total, 8 train, 2 eval 9 test\n",
      "Absolute (training): IR 0.00, N 1.00, R 12.00, HR 1.00\n",
      "Relative (training): IR 0.00, N 0.07, R 0.86, HR 0.07\n",
      "Absolute (eval): IR 0.00, N 0.00, R 2.00, HR 4.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 0.33, HR 0.67\n",
      "Role samples for SELLER in train: 14, eval: 6, test: 49\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.868881436019 | std = 0.0351805745629\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.674252161736\n",
      "Accuracy | role : 0.34693877551\n",
      "Accuracy | full : 0.510204081633\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.00      0.00      0.00        10\n",
      "   relevant       0.40      0.59      0.48        17\n",
      "     highly       0.75      0.83      0.79        18\n",
      "\n",
      "avg / total       0.41      0.51      0.46        49\n",
      "\n",
      "[[ 0  0  4  0]\n",
      " [ 0  0  8  2]\n",
      " [ 0  4 10  3]\n",
      " [ 0  0  3 15]]\n",
      "> NDCG Score | role | categ  | 0.80424\n",
      "> NDCG Score | role | proba* | 0.93901\n",
      "> NDCG Score | full | categ  | 0.95808\n",
      "> NDCG Score | full | proba* | 0.96108\n",
      "=== AGENT ======\n",
      "Items in training set: 52 (85.25%)\n",
      "Items in eval set: 9\n",
      "Items in test set: 40\n",
      " = 101\n",
      "Number of source documents: 25 total, 14 train, 2 eval 9 test\n",
      "Absolute (training): IR 1.00, N 30.00, R 16.00, HR 5.00\n",
      "Relative (training): IR 0.02, N 0.58, R 0.31, HR 0.10\n",
      "Absolute (eval): IR 2.00, N 5.00, R 2.00, HR 0.00\n",
      "Relative (eval): IR 0.22, N 0.56, R 0.22, HR 0.00\n",
      "Role samples for AGENT in train: 52, eval: 9, test: 40\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.85234128452 | std = 0.0435860187421\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.650212572736\n",
      "Accuracy | role : 0.325\n",
      "Accuracy | full : 0.425\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.29      0.25      0.27         8\n",
      "   relevant       0.45      0.60      0.51        15\n",
      "     highly       0.46      0.46      0.46        13\n",
      "\n",
      "avg / total       0.38      0.42      0.40        40\n",
      "\n",
      "[[0 3 1 0]\n",
      " [0 2 3 3]\n",
      " [0 2 9 4]\n",
      " [0 0 7 6]]\n",
      "> NDCG Score | role | categ  | 0.83171\n",
      "> NDCG Score | role | proba* | 0.78325\n",
      "> NDCG Score | full | categ  | 0.86348\n",
      "> NDCG Score | full | proba* | 0.87004\n",
      "=== AFFILIATE ======\n",
      "Items in training set: 152 (81.72%)\n",
      "Items in eval set: 34\n",
      "Items in test set: 129\n",
      " = 315\n",
      "Number of source documents: 41 total, 19 train, 3 eval 19 test\n",
      "Absolute (training): IR 22.00, N 23.00, R 57.00, HR 50.00\n",
      "Relative (training): IR 0.14, N 0.15, R 0.38, HR 0.33\n",
      "Absolute (eval): IR 4.00, N 17.00, R 5.00, HR 8.00\n",
      "Relative (eval): IR 0.12, N 0.50, R 0.15, HR 0.24\n",
      "Role samples for AFFILIATE in train: 152, eval: 34, test: 129\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.893092574142 | std = 0.0212808638697\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.720304562167\n",
      "Accuracy | role : 0.480620155039\n",
      "Accuracy | full : 0.511627906977\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.14      0.06      0.08        17\n",
      "    neutral       0.00      0.00      0.00        12\n",
      "   relevant       0.52      0.59      0.55        54\n",
      "     highly       0.59      0.72      0.65        46\n",
      "\n",
      "avg / total       0.45      0.51      0.47       129\n",
      "\n",
      "[[ 1  2 12  2]\n",
      " [ 2  0  6  4]\n",
      " [ 4  1 32 17]\n",
      " [ 0  1 12 33]]\n",
      "> NDCG Score | role | categ  | 0.94274\n",
      "> NDCG Score | role | proba* | 0.97644\n",
      "> NDCG Score | full | categ  | 0.92611\n",
      "> NDCG Score | full | proba* | 0.97893\n",
      "=== INSURER ======\n",
      "Items in training set: 17 (89.47%)\n",
      "Items in eval set: 2\n",
      "Items in test set: 47\n",
      " = 66\n",
      "Number of source documents: 15 total, 7 train, 1 eval 7 test\n",
      "Absolute (training): IR 1.00, N 1.00, R 6.00, HR 9.00\n",
      "Relative (training): IR 0.06, N 0.06, R 0.35, HR 0.53\n",
      "Absolute (eval): IR 0.00, N 0.00, R 2.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for INSURER in train: 17, eval: 2, test: 47\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.903863743192 | std = 0.0311355147878\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.755505248883\n",
      "Accuracy | role : 0.446808510638\n",
      "Accuracy | full : 0.531914893617\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         1\n",
      "    neutral       0.00      0.00      0.00         7\n",
      "   relevant       0.45      0.47      0.46        19\n",
      "     highly       0.62      0.80      0.70        20\n",
      "\n",
      "avg / total       0.44      0.53      0.48        47\n",
      "\n",
      "[[ 0  0  1  0]\n",
      " [ 0  0  6  1]\n",
      " [ 0  1  9  9]\n",
      " [ 0  0  4 16]]\n",
      "> NDCG Score | role | categ  | 0.92402\n",
      "> NDCG Score | role | proba* | 0.93170\n",
      "> NDCG Score | full | categ  | 0.95295\n",
      "> NDCG Score | full | proba* | 0.98303\n",
      "TOTAL NDCG | role | categ  | 0.94500\n",
      "TOTAL NDCG | role | proba* | 0.96156\n",
      "TOTAL NDCG | full | categ  | 0.96773\n",
      "TOTAL NDCG | full | proba* | 0.96267\n",
      "baseline_rand 10\n",
      "baseline_worst 10\n",
      "ndcg_role 10\n",
      "ndcg_full 10\n",
      "ndcg_role_proba 10\n",
      "ndcg_full_proba 10\n",
      "acc_role 10\n",
      "acc_full 10\n",
      "f1_role 10\n",
      "f1_full 10\n",
      "\n",
      "\n",
      "==========================================================================\n",
      "===                      CROSSEVAL ITERATION 4/5                     =====\n",
      "==========================================================================\n",
      "\n",
      "\n",
      "Items in training set: 862 (88.41%)\n",
      "Items in eval set: 113\n",
      "Items in test set: 900\n",
      " = 1875\n",
      "Number of source documents: 50 total, 22 train, 3 eval 25 test\n",
      "Absolute (training): IR 70.00, N 256.00, R 249.00, HR 287.00\n",
      "Relative (training): IR 0.08, N 0.30, R 0.29, HR 0.33\n",
      "Absolute (eval): IR 20.00, N 51.00, R 34.00, HR 8.00\n",
      "Relative (eval): IR 0.18, N 0.45, R 0.30, HR 0.07\n",
      "Role samples for TRUSTEE in train: 390, eval: 30, test: 304\n",
      "Role samples for COUNTERPART in train: 55, eval: 9, test: 108\n",
      "Role samples for SELLER in train: 20, eval: 0, test: 49\n",
      "Role samples for GUARANTOR in train: 22, eval: 12, test: 28\n",
      "Role samples for ISSUER in train: 113, eval: 16, test: 98\n",
      "Role samples for AGENT in train: 49, eval: 12, test: 40\n",
      "Role samples for UNDERWRITER in train: 14, eval: 7, test: 40\n",
      "Role samples for SERVICER in train: 21, eval: 0, test: 57\n",
      "Role samples for AFFILIATE in train: 160, eval: 26, test: 129\n",
      "Role samples for INSURER in train: 18, eval: 1, test: 47\n",
      "=== GUARANTOR ======\n",
      "Items in training set: 22 (64.71%)\n",
      "Items in eval set: 12\n",
      "Items in test set: 28\n",
      " = 62\n",
      "Number of source documents: 15 total, 5 train, 1 eval 9 test\n",
      "Absolute (training): IR 1.00, N 3.00, R 10.00, HR 8.00\n",
      "Relative (training): IR 0.05, N 0.14, R 0.45, HR 0.36\n",
      "Absolute (eval): IR 0.00, N 0.00, R 12.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for GUARANTOR in train: 22, eval: 12, test: 28\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.905541640706 | std = 0.027626630816\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.810440241115\n",
      "Accuracy | role : 0.607142857143\n",
      "Accuracy | full : 0.535714285714\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         0\n",
      "    neutral       0.20      0.33      0.25         3\n",
      "   relevant       0.85      0.50      0.63        22\n",
      "     highly       0.30      1.00      0.46         3\n",
      "\n",
      "avg / total       0.72      0.54      0.57        28\n",
      "\n",
      "[[ 0  0  0  0]\n",
      " [ 0  1  2  0]\n",
      " [ 0  4 11  7]\n",
      " [ 0  0  0  3]]\n",
      "> NDCG Score | role | categ  | 0.91227\n",
      "> NDCG Score | role | proba* | 0.96087\n",
      "> NDCG Score | full | categ  | 0.94953\n",
      "> NDCG Score | full | proba* | 0.92726\n",
      "=== ISSUER ======\n",
      "Items in training set: 113 (87.60%)\n",
      "Items in eval set: 16\n",
      "Items in test set: 98\n",
      " = 227\n",
      "Number of source documents: 46 total, 21 train, 3 eval 22 test\n",
      "Absolute (training): IR 25.00, N 38.00, R 30.00, HR 20.00\n",
      "Relative (training): IR 0.22, N 0.34, R 0.27, HR 0.18\n",
      "Absolute (eval): IR 7.00, N 7.00, R 0.00, HR 2.00\n",
      "Relative (eval): IR 0.44, N 0.44, R 0.00, HR 0.12\n",
      "Role samples for ISSUER in train: 113, eval: 16, test: 98\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.815393615246 | std = 0.0325199993154\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.621677771913\n",
      "Accuracy | role : 0.336734693878\n",
      "Accuracy | full : 0.285714285714\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.67      0.38      0.48        21\n",
      "    neutral       0.14      0.07      0.10        27\n",
      "   relevant       0.23      0.44      0.30        27\n",
      "     highly       0.30      0.26      0.28        23\n",
      "\n",
      "avg / total       0.32      0.29      0.28        98\n",
      "\n",
      "[[ 8  3  7  3]\n",
      " [ 2  2 17  6]\n",
      " [ 2  8 12  5]\n",
      " [ 0  1 16  6]]\n",
      "> NDCG Score | role | categ  | 0.89409\n",
      "> NDCG Score | role | proba* | 0.75693\n",
      "> NDCG Score | full | categ  | 0.83360\n",
      "> NDCG Score | full | proba* | 0.86607\n",
      "=== UNDERWRITER ======\n",
      "Items in training set: 14 (66.67%)\n",
      "Items in eval set: 7\n",
      "Items in test set: 40\n",
      " = 61\n",
      "Number of source documents: 16 total, 6 train, 1 eval 9 test\n",
      "Absolute (training): IR 0.00, N 0.00, R 7.00, HR 7.00\n",
      "Relative (training): IR 0.00, N 0.00, R 0.50, HR 0.50\n",
      "Absolute (eval): IR 0.00, N 0.00, R 7.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for UNDERWRITER in train: 14, eval: 7, test: 40\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.913116354475 | std = 0.0325111290428\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.721180267881\n",
      "Accuracy | role : 0.475\n",
      "Accuracy | full : 0.475\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.00      0.00      0.00         2\n",
      "   relevant       0.38      0.40      0.39        15\n",
      "     highly       0.62      0.65      0.63        20\n",
      "\n",
      "avg / total       0.45      0.47      0.46        40\n",
      "\n",
      "[[ 0  0  3  0]\n",
      " [ 0  0  0  2]\n",
      " [ 0  3  6  6]\n",
      " [ 0  0  7 13]]\n",
      "> NDCG Score | role | categ  | 0.95653\n",
      "> NDCG Score | role | proba* | 0.96143\n",
      "> NDCG Score | full | categ  | 0.95765\n",
      "> NDCG Score | full | proba* | 0.96169\n",
      "=== SERVICER ======\n",
      "Items in training set: 21 (100.00%)\n",
      "Items in eval set: 0\n",
      "Items in test set: 57\n",
      " = 78\n",
      "Number of source documents: 18 total, 8 train, 0 eval 10 test\n",
      "Absolute (training): IR 0.00, N 6.00, R 6.00, HR 9.00\n",
      "Relative (training): IR 0.00, N 0.29, R 0.29, HR 0.43\n",
      "Absolute (eval): IR 0.00, N 0.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR nan, N nan, R nan, HR nan\n",
      "Role samples for SERVICER in train: 21, eval: 0, test: 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/Uni/HPI/workspace/FEII/code/feiii_data.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  rating_agg.sum()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.882757704292 | std = 0.02316517598\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.717524169319\n",
      "Accuracy | role : 0.157894736842\n",
      "Accuracy | full : 0.385964912281\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.17      0.11      0.13         9\n",
      "   relevant       0.56      0.47      0.51        38\n",
      "     highly       0.16      0.43      0.23         7\n",
      "\n",
      "avg / total       0.42      0.39      0.39        57\n",
      "\n",
      "[[ 0  1  2  0]\n",
      " [ 0  1  8  0]\n",
      " [ 0  4 18 16]\n",
      " [ 0  0  4  3]]\n",
      "> NDCG Score | role | categ  | 0.85835\n",
      "> NDCG Score | role | proba* | 0.90150\n",
      "> NDCG Score | full | categ  | 0.91187\n",
      "> NDCG Score | full | proba* | 0.90128\n",
      "=== TRUSTEE ======\n",
      "Items in training set: 390 (92.86%)\n",
      "Items in eval set: 30\n",
      "Items in test set: 304\n",
      " = 724\n",
      "Number of source documents: 40 total, 19 train, 2 eval 19 test\n",
      "Absolute (training): IR 20.00, N 137.00, R 78.00, HR 155.00\n",
      "Relative (training): IR 0.05, N 0.35, R 0.20, HR 0.40\n",
      "Absolute (eval): IR 1.00, N 27.00, R 1.00, HR 1.00\n",
      "Relative (eval): IR 0.03, N 0.90, R 0.03, HR 0.03\n",
      "Role samples for TRUSTEE in train: 390, eval: 30, test: 304\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.931529735539 | std = 0.0104156144629\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.814144820697\n",
      "Accuracy | role : 0.496710526316\n",
      "Accuracy | full : 0.5\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         9\n",
      "    neutral       0.15      0.28      0.20        47\n",
      "   relevant       0.42      0.16      0.23       124\n",
      "     highly       0.70      0.96      0.81       124\n",
      "\n",
      "avg / total       0.48      0.50      0.45       304\n",
      "\n",
      "[[  0   3   2   4]\n",
      " [  1  13  21  12]\n",
      " [  0  68  20  36]\n",
      " [  0   0   5 119]]\n",
      "> NDCG Score | role | categ  | 0.97374\n",
      "> NDCG Score | role | proba* | 0.98811\n",
      "> NDCG Score | full | categ  | 0.98211\n",
      "> NDCG Score | full | proba* | 0.99244\n",
      "=== COUNTERPART ======\n",
      "Items in training set: 55 (85.94%)\n",
      "Items in eval set: 9\n",
      "Items in test set: 108\n",
      " = 172\n",
      "Number of source documents: 31 total, 14 train, 3 eval 14 test\n",
      "Absolute (training): IR 6.00, N 11.00, R 23.00, HR 15.00\n",
      "Relative (training): IR 0.11, N 0.20, R 0.42, HR 0.27\n",
      "Absolute (eval): IR 0.00, N 1.00, R 7.00, HR 1.00\n",
      "Relative (eval): IR 0.00, N 0.11, R 0.78, HR 0.11\n",
      "Role samples for COUNTERPART in train: 55, eval: 9, test: 108\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.860756498205 | std = 0.0295078982385\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.677182649456\n",
      "Accuracy | role : 0.37037037037\n",
      "Accuracy | full : 0.407407407407\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00        18\n",
      "    neutral       0.14      0.12      0.13        17\n",
      "   relevant       0.35      0.55      0.43        38\n",
      "     highly       0.62      0.60      0.61        35\n",
      "\n",
      "avg / total       0.35      0.41      0.37       108\n",
      "\n",
      "[[ 0  4 14  0]\n",
      " [ 0  2 12  3]\n",
      " [ 0  7 21 10]\n",
      " [ 0  1 13 21]]\n",
      "> NDCG Score | role | categ  | 0.93008\n",
      "> NDCG Score | role | proba* | 0.91926\n",
      "> NDCG Score | full | categ  | 0.96318\n",
      "> NDCG Score | full | proba* | 0.95639\n",
      "=== SELLER ======\n",
      "Items in training set: 20 (100.00%)\n",
      "Items in eval set: 0\n",
      "Items in test set: 49\n",
      " = 69\n",
      "Number of source documents: 19 total, 10 train, 0 eval 9 test\n",
      "Absolute (training): IR 0.00, N 1.00, R 14.00, HR 5.00\n",
      "Relative (training): IR 0.00, N 0.05, R 0.70, HR 0.25\n",
      "Absolute (eval): IR 0.00, N 0.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR nan, N nan, R nan, HR nan\n",
      "Role samples for SELLER in train: 20, eval: 0, test: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/Uni/HPI/workspace/FEII/code/feiii_data.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  rating_agg.sum()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.867105950862 | std = 0.0375874481987\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.674252161736\n",
      "Accuracy | role : 0.34693877551\n",
      "Accuracy | full : 0.469387755102\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.00      0.00      0.00        10\n",
      "   relevant       0.39      0.65      0.49        17\n",
      "     highly       0.60      0.67      0.63        18\n",
      "\n",
      "avg / total       0.36      0.47      0.40        49\n",
      "\n",
      "[[ 0  0  3  1]\n",
      " [ 0  0  9  1]\n",
      " [ 0  0 11  6]\n",
      " [ 0  1  5 12]]\n",
      "> NDCG Score | role | categ  | 0.80424\n",
      "> NDCG Score | role | proba* | 0.95403\n",
      "> NDCG Score | full | categ  | 0.94446\n",
      "> NDCG Score | full | proba* | 0.93493\n",
      "=== AGENT ======\n",
      "Items in training set: 49 (80.33%)\n",
      "Items in eval set: 12\n",
      "Items in test set: 40\n",
      " = 101\n",
      "Number of source documents: 25 total, 13 train, 3 eval 9 test\n",
      "Absolute (training): IR 2.00, N 27.00, R 17.00, HR 3.00\n",
      "Relative (training): IR 0.04, N 0.55, R 0.35, HR 0.06\n",
      "Absolute (eval): IR 1.00, N 8.00, R 1.00, HR 2.00\n",
      "Relative (eval): IR 0.08, N 0.67, R 0.08, HR 0.17\n",
      "Role samples for AGENT in train: 49, eval: 12, test: 40\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.859654460008 | std = 0.0470132507564\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.650212572736\n",
      "Accuracy | role : 0.35\n",
      "Accuracy | full : 0.375\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.14      0.12      0.13         8\n",
      "   relevant       0.40      0.53      0.46        15\n",
      "     highly       0.46      0.46      0.46        13\n",
      "\n",
      "avg / total       0.33      0.38      0.35        40\n",
      "\n",
      "[[0 2 1 1]\n",
      " [0 1 4 3]\n",
      " [0 4 8 3]\n",
      " [0 0 7 6]]\n",
      "> NDCG Score | role | categ  | 0.90456\n",
      "> NDCG Score | role | proba* | 0.82898\n",
      "> NDCG Score | full | categ  | 0.86880\n",
      "> NDCG Score | full | proba* | 0.83880\n",
      "=== AFFILIATE ======\n",
      "Items in training set: 160 (86.02%)\n",
      "Items in eval set: 26\n",
      "Items in test set: 129\n",
      " = 315\n",
      "Number of source documents: 41 total, 19 train, 3 eval 19 test\n",
      "Absolute (training): IR 15.00, N 32.00, R 57.00, HR 56.00\n",
      "Relative (training): IR 0.09, N 0.20, R 0.36, HR 0.35\n",
      "Absolute (eval): IR 11.00, N 8.00, R 5.00, HR 2.00\n",
      "Relative (eval): IR 0.42, N 0.31, R 0.19, HR 0.08\n",
      "Role samples for AFFILIATE in train: 160, eval: 26, test: 129\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.894207838815 | std = 0.0245708850483\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.720304562167\n",
      "Accuracy | role : 0.53488372093\n",
      "Accuracy | full : 0.472868217054\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00        17\n",
      "    neutral       0.15      0.17      0.16        12\n",
      "   relevant       0.44      0.50      0.47        54\n",
      "     highly       0.62      0.70      0.65        46\n",
      "\n",
      "avg / total       0.42      0.47      0.44       129\n",
      "\n",
      "[[ 0  2 15  0]\n",
      " [ 0  2  8  2]\n",
      " [ 3  6 27 18]\n",
      " [ 0  3 11 32]]\n",
      "> NDCG Score | role | categ  | 0.93934\n",
      "> NDCG Score | role | proba* | 0.97101\n",
      "> NDCG Score | full | categ  | 0.93966\n",
      "> NDCG Score | full | proba* | 0.98252\n",
      "=== INSURER ======\n",
      "Items in training set: 18 (94.74%)\n",
      "Items in eval set: 1\n",
      "Items in test set: 47\n",
      " = 66\n",
      "Number of source documents: 15 total, 7 train, 1 eval 7 test\n",
      "Absolute (training): IR 1.00, N 1.00, R 7.00, HR 9.00\n",
      "Relative (training): IR 0.06, N 0.06, R 0.39, HR 0.50\n",
      "Absolute (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for INSURER in train: 18, eval: 1, test: 47\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.910240085707 | std = 0.0270978748697\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.755505248883\n",
      "Accuracy | role : 0.404255319149\n",
      "Accuracy | full : 0.510638297872\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         1\n",
      "    neutral       1.00      0.14      0.25         7\n",
      "   relevant       0.42      0.42      0.42        19\n",
      "     highly       0.56      0.75      0.64        20\n",
      "\n",
      "avg / total       0.56      0.51      0.48        47\n",
      "\n",
      "[[ 0  0  1  0]\n",
      " [ 0  1  5  1]\n",
      " [ 0  0  8 11]\n",
      " [ 0  0  5 15]]\n",
      "> NDCG Score | role | categ  | 0.89579\n",
      "> NDCG Score | role | proba* | 0.85498\n",
      "> NDCG Score | full | categ  | 0.91815\n",
      "> NDCG Score | full | proba* | 0.97594\n",
      "TOTAL NDCG | role | categ  | 0.97528\n",
      "TOTAL NDCG | role | proba* | 0.95567\n",
      "TOTAL NDCG | full | categ  | 0.96318\n",
      "TOTAL NDCG | full | proba* | 0.97095\n",
      "baseline_rand 10\n",
      "baseline_worst 10\n",
      "ndcg_role 10\n",
      "ndcg_full 10\n",
      "ndcg_role_proba 10\n",
      "ndcg_full_proba 10\n",
      "acc_role 10\n",
      "acc_full 10\n",
      "f1_role 10\n",
      "f1_full 10\n",
      "\n",
      "\n",
      "==========================================================================\n",
      "===                      CROSSEVAL ITERATION 5/5                     =====\n",
      "==========================================================================\n",
      "\n",
      "\n",
      "Items in training set: 849 (87.08%)\n",
      "Items in eval set: 126\n",
      "Items in test set: 900\n",
      " = 1875\n",
      "Number of source documents: 50 total, 22 train, 3 eval 25 test\n",
      "Absolute (training): IR 87.00, N 240.00, R 241.00, HR 281.00\n",
      "Relative (training): IR 0.10, N 0.28, R 0.28, HR 0.33\n",
      "Absolute (eval): IR 3.00, N 67.00, R 42.00, HR 14.00\n",
      "Relative (eval): IR 0.02, N 0.53, R 0.33, HR 0.11\n",
      "Role samples for TRUSTEE in train: 373, eval: 47, test: 304\n",
      "Role samples for COUNTERPART in train: 58, eval: 6, test: 108\n",
      "Role samples for SELLER in train: 18, eval: 2, test: 49\n",
      "Role samples for GUARANTOR in train: 19, eval: 15, test: 28\n",
      "Role samples for ISSUER in train: 116, eval: 13, test: 98\n",
      "Role samples for AGENT in train: 53, eval: 8, test: 40\n",
      "Role samples for UNDERWRITER in train: 21, eval: 0, test: 40\n",
      "Role samples for SERVICER in train: 18, eval: 3, test: 57\n",
      "Role samples for AFFILIATE in train: 154, eval: 32, test: 129\n",
      "Role samples for INSURER in train: 19, eval: 0, test: 47\n",
      "=== GUARANTOR ======\n",
      "Items in training set: 19 (55.88%)\n",
      "Items in eval set: 15\n",
      "Items in test set: 28\n",
      " = 62\n",
      "Number of source documents: 15 total, 4 train, 2 eval 9 test\n",
      "Absolute (training): IR 1.00, N 1.00, R 17.00, HR 0.00\n",
      "Relative (training): IR 0.05, N 0.05, R 0.89, HR 0.00\n",
      "Absolute (eval): IR 0.00, N 2.00, R 5.00, HR 8.00\n",
      "Relative (eval): IR 0.00, N 0.13, R 0.33, HR 0.53\n",
      "Role samples for GUARANTOR in train: 19, eval: 15, test: 28\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.901606874435 | std = 0.023864473144\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.810440241115\n",
      "Accuracy | role : 0.785714285714\n",
      "Accuracy | full : 0.5\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         0\n",
      "    neutral       0.00      0.00      0.00         3\n",
      "   relevant       0.72      0.59      0.65        22\n",
      "     highly       0.33      0.33      0.33         3\n",
      "\n",
      "avg / total       0.60      0.50      0.55        28\n",
      "\n",
      "[[ 0  0  0  0]\n",
      " [ 0  0  3  0]\n",
      " [ 0  7 13  2]\n",
      " [ 0  0  2  1]]\n",
      "> NDCG Score | role | categ  | 0.84067\n",
      "> NDCG Score | role | proba* | 0.91304\n",
      "> NDCG Score | full | categ  | 0.93433\n",
      "> NDCG Score | full | proba* | 0.93720\n",
      "=== ISSUER ======\n",
      "Items in training set: 116 (89.92%)\n",
      "Items in eval set: 13\n",
      "Items in test set: 98\n",
      " = 227\n",
      "Number of source documents: 46 total, 21 train, 3 eval 22 test\n",
      "Absolute (training): IR 29.00, N 40.00, R 28.00, HR 19.00\n",
      "Relative (training): IR 0.25, N 0.34, R 0.24, HR 0.16\n",
      "Absolute (eval): IR 3.00, N 5.00, R 2.00, HR 3.00\n",
      "Relative (eval): IR 0.23, N 0.38, R 0.15, HR 0.23\n",
      "Role samples for ISSUER in train: 116, eval: 13, test: 98\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.81810181662 | std = 0.0400383470383\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.621677771913\n",
      "Accuracy | role : 0.357142857143\n",
      "Accuracy | full : 0.285714285714\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.71      0.48      0.57        21\n",
      "    neutral       0.20      0.07      0.11        27\n",
      "   relevant       0.21      0.37      0.27        27\n",
      "     highly       0.22      0.26      0.24        23\n",
      "\n",
      "avg / total       0.32      0.29      0.28        98\n",
      "\n",
      "[[10  2  9  0]\n",
      " [ 2  2 13 10]\n",
      " [ 2  4 10 11]\n",
      " [ 0  2 15  6]]\n",
      "> NDCG Score | role | categ  | 0.81841\n",
      "> NDCG Score | role | proba* | 0.80151\n",
      "> NDCG Score | full | categ  | 0.84495\n",
      "> NDCG Score | full | proba* | 0.86778\n",
      "=== UNDERWRITER ======\n",
      "Items in training set: 21 (100.00%)\n",
      "Items in eval set: 0\n",
      "Items in test set: 40\n",
      " = 61\n",
      "Number of source documents: 16 total, 7 train, 0 eval 9 test\n",
      "Absolute (training): IR 0.00, N 0.00, R 14.00, HR 7.00\n",
      "Relative (training): IR 0.00, N 0.00, R 0.67, HR 0.33\n",
      "Absolute (eval): IR 0.00, N 0.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR nan, N nan, R nan, HR nan\n",
      "Role samples for UNDERWRITER in train: 21, eval: 0, test: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/Uni/HPI/workspace/FEII/code/feiii_data.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  rating_agg.sum()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.916107269535 | std = 0.0308838179733\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.721180267881\n",
      "Accuracy | role : 0.375\n",
      "Accuracy | full : 0.625\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.00      0.00      0.00         2\n",
      "   relevant       0.58      0.73      0.65        15\n",
      "     highly       0.70      0.70      0.70        20\n",
      "\n",
      "avg / total       0.57      0.62      0.59        40\n",
      "\n",
      "[[ 0  0  3  0]\n",
      " [ 0  0  0  2]\n",
      " [ 0  0 11  4]\n",
      " [ 0  1  5 14]]\n",
      "> NDCG Score | role | categ  | 0.86908\n",
      "> NDCG Score | role | proba* | 0.88818\n",
      "> NDCG Score | full | categ  | 0.96397\n",
      "> NDCG Score | full | proba* | 0.96922\n",
      "=== SERVICER ======\n",
      "Items in training set: 18 (85.71%)\n",
      "Items in eval set: 3\n",
      "Items in test set: 57\n",
      " = 78\n",
      "Number of source documents: 18 total, 7 train, 1 eval 10 test\n",
      "Absolute (training): IR 0.00, N 6.00, R 3.00, HR 9.00\n",
      "Relative (training): IR 0.00, N 0.33, R 0.17, HR 0.50\n",
      "Absolute (eval): IR 0.00, N 0.00, R 3.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for SERVICER in train: 18, eval: 3, test: 57\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.877986695032 | std = 0.0293329630637\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.717524169319\n",
      "Accuracy | role : 0.122807017544\n",
      "Accuracy | full : 0.228070175439\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         3\n",
      "    neutral       0.14      0.11      0.12         9\n",
      "   relevant       0.46      0.29      0.35        38\n",
      "     highly       0.04      0.14      0.06         7\n",
      "\n",
      "avg / total       0.33      0.23      0.26        57\n",
      "\n",
      "[[ 0  0  3  0]\n",
      " [ 0  1  7  1]\n",
      " [ 0  3 11 24]\n",
      " [ 0  3  3  1]]\n",
      "> NDCG Score | role | categ  | 0.85663\n",
      "> NDCG Score | role | proba* | 0.90505\n",
      "> NDCG Score | full | categ  | 0.87517\n",
      "> NDCG Score | full | proba* | 0.89168\n",
      "=== TRUSTEE ======\n",
      "Items in training set: 373 (88.81%)\n",
      "Items in eval set: 47\n",
      "Items in test set: 304\n",
      " = 724\n",
      "Number of source documents: 40 total, 18 train, 3 eval 19 test\n",
      "Absolute (training): IR 21.00, N 124.00, R 72.00, HR 156.00\n",
      "Relative (training): IR 0.06, N 0.33, R 0.19, HR 0.42\n",
      "Absolute (eval): IR 0.00, N 40.00, R 7.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.85, R 0.15, HR 0.00\n",
      "Role samples for TRUSTEE in train: 373, eval: 47, test: 304\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.930382398812 | std = 0.0110172153556\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.814144820697\n",
      "Accuracy | role : 0.5\n",
      "Accuracy | full : 0.503289473684\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         9\n",
      "    neutral       0.20      0.40      0.27        47\n",
      "   relevant       0.43      0.12      0.19       124\n",
      "     highly       0.69      0.96      0.80       124\n",
      "\n",
      "avg / total       0.49      0.50      0.45       304\n",
      "\n",
      "[[  0   2   3   4]\n",
      " [  1  19  14  13]\n",
      " [  0  73  15  36]\n",
      " [  0   2   3 119]]\n",
      "> NDCG Score | role | categ  | 0.97358\n",
      "> NDCG Score | role | proba* | 0.98646\n",
      "> NDCG Score | full | categ  | 0.98180\n",
      "> NDCG Score | full | proba* | 0.99168\n",
      "=== COUNTERPART ======\n",
      "Items in training set: 58 (90.62%)\n",
      "Items in eval set: 6\n",
      "Items in test set: 108\n",
      " = 172\n",
      "Number of source documents: 31 total, 14 train, 3 eval 14 test\n",
      "Absolute (training): IR 6.00, N 9.00, R 28.00, HR 15.00\n",
      "Relative (training): IR 0.10, N 0.16, R 0.48, HR 0.26\n",
      "Absolute (eval): IR 0.00, N 3.00, R 2.00, HR 1.00\n",
      "Relative (eval): IR 0.00, N 0.50, R 0.33, HR 0.17\n",
      "Role samples for COUNTERPART in train: 58, eval: 6, test: 108\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.86074657219 | std = 0.0350449813983\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.677182649456\n",
      "Accuracy | role : 0.342592592593\n",
      "Accuracy | full : 0.37037037037\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00        18\n",
      "    neutral       0.17      0.06      0.09        17\n",
      "   relevant       0.33      0.47      0.39        38\n",
      "     highly       0.45      0.60      0.51        35\n",
      "\n",
      "avg / total       0.29      0.37      0.32       108\n",
      "\n",
      "[[ 0  3 11  4]\n",
      " [ 0  1 13  3]\n",
      " [ 0  1 18 19]\n",
      " [ 0  1 13 21]]\n",
      "> NDCG Score | role | categ  | 0.93940\n",
      "> NDCG Score | role | proba* | 0.87305\n",
      "> NDCG Score | full | categ  | 0.89533\n",
      "> NDCG Score | full | proba* | 0.94950\n",
      "=== SELLER ======\n",
      "Items in training set: 18 (90.00%)\n",
      "Items in eval set: 2\n",
      "Items in test set: 49\n",
      " = 69\n",
      "Number of source documents: 19 total, 9 train, 1 eval 9 test\n",
      "Absolute (training): IR 0.00, N 1.00, R 12.00, HR 5.00\n",
      "Relative (training): IR 0.00, N 0.06, R 0.67, HR 0.28\n",
      "Absolute (eval): IR 0.00, N 0.00, R 2.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.00, R 1.00, HR 0.00\n",
      "Role samples for SELLER in train: 18, eval: 2, test: 49\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.873967470984 | std = 0.0366774022704\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.674252161736\n",
      "Accuracy | role : 0.34693877551\n",
      "Accuracy | full : 0.510204081633\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.25      0.10      0.14        10\n",
      "   relevant       0.45      0.76      0.57        17\n",
      "     highly       0.69      0.61      0.65        18\n",
      "\n",
      "avg / total       0.46      0.51      0.46        49\n",
      "\n",
      "[[ 0  0  3  1]\n",
      " [ 0  1  8  1]\n",
      " [ 0  1 13  3]\n",
      " [ 0  2  5 11]]\n",
      "> NDCG Score | role | categ  | 0.80424\n",
      "> NDCG Score | role | proba* | 0.95179\n",
      "> NDCG Score | full | categ  | 0.95020\n",
      "> NDCG Score | full | proba* | 0.95388\n",
      "=== AGENT ======\n",
      "Items in training set: 53 (86.89%)\n",
      "Items in eval set: 8\n",
      "Items in test set: 40\n",
      " = 101\n",
      "Number of source documents: 25 total, 13 train, 3 eval 9 test\n",
      "Absolute (training): IR 3.00, N 28.00, R 17.00, HR 5.00\n",
      "Relative (training): IR 0.06, N 0.53, R 0.32, HR 0.09\n",
      "Absolute (eval): IR 0.00, N 7.00, R 1.00, HR 0.00\n",
      "Relative (eval): IR 0.00, N 0.88, R 0.12, HR 0.00\n",
      "Role samples for AGENT in train: 53, eval: 8, test: 40\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.856246878107 | std = 0.0424902383107\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.650212572736\n",
      "Accuracy | role : 0.175\n",
      "Accuracy | full : 0.45\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         4\n",
      "    neutral       0.33      0.25      0.29         8\n",
      "   relevant       0.44      0.53      0.48        15\n",
      "     highly       0.50      0.62      0.55        13\n",
      "\n",
      "avg / total       0.40      0.45      0.42        40\n",
      "\n",
      "[[0 3 0 1]\n",
      " [0 2 5 1]\n",
      " [0 1 8 6]\n",
      " [0 0 5 8]]\n",
      "> NDCG Score | role | categ  | 0.80376\n",
      "> NDCG Score | role | proba* | 0.76183\n",
      "> NDCG Score | full | categ  | 0.91745\n",
      "> NDCG Score | full | proba* | 0.94028\n",
      "=== AFFILIATE ======\n",
      "Items in training set: 154 (82.80%)\n",
      "Items in eval set: 32\n",
      "Items in test set: 129\n",
      " = 315\n",
      "Number of source documents: 41 total, 19 train, 3 eval 19 test\n",
      "Absolute (training): IR 26.00, N 30.00, R 42.00, HR 56.00\n",
      "Relative (training): IR 0.17, N 0.19, R 0.27, HR 0.36\n",
      "Absolute (eval): IR 0.00, N 10.00, R 20.00, HR 2.00\n",
      "Relative (eval): IR 0.00, N 0.31, R 0.62, HR 0.06\n",
      "Role samples for AFFILIATE in train: 154, eval: 32, test: 129\n",
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.887242476356 | std = 0.0245925933893\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.720304562167\n",
      "Accuracy | role : 0.457364341085\n",
      "Accuracy | full : 0.550387596899\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.25      0.06      0.10        17\n",
      "    neutral       0.29      0.17      0.21        12\n",
      "   relevant       0.52      0.57      0.54        54\n",
      "     highly       0.64      0.80      0.71        46\n",
      "\n",
      "avg / total       0.50      0.55      0.51       129\n",
      "\n",
      "[[ 1  1 14  1]\n",
      " [ 0  2  6  4]\n",
      " [ 3  4 31 16]\n",
      " [ 0  0  9 37]]\n",
      "> NDCG Score | role | categ  | 0.90354\n",
      "> NDCG Score | role | proba* | 0.98552\n",
      "> NDCG Score | full | categ  | 0.92719\n",
      "> NDCG Score | full | proba* | 0.98248\n",
      "=== INSURER ======\n",
      "Items in training set: 19 (100.00%)\n",
      "Items in eval set: 0\n",
      "Items in test set: 47\n",
      " = 66\n",
      "Number of source documents: 15 total, 8 train, 0 eval 7 test\n",
      "Absolute (training): IR 1.00, N 1.00, R 8.00, HR 9.00\n",
      "Relative (training): IR 0.05, N 0.05, R 0.42, HR 0.47\n",
      "Absolute (eval): IR 0.00, N 0.00, R 0.00, HR 0.00\n",
      "Relative (eval): IR nan, N nan, R nan, HR nan\n",
      "Role samples for INSURER in train: 19, eval: 0, test: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tim/Uni/HPI/workspace/FEII/code/feiii_data.py:222: RuntimeWarning: invalid value encountered in true_divide\n",
      "  rating_agg.sum()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG after 100x random order:\n",
      " > mean ndcg = 0.902033514386 | std = 0.0306103766175\n",
      "NDCG for worst case (inverted best) order:\n",
      " > ndcg = 0.755505248883\n",
      "Accuracy | role : 0.617021276596\n",
      "Accuracy | full : 0.446808510638\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " irrelevant       0.00      0.00      0.00         1\n",
      "    neutral       0.25      0.14      0.18         7\n",
      "   relevant       0.27      0.16      0.20        19\n",
      "     highly       0.53      0.85      0.65        20\n",
      "\n",
      "avg / total       0.37      0.45      0.39        47\n",
      "\n",
      "[[ 0  0  1  0]\n",
      " [ 0  1  4  2]\n",
      " [ 0  3  3 13]\n",
      " [ 0  0  3 17]]\n",
      "> NDCG Score | role | categ  | 0.92645\n",
      "> NDCG Score | role | proba* | 0.94048\n",
      "> NDCG Score | full | categ  | 0.90220\n",
      "> NDCG Score | full | proba* | 0.98437\n",
      "TOTAL NDCG | role | categ  | 0.96265\n",
      "TOTAL NDCG | role | proba* | 0.96103\n",
      "TOTAL NDCG | full | categ  | 0.96790\n",
      "TOTAL NDCG | full | proba* | 0.97142\n",
      "baseline_rand 10\n",
      "baseline_worst 10\n",
      "ndcg_role 10\n",
      "ndcg_full 10\n",
      "ndcg_role_proba 10\n",
      "ndcg_full_proba 10\n",
      "acc_role 10\n",
      "acc_full 10\n",
      "f1_role 10\n",
      "f1_full 10\n"
     ]
    }
   ],
   "source": [
    "res, macro_res, conf_matrix_role, conf_matrix_full = kfold(5, data, pipeline, score_func, predict_on='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.458641</td>\n",
       "      <td>0.465336</td>\n",
       "      <td>0.841509</td>\n",
       "      <td>0.664118</td>\n",
       "      <td>0.445232</td>\n",
       "      <td>0.447596</td>\n",
       "      <td>0.911441</td>\n",
       "      <td>0.924117</td>\n",
       "      <td>0.901806</td>\n",
       "      <td>0.899458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.226241</td>\n",
       "      <td>0.204225</td>\n",
       "      <td>0.050487</td>\n",
       "      <td>0.098105</td>\n",
       "      <td>0.213188</td>\n",
       "      <td>0.205387</td>\n",
       "      <td>0.069761</td>\n",
       "      <td>0.055213</td>\n",
       "      <td>0.061367</td>\n",
       "      <td>0.076060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.776845</td>\n",
       "      <td>0.547873</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.105195</td>\n",
       "      <td>0.787613</td>\n",
       "      <td>0.831658</td>\n",
       "      <td>0.814113</td>\n",
       "      <td>0.776319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.366787</td>\n",
       "      <td>0.822211</td>\n",
       "      <td>0.621314</td>\n",
       "      <td>0.293173</td>\n",
       "      <td>0.312556</td>\n",
       "      <td>0.858281</td>\n",
       "      <td>0.885142</td>\n",
       "      <td>0.849941</td>\n",
       "      <td>0.832468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.420242</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.829097</td>\n",
       "      <td>0.639927</td>\n",
       "      <td>0.410037</td>\n",
       "      <td>0.423684</td>\n",
       "      <td>0.939201</td>\n",
       "      <td>0.921848</td>\n",
       "      <td>0.918070</td>\n",
       "      <td>0.914682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.594697</td>\n",
       "      <td>0.605392</td>\n",
       "      <td>0.866064</td>\n",
       "      <td>0.680593</td>\n",
       "      <td>0.619769</td>\n",
       "      <td>0.602674</td>\n",
       "      <td>0.961578</td>\n",
       "      <td>0.958443</td>\n",
       "      <td>0.957638</td>\n",
       "      <td>0.956902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.944515</td>\n",
       "      <td>0.882791</td>\n",
       "      <td>0.780952</td>\n",
       "      <td>0.780952</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966482</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acc_full   acc_role  baseline_rand  baseline_worst    f1_full  \\\n",
       "count  10.000000  10.000000      10.000000       10.000000  10.000000   \n",
       "mean    0.458641   0.465336       0.841509        0.664118   0.445232   \n",
       "std     0.226241   0.204225       0.050487        0.098105   0.213188   \n",
       "min     0.117647   0.136364       0.776845        0.547873   0.156863   \n",
       "25%     0.305556   0.366787       0.822211        0.621314   0.293173   \n",
       "50%     0.420242   0.444444       0.829097        0.639927   0.410037   \n",
       "75%     0.594697   0.605392       0.866064        0.680593   0.619769   \n",
       "max     0.800000   0.800000       0.944515        0.882791   0.780952   \n",
       "\n",
       "         f1_role  ndcg_full  ndcg_full_proba  ndcg_role  ndcg_role_proba  \n",
       "count  10.000000  10.000000        10.000000  10.000000        10.000000  \n",
       "mean    0.447596   0.911441         0.924117   0.901806         0.899458  \n",
       "std     0.205387   0.069761         0.055213   0.061367         0.076060  \n",
       "min     0.105195   0.787613         0.831658   0.814113         0.776319  \n",
       "25%     0.312556   0.858281         0.885142   0.849941         0.832468  \n",
       "50%     0.423684   0.939201         0.921848   0.918070         0.914682  \n",
       "75%     0.602674   0.961578         0.958443   0.957638         0.956902  \n",
       "max     0.780952   1.000000         1.000000   0.966482         1.000000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(res).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2>GUARANTOR</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.583069</td>\n",
       "      <td>0.888915</td>\n",
       "      <td>0.757137</td>\n",
       "      <td>0.449566</td>\n",
       "      <td>0.513951</td>\n",
       "      <td>0.932977</td>\n",
       "      <td>0.933457</td>\n",
       "      <td>0.873171</td>\n",
       "      <td>0.866518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.074788</td>\n",
       "      <td>0.252112</td>\n",
       "      <td>0.023027</td>\n",
       "      <td>0.072988</td>\n",
       "      <td>0.127639</td>\n",
       "      <td>0.247514</td>\n",
       "      <td>0.016366</td>\n",
       "      <td>0.020814</td>\n",
       "      <td>0.062850</td>\n",
       "      <td>0.061798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.860437</td>\n",
       "      <td>0.677183</td>\n",
       "      <td>0.270559</td>\n",
       "      <td>0.178709</td>\n",
       "      <td>0.915284</td>\n",
       "      <td>0.903181</td>\n",
       "      <td>0.804398</td>\n",
       "      <td>0.764600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.398148</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.867251</td>\n",
       "      <td>0.677183</td>\n",
       "      <td>0.364652</td>\n",
       "      <td>0.317714</td>\n",
       "      <td>0.919376</td>\n",
       "      <td>0.921867</td>\n",
       "      <td>0.840668</td>\n",
       "      <td>0.851214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.904693</td>\n",
       "      <td>0.810440</td>\n",
       "      <td>0.498430</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.930722</td>\n",
       "      <td>0.939955</td>\n",
       "      <td>0.840668</td>\n",
       "      <td>0.902075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.905325</td>\n",
       "      <td>0.810440</td>\n",
       "      <td>0.542929</td>\n",
       "      <td>0.691429</td>\n",
       "      <td>0.948101</td>\n",
       "      <td>0.948476</td>\n",
       "      <td>0.937921</td>\n",
       "      <td>0.902075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.535714</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.906869</td>\n",
       "      <td>0.810440</td>\n",
       "      <td>0.571259</td>\n",
       "      <td>0.691429</td>\n",
       "      <td>0.951402</td>\n",
       "      <td>0.953807</td>\n",
       "      <td>0.942201</td>\n",
       "      <td>0.912627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><h2>ISSUER</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.393878</td>\n",
       "      <td>0.338776</td>\n",
       "      <td>0.834192</td>\n",
       "      <td>0.642708</td>\n",
       "      <td>0.368801</td>\n",
       "      <td>0.256569</td>\n",
       "      <td>0.897518</td>\n",
       "      <td>0.892053</td>\n",
       "      <td>0.829137</td>\n",
       "      <td>0.874016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.107167</td>\n",
       "      <td>0.018254</td>\n",
       "      <td>0.030302</td>\n",
       "      <td>0.028796</td>\n",
       "      <td>0.080802</td>\n",
       "      <td>0.074041</td>\n",
       "      <td>0.055830</td>\n",
       "      <td>0.045148</td>\n",
       "      <td>0.024649</td>\n",
       "      <td>0.027340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.295918</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>0.807877</td>\n",
       "      <td>0.621678</td>\n",
       "      <td>0.295756</td>\n",
       "      <td>0.178726</td>\n",
       "      <td>0.851504</td>\n",
       "      <td>0.851856</td>\n",
       "      <td>0.804237</td>\n",
       "      <td>0.833358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.316327</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.811261</td>\n",
       "      <td>0.621678</td>\n",
       "      <td>0.314412</td>\n",
       "      <td>0.178726</td>\n",
       "      <td>0.853683</td>\n",
       "      <td>0.857326</td>\n",
       "      <td>0.804237</td>\n",
       "      <td>0.871015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.336735</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.817504</td>\n",
       "      <td>0.621678</td>\n",
       "      <td>0.320325</td>\n",
       "      <td>0.276018</td>\n",
       "      <td>0.865819</td>\n",
       "      <td>0.875407</td>\n",
       "      <td>0.830913</td>\n",
       "      <td>0.873814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.866089</td>\n",
       "      <td>0.674252</td>\n",
       "      <td>0.456201</td>\n",
       "      <td>0.316005</td>\n",
       "      <td>0.953944</td>\n",
       "      <td>0.917076</td>\n",
       "      <td>0.849032</td>\n",
       "      <td>0.882422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>0.674252</td>\n",
       "      <td>0.457309</td>\n",
       "      <td>0.333371</td>\n",
       "      <td>0.962637</td>\n",
       "      <td>0.958599</td>\n",
       "      <td>0.857266</td>\n",
       "      <td>0.909472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><h2>UNDERWRITER</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.505000</td>\n",
       "      <td>0.335000</td>\n",
       "      <td>0.886326</td>\n",
       "      <td>0.692793</td>\n",
       "      <td>0.478200</td>\n",
       "      <td>0.235669</td>\n",
       "      <td>0.940803</td>\n",
       "      <td>0.939043</td>\n",
       "      <td>0.863808</td>\n",
       "      <td>0.848404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.124248</td>\n",
       "      <td>0.137614</td>\n",
       "      <td>0.029954</td>\n",
       "      <td>0.038871</td>\n",
       "      <td>0.141670</td>\n",
       "      <td>0.131871</td>\n",
       "      <td>0.043365</td>\n",
       "      <td>0.051621</td>\n",
       "      <td>0.045118</td>\n",
       "      <td>0.070396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.852340</td>\n",
       "      <td>0.650213</td>\n",
       "      <td>0.278373</td>\n",
       "      <td>0.147826</td>\n",
       "      <td>0.886260</td>\n",
       "      <td>0.857001</td>\n",
       "      <td>0.811185</td>\n",
       "      <td>0.762135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.854805</td>\n",
       "      <td>0.650213</td>\n",
       "      <td>0.398088</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.911807</td>\n",
       "      <td>0.933275</td>\n",
       "      <td>0.837770</td>\n",
       "      <td>0.811598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.906009</td>\n",
       "      <td>0.721180</td>\n",
       "      <td>0.526464</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.940046</td>\n",
       "      <td>0.938600</td>\n",
       "      <td>0.869081</td>\n",
       "      <td>0.845257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.908007</td>\n",
       "      <td>0.721180</td>\n",
       "      <td>0.545219</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.974210</td>\n",
       "      <td>0.982893</td>\n",
       "      <td>0.869081</td>\n",
       "      <td>0.872469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.910469</td>\n",
       "      <td>0.721180</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.991694</td>\n",
       "      <td>0.983446</td>\n",
       "      <td>0.931923</td>\n",
       "      <td>0.950562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><h2>SERVICER</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.444798</td>\n",
       "      <td>0.336026</td>\n",
       "      <td>0.885952</td>\n",
       "      <td>0.718636</td>\n",
       "      <td>0.435237</td>\n",
       "      <td>0.286383</td>\n",
       "      <td>0.922130</td>\n",
       "      <td>0.901465</td>\n",
       "      <td>0.905524</td>\n",
       "      <td>0.923599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.123198</td>\n",
       "      <td>0.185316</td>\n",
       "      <td>0.006011</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.109749</td>\n",
       "      <td>0.223507</td>\n",
       "      <td>0.026386</td>\n",
       "      <td>0.053396</td>\n",
       "      <td>0.055626</td>\n",
       "      <td>0.057351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.228070</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.878927</td>\n",
       "      <td>0.717524</td>\n",
       "      <td>0.240732</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.895692</td>\n",
       "      <td>0.848930</td>\n",
       "      <td>0.856627</td>\n",
       "      <td>0.831025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.882676</td>\n",
       "      <td>0.717524</td>\n",
       "      <td>0.471120</td>\n",
       "      <td>0.077913</td>\n",
       "      <td>0.898584</td>\n",
       "      <td>0.858713</td>\n",
       "      <td>0.856627</td>\n",
       "      <td>0.918088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.350877</td>\n",
       "      <td>0.883668</td>\n",
       "      <td>0.717524</td>\n",
       "      <td>0.475761</td>\n",
       "      <td>0.347827</td>\n",
       "      <td>0.923540</td>\n",
       "      <td>0.883264</td>\n",
       "      <td>0.886104</td>\n",
       "      <td>0.926324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.496124</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.892178</td>\n",
       "      <td>0.720305</td>\n",
       "      <td>0.479382</td>\n",
       "      <td>0.453620</td>\n",
       "      <td>0.933194</td>\n",
       "      <td>0.953429</td>\n",
       "      <td>0.950930</td>\n",
       "      <td>0.970300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.542636</td>\n",
       "      <td>0.892312</td>\n",
       "      <td>0.720305</td>\n",
       "      <td>0.509189</td>\n",
       "      <td>0.525691</td>\n",
       "      <td>0.959643</td>\n",
       "      <td>0.962991</td>\n",
       "      <td>0.977332</td>\n",
       "      <td>0.972260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><h2>TRUSTEE</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.531761</td>\n",
       "      <td>0.435694</td>\n",
       "      <td>0.922001</td>\n",
       "      <td>0.790689</td>\n",
       "      <td>0.515563</td>\n",
       "      <td>0.345039</td>\n",
       "      <td>0.968958</td>\n",
       "      <td>0.972104</td>\n",
       "      <td>0.939481</td>\n",
       "      <td>0.944232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.085394</td>\n",
       "      <td>0.039366</td>\n",
       "      <td>0.012559</td>\n",
       "      <td>0.032118</td>\n",
       "      <td>0.075589</td>\n",
       "      <td>0.095892</td>\n",
       "      <td>0.021797</td>\n",
       "      <td>0.026979</td>\n",
       "      <td>0.040592</td>\n",
       "      <td>0.046479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.906264</td>\n",
       "      <td>0.755505</td>\n",
       "      <td>0.442245</td>\n",
       "      <td>0.232753</td>\n",
       "      <td>0.933890</td>\n",
       "      <td>0.925168</td>\n",
       "      <td>0.896197</td>\n",
       "      <td>0.883234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.486842</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.910414</td>\n",
       "      <td>0.755505</td>\n",
       "      <td>0.472523</td>\n",
       "      <td>0.254049</td>\n",
       "      <td>0.964152</td>\n",
       "      <td>0.975627</td>\n",
       "      <td>0.896197</td>\n",
       "      <td>0.911696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.506579</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.930753</td>\n",
       "      <td>0.814145</td>\n",
       "      <td>0.510991</td>\n",
       "      <td>0.392077</td>\n",
       "      <td>0.973431</td>\n",
       "      <td>0.979899</td>\n",
       "      <td>0.953381</td>\n",
       "      <td>0.950314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.516447</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.931202</td>\n",
       "      <td>0.814145</td>\n",
       "      <td>0.511631</td>\n",
       "      <td>0.395992</td>\n",
       "      <td>0.985966</td>\n",
       "      <td>0.989272</td>\n",
       "      <td>0.973513</td>\n",
       "      <td>0.987349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.480263</td>\n",
       "      <td>0.931370</td>\n",
       "      <td>0.814145</td>\n",
       "      <td>0.640426</td>\n",
       "      <td>0.450324</td>\n",
       "      <td>0.987353</td>\n",
       "      <td>0.990557</td>\n",
       "      <td>0.978115</td>\n",
       "      <td>0.988567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><h2>COUNTERPART</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.429894</td>\n",
       "      <td>0.494444</td>\n",
       "      <td>0.880211</td>\n",
       "      <td>0.730486</td>\n",
       "      <td>0.415232</td>\n",
       "      <td>0.414508</td>\n",
       "      <td>0.926225</td>\n",
       "      <td>0.931984</td>\n",
       "      <td>0.886088</td>\n",
       "      <td>0.843314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.057498</td>\n",
       "      <td>0.240694</td>\n",
       "      <td>0.023785</td>\n",
       "      <td>0.072988</td>\n",
       "      <td>0.108353</td>\n",
       "      <td>0.258042</td>\n",
       "      <td>0.013593</td>\n",
       "      <td>0.019213</td>\n",
       "      <td>0.061112</td>\n",
       "      <td>0.066697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.351852</td>\n",
       "      <td>0.240741</td>\n",
       "      <td>0.860437</td>\n",
       "      <td>0.677183</td>\n",
       "      <td>0.270559</td>\n",
       "      <td>0.178709</td>\n",
       "      <td>0.915284</td>\n",
       "      <td>0.903181</td>\n",
       "      <td>0.804398</td>\n",
       "      <td>0.764600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.398148</td>\n",
       "      <td>0.342593</td>\n",
       "      <td>0.861172</td>\n",
       "      <td>0.677183</td>\n",
       "      <td>0.364652</td>\n",
       "      <td>0.194213</td>\n",
       "      <td>0.917641</td>\n",
       "      <td>0.921867</td>\n",
       "      <td>0.840668</td>\n",
       "      <td>0.786054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.435185</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.867251</td>\n",
       "      <td>0.677183</td>\n",
       "      <td>0.399587</td>\n",
       "      <td>0.317714</td>\n",
       "      <td>0.919376</td>\n",
       "      <td>0.939955</td>\n",
       "      <td>0.905253</td>\n",
       "      <td>0.851214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.905325</td>\n",
       "      <td>0.810440</td>\n",
       "      <td>0.498430</td>\n",
       "      <td>0.690476</td>\n",
       "      <td>0.930722</td>\n",
       "      <td>0.946443</td>\n",
       "      <td>0.937921</td>\n",
       "      <td>0.902075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.906869</td>\n",
       "      <td>0.810440</td>\n",
       "      <td>0.542929</td>\n",
       "      <td>0.691429</td>\n",
       "      <td>0.948101</td>\n",
       "      <td>0.948476</td>\n",
       "      <td>0.942201</td>\n",
       "      <td>0.912627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><h2>SELLER</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.432653</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.844468</td>\n",
       "      <td>0.653222</td>\n",
       "      <td>0.396962</td>\n",
       "      <td>0.237111</td>\n",
       "      <td>0.918397</td>\n",
       "      <td>0.909187</td>\n",
       "      <td>0.820178</td>\n",
       "      <td>0.895146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.107167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031897</td>\n",
       "      <td>0.028796</td>\n",
       "      <td>0.081641</td>\n",
       "      <td>0.080182</td>\n",
       "      <td>0.054851</td>\n",
       "      <td>0.052851</td>\n",
       "      <td>0.023734</td>\n",
       "      <td>0.028845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.295918</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.807877</td>\n",
       "      <td>0.621678</td>\n",
       "      <td>0.295756</td>\n",
       "      <td>0.178726</td>\n",
       "      <td>0.851504</td>\n",
       "      <td>0.851856</td>\n",
       "      <td>0.804237</td>\n",
       "      <td>0.871015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.336735</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.811261</td>\n",
       "      <td>0.621678</td>\n",
       "      <td>0.320325</td>\n",
       "      <td>0.178726</td>\n",
       "      <td>0.865819</td>\n",
       "      <td>0.857326</td>\n",
       "      <td>0.804237</td>\n",
       "      <td>0.873814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.866089</td>\n",
       "      <td>0.674252</td>\n",
       "      <td>0.455220</td>\n",
       "      <td>0.178726</td>\n",
       "      <td>0.953944</td>\n",
       "      <td>0.917076</td>\n",
       "      <td>0.804237</td>\n",
       "      <td>0.882422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>0.674252</td>\n",
       "      <td>0.456201</td>\n",
       "      <td>0.316005</td>\n",
       "      <td>0.958081</td>\n",
       "      <td>0.958599</td>\n",
       "      <td>0.830913</td>\n",
       "      <td>0.909472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.868881</td>\n",
       "      <td>0.674252</td>\n",
       "      <td>0.457309</td>\n",
       "      <td>0.333371</td>\n",
       "      <td>0.962637</td>\n",
       "      <td>0.961080</td>\n",
       "      <td>0.857266</td>\n",
       "      <td>0.939006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><h2>AGENT</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.485000</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.875193</td>\n",
       "      <td>0.678600</td>\n",
       "      <td>0.448395</td>\n",
       "      <td>0.244531</td>\n",
       "      <td>0.918658</td>\n",
       "      <td>0.916473</td>\n",
       "      <td>0.856334</td>\n",
       "      <td>0.836004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.128209</td>\n",
       "      <td>0.135785</td>\n",
       "      <td>0.030225</td>\n",
       "      <td>0.038871</td>\n",
       "      <td>0.139708</td>\n",
       "      <td>0.130740</td>\n",
       "      <td>0.049831</td>\n",
       "      <td>0.052321</td>\n",
       "      <td>0.047078</td>\n",
       "      <td>0.076302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.852340</td>\n",
       "      <td>0.650213</td>\n",
       "      <td>0.278373</td>\n",
       "      <td>0.147826</td>\n",
       "      <td>0.863483</td>\n",
       "      <td>0.857001</td>\n",
       "      <td>0.811185</td>\n",
       "      <td>0.762135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.852341</td>\n",
       "      <td>0.650213</td>\n",
       "      <td>0.396190</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.886260</td>\n",
       "      <td>0.870042</td>\n",
       "      <td>0.831712</td>\n",
       "      <td>0.783254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.854805</td>\n",
       "      <td>0.650213</td>\n",
       "      <td>0.398088</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.911807</td>\n",
       "      <td>0.933275</td>\n",
       "      <td>0.837770</td>\n",
       "      <td>0.811598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.906009</td>\n",
       "      <td>0.721180</td>\n",
       "      <td>0.526464</td>\n",
       "      <td>0.248857</td>\n",
       "      <td>0.940046</td>\n",
       "      <td>0.938600</td>\n",
       "      <td>0.869081</td>\n",
       "      <td>0.872469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.675000</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.910469</td>\n",
       "      <td>0.721180</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.991694</td>\n",
       "      <td>0.983446</td>\n",
       "      <td>0.931923</td>\n",
       "      <td>0.950562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><h2>AFFILIATE</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.448878</td>\n",
       "      <td>0.361975</td>\n",
       "      <td>0.888036</td>\n",
       "      <td>0.719192</td>\n",
       "      <td>0.434619</td>\n",
       "      <td>0.305327</td>\n",
       "      <td>0.928214</td>\n",
       "      <td>0.920598</td>\n",
       "      <td>0.916851</td>\n",
       "      <td>0.933623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.125437</td>\n",
       "      <td>0.196652</td>\n",
       "      <td>0.006385</td>\n",
       "      <td>0.001523</td>\n",
       "      <td>0.109472</td>\n",
       "      <td>0.233794</td>\n",
       "      <td>0.021890</td>\n",
       "      <td>0.061732</td>\n",
       "      <td>0.056443</td>\n",
       "      <td>0.062128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.228070</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.878927</td>\n",
       "      <td>0.717524</td>\n",
       "      <td>0.240732</td>\n",
       "      <td>0.026864</td>\n",
       "      <td>0.898584</td>\n",
       "      <td>0.848930</td>\n",
       "      <td>0.856627</td>\n",
       "      <td>0.831025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.175439</td>\n",
       "      <td>0.883668</td>\n",
       "      <td>0.717524</td>\n",
       "      <td>0.471120</td>\n",
       "      <td>0.077913</td>\n",
       "      <td>0.923540</td>\n",
       "      <td>0.858713</td>\n",
       "      <td>0.856627</td>\n",
       "      <td>0.918088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.496124</td>\n",
       "      <td>0.480620</td>\n",
       "      <td>0.892178</td>\n",
       "      <td>0.720305</td>\n",
       "      <td>0.472670</td>\n",
       "      <td>0.442545</td>\n",
       "      <td>0.926111</td>\n",
       "      <td>0.953429</td>\n",
       "      <td>0.942739</td>\n",
       "      <td>0.970300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.511628</td>\n",
       "      <td>0.488372</td>\n",
       "      <td>0.892312</td>\n",
       "      <td>0.720305</td>\n",
       "      <td>0.479382</td>\n",
       "      <td>0.453620</td>\n",
       "      <td>0.933194</td>\n",
       "      <td>0.962991</td>\n",
       "      <td>0.950930</td>\n",
       "      <td>0.972260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.534884</td>\n",
       "      <td>0.542636</td>\n",
       "      <td>0.893093</td>\n",
       "      <td>0.720305</td>\n",
       "      <td>0.509189</td>\n",
       "      <td>0.525691</td>\n",
       "      <td>0.959643</td>\n",
       "      <td>0.978927</td>\n",
       "      <td>0.977332</td>\n",
       "      <td>0.976444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><h2>INSURER</h2><table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc_full</th>\n",
       "      <th>acc_role</th>\n",
       "      <th>baseline_rand</th>\n",
       "      <th>baseline_worst</th>\n",
       "      <th>f1_full</th>\n",
       "      <th>f1_role</th>\n",
       "      <th>ndcg_full</th>\n",
       "      <th>ndcg_full_proba</th>\n",
       "      <th>ndcg_role</th>\n",
       "      <th>ndcg_role_proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.534854</td>\n",
       "      <td>0.446109</td>\n",
       "      <td>0.916533</td>\n",
       "      <td>0.778961</td>\n",
       "      <td>0.509757</td>\n",
       "      <td>0.332948</td>\n",
       "      <td>0.962354</td>\n",
       "      <td>0.972730</td>\n",
       "      <td>0.933608</td>\n",
       "      <td>0.940510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.084980</td>\n",
       "      <td>0.032026</td>\n",
       "      <td>0.013470</td>\n",
       "      <td>0.032118</td>\n",
       "      <td>0.077067</td>\n",
       "      <td>0.092219</td>\n",
       "      <td>0.020308</td>\n",
       "      <td>0.027240</td>\n",
       "      <td>0.040200</td>\n",
       "      <td>0.046615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.468085</td>\n",
       "      <td>0.404255</td>\n",
       "      <td>0.903864</td>\n",
       "      <td>0.755505</td>\n",
       "      <td>0.442245</td>\n",
       "      <td>0.232753</td>\n",
       "      <td>0.933890</td>\n",
       "      <td>0.925168</td>\n",
       "      <td>0.896197</td>\n",
       "      <td>0.883234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.486842</td>\n",
       "      <td>0.425532</td>\n",
       "      <td>0.906264</td>\n",
       "      <td>0.755505</td>\n",
       "      <td>0.472523</td>\n",
       "      <td>0.254049</td>\n",
       "      <td>0.952946</td>\n",
       "      <td>0.975627</td>\n",
       "      <td>0.896197</td>\n",
       "      <td>0.911696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.506579</td>\n",
       "      <td>0.446809</td>\n",
       "      <td>0.910414</td>\n",
       "      <td>0.755505</td>\n",
       "      <td>0.482602</td>\n",
       "      <td>0.331623</td>\n",
       "      <td>0.964152</td>\n",
       "      <td>0.983025</td>\n",
       "      <td>0.924019</td>\n",
       "      <td>0.931703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.531915</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.930753</td>\n",
       "      <td>0.814145</td>\n",
       "      <td>0.510991</td>\n",
       "      <td>0.395992</td>\n",
       "      <td>0.973431</td>\n",
       "      <td>0.989272</td>\n",
       "      <td>0.973513</td>\n",
       "      <td>0.987349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.480263</td>\n",
       "      <td>0.931370</td>\n",
       "      <td>0.814145</td>\n",
       "      <td>0.640426</td>\n",
       "      <td>0.450324</td>\n",
       "      <td>0.987353</td>\n",
       "      <td>0.990557</td>\n",
       "      <td>0.978115</td>\n",
       "      <td>0.988567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = \"\"\n",
    "for ir, r in enumerate(data.get_roles()):\n",
    "    out+=\"<h2>\"+r.upper()+\"</h2>\"\n",
    "    out+=pd.DataFrame(res).iloc[[ir+(ii*5) for ii in range(5)]].describe().to_html()\n",
    "HTML(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGENT       : 0.6074 (ndcg), 0.5558 (rand) | min: 0.6315, rand: 0.8363, ndcg: 0.8553 | f1_full: 0.16 f1_role: 0.18\n",
      "TRUSTEE     : 0.8385 (ndcg), 0.5210 (rand) | min: 0.6349, rand: 0.8251, ndcg: 0.9411 | f1_full: 0.19 f1_role: 0.45\n",
      "SERVICER    : 0.6737 (ndcg), 0.4640 (rand) | min: 0.6821, rand: 0.8296, ndcg: 0.8963 | f1_full: 0.00 f1_role: 0.17\n",
      "COUNTERPART : 0.3135 (ndcg), 0.5043 (rand) | min: 0.5479, rand: 0.7759, ndcg: 0.6896 | f1_full: 0.12 f1_role: 0.08\n",
      "ISSUER      : 0.7296 (ndcg), 0.5024 (rand) | min: 0.5606, rand: 0.7814, ndcg: 0.8812 | f1_full: 0.22 f1_role: 0.25\n",
      "AFFILIATE   : 0.7823 (ndcg), 0.6041 (rand) | min: 0.6759, rand: 0.8717, ndcg: 0.9294 | f1_full: 0.40 f1_role: 0.22\n",
      "INSURER     : 0.5179 (ndcg), 0.5372 (rand) | min: 0.6449, rand: 0.8357, ndcg: 0.8288 | f1_full: 0.14 f1_role: 0.19\n",
      "UNDERWRITER : 0.6882 (ndcg), 0.5712 (rand) | min: 0.6179, rand: 0.8362, ndcg: 0.8809 | f1_full: 0.11 f1_role: 0.40\n",
      "SELLER      : 0.0000 (ndcg), 0.4707 (rand) | min: 0.8828, rand: 0.9380, ndcg: 0.8828 | f1_full: 0.20 f1_role: 0.45\n",
      "GUARANTOR   : 0.6198 (ndcg), 0.5733 (rand) | min: 0.7626, rand: 0.8987, ndcg: 0.9097 | f1_full: 0.65 f1_role: 0.68\n",
      "ALL         : 0.6115 (ndcg), 0.5321 (rand) | min: 0.6641, rand: 0.8428, ndcg: 0.8695 | f1_full: 0.22 f1_role: 0.31\n"
     ]
    }
   ],
   "source": [
    "folds = 1\n",
    "for ir, r in enumerate(data.get_roles()):\n",
    "    tmp = pd.DataFrame(res).iloc[[ir+(ii*folds) for ii in range(folds)]].describe()\n",
    "    m = tmp.loc['mean']['baseline_worst']\n",
    "    print(\"{: <12}: {:.4f} (ndcg), {:.4f} (rand) | min: {:.4f}, rand: {:.4f}, ndcg: {:.4f} | f1_full: {:.2f} f1_role: {:.2f}\".format(\n",
    "        r.upper(),\n",
    "        (tmp.loc['mean']['ndcg_full_proba']-m)/(1-m),\n",
    "        (tmp.loc['mean']['baseline_rand']-m)/(1-m),\n",
    "        m,tmp.loc['mean']['baseline_rand'], tmp.loc['mean']['ndcg_full_proba'],\n",
    "        tmp.loc['mean']['f1_full'], tmp.loc['mean']['f1_role']) )\n",
    "    \n",
    "tmp = pd.DataFrame(res).describe()\n",
    "m = tmp.loc['mean']['baseline_worst']\n",
    "print(\"{: <12}: {:.4f} (ndcg), {:.4f} (rand) | min: {:.4f}, rand: {:.4f}, ndcg: {:.4f} | f1_full: {:.2f} f1_role: {:.2f}\".format(\n",
    "    'ALL',\n",
    "    (tmp.loc['mean']['ndcg_full_proba']-m)/(1-m),\n",
    "    (tmp.loc['mean']['baseline_rand']-m)/(1-m),\n",
    "    m,tmp.loc['mean']['baseline_rand'], tmp.loc['mean']['ndcg_full_proba'],\n",
    "        tmp.loc['mean']['f1_full'], tmp.loc['mean']['f1_role']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1c15eb87b8>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAE0CAYAAADJxUOPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA1VYabmLk7EdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c1bc8e048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "pd.DataFrame(res)[['baseline_worst','baseline_rand','ndcg_full','ndcg_role','ndcg_full_proba','ndcg_role_proba']]\\\n",
    "    .boxplot(figsize=(5,8), rot=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAEECAYAAAAlJU69AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xd4FOXax/HvjQtYIXSSUAIc6VISqnRFpApSVAQEC756\njuXYjl1RUVEEuxzRo9iRJlU6IoJIFUTBCqGkSFFALEGW5/1jNyGNECDZTYbf57r2Ymfnmdn7mWfv\nmXtnZoM55xARERHxkiLhDkBEREQkr6nAEREREc9RgSMiIiKeowJHREREPEcFjoiIiHiOChwRERHx\nHBU4clws4E0z+9XMVoY7HgAzG2Bm88Idh0g4KCdFsqcCR45Xa+AioJJzrll+vpGZxZiZMzNfTu2c\nc+855zqd4Hs4M/vdzA4EH3uDrw8xs6XZtI83s47B5+PMbHgO6/3HicQkcpy8nJMJZjbazE7L1Ka7\nma0MtttjZu+ZWaV087PN3+C8xWb2V7qcP2BmM04kVinYVODI8aoKxDvnfg93IADH2tHmUkPn3NnB\nR0QerE8klDybk0A74HLgmnTr7wu8DzwHlAXqASnAUjMrlcv135Qu5892zvXIg5ilgFGB42FmVtnM\nppjZruC3nJeCrxcxswfMbKuZ7TSzt82sZHBe6je0wWa2zcx2m9n9wXnXAq8DLYPfeh7J5j2HmNky\nM3vWzPaa2WYzOz/4+vbg+w1O176bmX1pZvuD84elW92S4L97g+/XMtP69wDD0n9bC77XbjOrHJxu\nGDx1Xzvvt7DI8VFOHl9OOud+BJYBjYLLGjAKGO6ce98596dzLhm4DjgA3HYcwyEepwLHo4KndGcC\nW4EYIBoYH5w9JPjoAFQHzgZeyrSK1kAt4ELgITOr45z7H3ADsDz4refho7x9c+AroAyBb1rjgabA\nP4CBwEtmdnaw7e/AVUAE0A240cx6Bee1Df4bEXy/5enWvxmoADye/o2dc58DrwJvmdkZwLvAg865\nb4+2rURCQTl5/DkZLILaAD8GX6oFVAEmZnqPw8BkApfqRAAVOF7WDIgC7nLO/e6c+8s5l3pNegAw\n2jm32Tl3ALgXuCLTqeVHgt+O1gPrgYbH8d5bnHNvOuf8wIdAZeBR51yKc24ecJDAjhXn3GLn3Abn\n3GHn3FfABwROS+ck0Tn3onPukHPuz2zmDwNKAiuBBODlY6xvbfCb7V4zeyHXvRQ5PsrJ48vJ34FN\nwGLgleDrZYP/JmWzTFK6+cfyQrqc32tmj+VyOSlEVOB4V2Vgq3PuUDbzogh8i0y1FfAR+PaVKjnd\n8z8IfKPMrZ/TPf8TwDmX+bWzAcysuZl9Ejxlv4/At9Fj7aS25zTTOfc3MA6oD4xyx/4fZWOdcxHB\nxy3B1w4BRbNpWxT4+xjrE8mOcvI4cjIYz+UEzg6dFXx9d/DfyGyWiUw3/1huSZfzEc65B3O5nBQi\nKnC8aztQxbK/4S+RwI2JqaoQOKD/nE3b/PY+MB2o7JwrCfwXsOC8o+0Ec9w5mlk08DDwJjDKzIqf\nQFzbCGy/1FgwszOB8mQ8EInklnLyOHLSBUwAlgMPBV/+DtgB9Mu0/iJAH2DhsdYrpw4VON61ksAp\n2xFmdpaZnW5mrYLzPgBuM7NqwevuTwAfHuWbZX47B/jFOfeXmTUDrkw3bxdwmMA9CbkSLEjGAf8D\nriWwDU7k9PMK4C/gnuC2OwsYAawmY4FzWnB+6qNYunnFMs3L8FNXOeUoJ08sJ0cAQ82sYvDMz53A\nA2Z2ZXAbViRwo3UJ4NlMb50+/04/jvcUD1CB41HBa+09CFxX30bgW8/lwdlvAO8Q+EXEFgIH8pvD\nECbAP4FHzew3At/SJqTOcM79QeCGxWXB6+QtcrG+WwicZXkwuDO8GrjazNocT1DOuRQCN1i2J7Dt\nNhO4jHBZptPr9xA4vZ/6WJRu3jeZ5l19PDGItygnTywnnXMbCGyXu4LTHwKDCPxiag+wETgDaOWc\n25Nu0fPJmH9/pjt79pJl/Ds4a3ITixQuduxLoSIiIiKFi87giIiIiOeowBERERHPUYEjIiIinqMC\nR0RERDxHBY6IiIh4Tl78r6+FVpmyZV2VKjHhDiNPpRzyhzuEPFfc580/H3PkTwh6x5dr1+x2zpU7\nmXWUKVvWVa5S9dgNC5G//d77tWrR0zz4AQaKeDAx8yIvC6NTusCpUiWGRUtXhDuMPLVl5+/hDiHP\nVSt/1rEbFUKnF/XeCdQzixU56b/yXLlKVRZ+5q28TN77V7hDyHMVSnrz7+YV93kvL0uccdop+dfX\nvTeSIiIicspTgSMiIiKeowJHREREPEcFjoiIiHiOChwRERHxHBU4IiIi4jkqcERERMRzVOCIiIiI\n56jAEREREc9RgSMiIiKeowJHREREPEcFjoiIiHiOChwRERHxHBU4IiIi4jkqcERERMRzVOCIiIiI\n56jAEREREc9RgSMiIiKeowJHREREPEcFjoiIiHiOCpyTtGDeHJo1qkvcebV47pmnssxPSUnhmqv6\nE3deLTq2a8m2rfEAfLJwPh1aNaNV00Z0aNWMJYsXhTjyo/v80wX0viCOnu0b8eaY0Vnmr12xjCu7\nt6HZP0qz4OOpGebdNLg37RpU4dZrLwtVuLnixXGSnC2cP5fmjevRtEFtnh/1dJb5KSkpXHvVlTRt\nUJtO7c9PG/O1q1fSvmUc7VvG0a5FLLOmT82ybLgs/WQ+3ds2pkurhrz+0qgs8w+mpHDHjYPp0qoh\n/bt3IGH7VgD+PniQB26/gUsvbE7vi1qy8vPPQh36US2cP5cWjevRtOHRx+m6wVfStGFtLu6QaZzO\njws8WhascZo/bw6xDerQsF5NRo/Mfn8zZOAVNKxXkw5tWrI12Kc9e/bQ7eILiSxbgjv+fXOIo/ae\nYxY4Zvb5yb6JmS02syYnu55cvld7Mzs/FO/l9/v5z+23MOGjmSxfs4HJEz/k200bM7R59603iIgo\nxZoN33HjTf9m2IP3AlCmTFnenzSVZavW8fLYN7jxuiGhCPmY/H4/Ix66gxfGTWLSvJXMnT6ZzT98\nm6FNxehKPDJyDJ0v6Zdl+auuv4XHRr8aqnBzxYvjJDnz+/3cffstfDhlBstWf8WUieP5LtOYv/fW\nG0RERLDqq2+54V+38siD9wFQu259Fny2gsXL1/Dh1Fncccs/OXToUDi6kYHf72f4A3cw5p0pTP9k\nFR9Pm8RP32fMzSnj36ZEyQhmL1vPoKH/YvQTDwEw6f1xAHy0cAWvfTCdZx67j8OHD4e6C1n4/X7u\nueMWxk+ZwbJVX/HRpPF8922mcXo7OE7rA+P06EPpxmnJChZ/vobxH83izlsLzjjd8e+bmTxtFqu+\n/JpJE8dn2d+8Pe4NIkqVYv033/Ovm2/l4fvvAeD000/ngYceYfiTWQu9/OTV4/wxCxznXJaVmJkv\n07SZWUE5G9QeCEmBs2b1SqpVr0FMteoUK1aM3n0vY/bM6RnafDxzOlcMGARAz0v7sGTxIpxzNGjU\nmMjIKADq1K3Hn3/9SUpKSijCztE369dQuWp1KlWpRtFixejUozeL58/K0CaqUlXOrVMfK5J1yJu1\nas+ZZ58dqnBzxYvjJDlbm2nML+17ObNnzcjQZvasGWljfsmlffgsOOZnnnkmPl9gF5fy11+YWcjj\nz86GdaupElOdylUDudmlZx8WzZuZoc2iebPo2e9KADp168WKpYtxzvHTD9/S7Px2AJQpW45zSpTk\nm/VrQ96HzNauXklMunHq1edyZs/MOk6XXxkYpx69Cv44rV61kuo1alAt2Kc+/S5nVqb9zayZ0+g/\n4CoAevXuy+Jgn8466yxatmrN6aefHtKYvXqcz80ZnAPBf9ub2WdmNh3YaGYxZvadmb0NfA1UNrNO\nZrbczNaa2UQzy3Kky66NmXU2s4np2rQ3s5nB52PMbLWZfWNmj6RrE29mjwTXs8HMaptZDHADcJuZ\nrTOzNsfq38lISkwkulLltOmo6EokJSUetY3P56NEiZL8smdPhjbTp06hYcPGFC9ePD/DzZWdyYlU\niIxOm65QMZpdyUlhjOjkeXGcJGdJiYlEVaqUNh0VHU1SYkKWNhnGvOSRMV+zagWtmjSkbfPGPPP8\ny2kH0nDamZRExUy5uTMpY27uTE6kYmSg3z6fj7NLlGTvr3uoVac+i+d/zKFDh9ixLZ6NG9aRnGl7\nhENSUiLR0ZnGKSljXMnHGKfWTRvStkVjRj5XMMYpKTGBShn2N9EkJmT97FU6xv4mlLx6nD/eaiwW\nuNU5VzM4fS7winOuHvA78ADQ0TkXC6wGbs/U6bJHabMAaG5mZwWbXg6MDz6/3znXBGgAtDOzBulW\nuTu4njHAnc65eOC/wLPOuUbOuYJzofkoNm38hkcevJfRL44JdyiSA43TqSWuaXOWrV7P/E+X89yo\np/jrr7/CHdJJufSKq6gQGc3lXdvy1LC7aRTXnCKnFZQv4ycurmlzlq5az/zFy3l+dOEfpwLCM8f5\n4/2Er3TObUk3vdU590XweQugLrDMzNYBg4GqmZbPto1z7hAwB+gRPC3WDZgWXOYyM1sLfAnUCy6f\nakrw3zVATG46YGbXByvF1bt378rNIkcVGRVFwo7tadOJCTvSLmdk1+bQoUPs37+P0mXKAJCQsIOr\n+vflldfepFr1GicVS14pXzGKn9N9g/o5OYFyFSPDGNHJ8+I4Sc4io6JI3LEjbToxIYHIqOgsbTKM\n+b4jY56qZu06nHXW2Wza+HX+B30M5SMjSc6Um+UjM+Zm+YpRJCcF+n3o0CEO7N9HRKky+Hw+7h42\ngsnzPufFNz5k//69xFQ/N6TxZycyMoqEhEzjFJlxnCoexzh9WwDGKTIqmh0Z9jcJREVn/eztOMr+\nJp+UTT3uBR/X59C20B/nUx1vgfN7DtMGzA9WVI2cc3Wdc9dmap9Tm/HAZcAFwGrn3G9mVg24E7jQ\nOdcAmAWkvziZejOEH8jVuUnn3FjnXBPnXJOyZcvlZpGjio1ryuaffmRr/BYOHjzIlEkT6NytR4Y2\nXbr1YPx77wAw7aPJtGnXATNj3969XNH7Eh569AlatGx1UnHkpboNYtke/xMJ2+P5++BB5s2YQruO\nXcMd1knx4jhJzhpnGvOPJn1I567dM7Tp3LV72phPTzfmW+O3pN2sun3bVn74/juqVIkJdReyqN8w\njm1bfmLHtkBuzp42mQ4XdcvQpsNFXZk28X0A5s2aSvNW7TAz/vzzD/74I7C7/nzJInw+HzVq1g55\nHzJrHNeULenGaerkD+ncLes4ffh+YJxmTJ1M6xzGqXIBGKe4Jk3Z/OOPxAf7NHnih3TNtL/p2u0S\nPnjvbQCmTplEu2Cf8tHu1ONe8DE2h7aF/jifKi8vWH4BvGxm/3DO/Rg8DRXtnPs+l20+Bd4AhnLk\ntFUJAht3n5lVALoAi48Rx2/B5fKdz+fj6VHP07dnV/x+PwOuGkKduvV44rGHaRzbhC7dejBw8DXc\ncN1g4s6rRalSpXj9rcDO57VXX2bL5h8Z+eRwRj45HIDJ02dTrnz5UIR+VD6fj/888gw3XdUb/2E/\nPfsNpEbNOowZ/Th1z2tMu4u68s36Ndx5w0D279vLZwtn8+pzTzJx3goAru3XmfjN3/Pn77/TpWUd\nHhzxIue36xj2PnltnCRnPp+PEaOep1+vbhz2+7ly0BBq163Hk48No1FsHF269WDA4Gv453VDaNqg\nNhGlSvHauPcAWLF8Gc+PGknRoj6sSBFGPvsiZcqWDXOPAn2677Fn+L8BvfAfPsyllw/iH7Xq8NLI\n4dRr2JgOnbrR+4qruPfWoXRp1ZCSEaUY+cqbAPyyexf/N6AXVqQIFSpG8eTzr4W5NwE+n48nn3me\ny3p14/BhP/0HDaF2nXqMGD6MRo3j6NytBwOuuoZ/Dh1C04a1KVWqFGPfPDJOL4weia+ojyJFivD0\n6IIzTiOffYFLe3TB7/czaPDV1Klbj+GPPkxsbBxdu1/CVUOu4fprrqJhvZqUKlWaN995P235+rWq\ns/+3/fx98CCzZkxj6sw51K5TN4d3DKlCdZw351zODcwOOOfONrP2BK5/dQ++HgPMdM7VT9f2AuAp\nIPUuzAecc9PNbHFw2dVHaxNc/iVgCFDeOfdH8LVxBO6W3g7sA6Y758aZWTzQxDm32wI/TXvGOdfe\nzGoCk4DDwM05XZ9rHNvELVq64ljbqFDZsjNz8V34VSt/1rEbFUKnFy3890BkdmaxImuC19JPWKPY\nOLfwM2/lZfJe790bUqFkaH/pEyrFfd7LyxJnnJZjXnr1OH/MAsfLVOAUDipwCg8VONlTgVN4nIoF\njld5byRFRETklKcCR0RERDxHBY6IiIh4jgocERER8RwVOCIiIuI5KnBERETEc1TgiIiIiOeowBER\nERHPUYEjIiIinqMCR0RERDxHBY6IiIh4jgocERER8RwVOCIiIuI5KnBERETEc1TgiIiIiOeowBER\nERHPUYEjIiIinqMCR0RERDxHBY6IiIh4jgocERER8RwVOCIiIuI5vnAHEE5+5/g95VC4w8hTFSNO\nD3cIee6LLXvCHUK+aFq1dLhDKJAM4zSzcIeRpyLOKhbuEPLcwh9+DncI+aJ9jfLhDkHyiM7giIiI\niOeowBERERHPUYEjIiIinqMCR0RERDxHBY6IiIh4jgocERER8RwVOCIiIuI5KnBERETEc1TgiIiI\niOeowBERERHPUYEjIiIinqMCR0RERDxHBY6IiIh4jgocERER8RwVOCIiIuI5KnBERETEc1TgiIiI\niOeowBERERHPUYEjIiIinqMCR0RERDxHBY6IiIh4jgqck/TJgnm0a3YerePq8vJzI7PMT0lJ4cZr\nBtI6ri49OrZh+7Z4ALZvi+cfURFc3LYZF7dtxr233xTiyI/ukwVzadO0Pq1i6/DSs9n36YZrBtAq\ntg7dO7ZO61OqhO3bOLdSaf774ugQRXxsq5cuYmj387m2S3MmvP5ClvkbVi/n5n4d6d4wiqXzZmSY\n171BJDf1uYCb+lzAIzcNClXIubJw/lxaNK5H04a1eX7U01nmp6SkcN3gK2nasDYXdzifbVvjAVi7\neiXtz48LPFrGMmv61BBHnr8WzJtD00Z1iT2vFs8+81SW+SkpKVxzVX9iz6tFx3Yt07bLJwvn075V\nM85v2oj2rZqxZPGiEEees08WzKXtMXLzxmxyc/u2eGpElqRTm6Z0atOUe277V4gjP7p1yz7htkvb\ncuslrZj25ktZ5m9a8wX3XNmZK5tW5YsFM9Ne/2bVMu6+olPaY1CLGqz6ZE4oQz+qRQvm0iquHi0a\n1eHF0dnn5fVDrqRFozp0uaBV2ucv1Y7t26geVYpXXig4+9DCyBfuAHJiZjHA+c65909g2QPOubPz\nPKh0/H4/D/znVt6fMovIqEp0v7AVF3XuTs3addLajH93HBERESxds5FpkyfwxLAHGPPGuwBUjanO\n3CUr8zPE4+b3+7n/rlv54KOPiYyqRNcLzqdTl4x9+uCdNylZMoJlazcxbfIEHh92P/994720+cMe\n+A8dOl4cjvCz5ff7eWX4PTz+2gTKVozi35dfTIsOF1OlRq20NuUjo7l9+PNMHjcmy/LFip/OS5ML\n1oEOAv26545bmDhtNlHRlejUrgWdu3WnVu26aW3ee/sNIiIiWLX+Wz6a9CGPPnQfr7/1PrXr1mfB\nkhX4fD6Sk5Po0DKOi7t2x+cr0LuEXPH7/dx1+y18NGMOUdGVuKBNC7p060HtOke2yztvvUHJiFKs\n3fAdkyd+yLAH7+WNtz+gTJmyfDBpKpGRUWz85mv69uzKxh+3hbE3R/j9fh6461beD+Zmt2xyc3ym\n3Hxi2P2MCeZmTEx15n22KlzhZ+uw388bTz3A/a+8T5kKkdw3sBtx7TpRqXrNtDZlIqO5cdhoZr7z\naoZl6zVtxVPj5wFwYN+v3NqzNQ1atAtp/Nnx+/3ce8etTJj6MZHRlejcoSWdumbMy/fffpOIiFJ8\nsW4TUyd9yPCH72PsuCOHuYfvu4sLCtA+NLcK2jG7oJ/BiQGuzG6GmYV9T7xuzSpiqtWgakx1ihUr\nxiW9+zFvdsZv//M+nkHfKwYC0K1nb5Yt+QTnXDjCzZUv16wipvqRPvXsfRlzP87Up9kz6Nc/cCaj\nW8/eLP30SJ/mzJpGlSoxGZI53L7fsJaoKtWIrBxD0aLFaNulF8sXZfymVyG6CtVq1aNIkYKeEkes\nXb2SmOo1iKkWGKtefS5n9syMYzV71gwuvzIwVj169eGzxYtwznHmmWemFTMpf/2FmYU8/vyyZvVK\nqqfbLr37XsbHM6dnaDN75nT6Dwhsl56X9uHT4HZp0KgxkZFRANSpW48///qTlJSUkPchO+uyyc15\nx5GbBdGPX6+jYqUYKlSqiq9oMc6/uCerF8/L0KZ8VGWq1qyL5ZCbXyyYRaNWHSh+xhn5HfIxfblm\nFdWq16Bqal72voy5szKO09yPZ3BZMC+79+qTYZxmz5xGlarVqFWn4OxDj0MMBeiYnS97czOLMbNN\nZvaamX1jZvPM7Awzq2Fmc8xsjZl9Zma1g+3HmVnfdMsfCD4dAbQxs3VmdpuZDTGz6Wa2CFhoZmeb\n2UIzW2tmG8ysZ37052iSkxKJiq6UNh0ZFU1yUuJR2/h8Ps4pUYJff9kDBE4bd27XnL7dO7Ji+dLQ\nBZ6DQLyV06YDfUrI2CYxY59KBPv0+4EDvPz8KG6/+4GQxnwse3YmU7ZiVNp02QpR7NmZnOvlDx5M\n4ZbLOnHblV34fOHH+RHiCUlKSiQ63ecvKjqapGzGKrpSYDx9Ph8lSpbklz2Bz9+aVSto3bQhbVs0\nZuRzL3vi7A1AUro+A0RFVyIpU14mZt4uJY5sl1TTp06hYcPGFC9ePP+DzoWkpEQi0+Vmxajsxzsy\nm9wE2LYtnovbNqNPt46s+Lxg7G9+2ZVEmYqRadOly1fkl51Jx72e5XOnc/7FvfIytBOWlJiQ8bgQ\nHZ3l85eUlJDpuFCSX4L70Jeee4Y77wntPtSrx+z83KOdC/R3zg01swlAH+Bq4Abn3A9m1hx4Bbgg\nh3XcA9zpnOsOYGZDgFiggXPul2BFeKlzbr+ZlQW+MLPpriB/ZQkqXyGSFV/9QKnSZfhq3VquG9iP\nhZ9/yTklSoQ7tBM26qnHGHrjLZx1dr5eGQy5cfPWULZCJEnb47n32r5UO7cukVViwh3WSYtr2pyl\nq9bz/bebuOmGa7iwU2dOP/30cIdVIGza+A3DHryXKdNnhzuUPFG+QiQrN/yYtr+5dkA/Fi0v3Pub\nVL/u+pltP35Lw5bhvzx1skY++RjX/zNs+1DPHbPzs8DZ4pxbF3y+hsCpq/OBielOh5/IV6P5zrlf\ngs8NeMLM2gKHgWigAnDUr+dmdj1wPZDhW96JqBgZRWLCjrTppMQEKkZGZdsmMroShw4d4rf9+ylV\nugxmlvbNsEGjWKpWq87mn36gYeO4k4rpZAXi3Z42HehTdMY2UYE+RQX7tD/Ypy9Xr2LWtI94/OH7\n2L9vL0WKFKF48dO5+vp/hrobGZQpX5HdyUe+Qe3+OZEy5SvmevmyFQLfMCMrx9Cg6fn89O2GAlHg\nREZGkZDu85eYkEBkNmOVsGP7kbHat4/SZcpkaFOzdh3OOutsvt34NY1im4Qk9vwUGexzqsSEHWmX\nnVJFBdtEp32Gj2yXhIQdDOrflzGvvUm16jVCGntOIiOjSEqXm8mJ2Y93Uja5WVD3N6XLRbIn+cgZ\nm192JlO6fGQOS2S1fP4MmnbojK9o0bwO74RERkVnPC4kJGT5/EVGRmfYh/62fx+lS5fhyzUrmTl9\nCo+l7kOtCMVPP51rT34fWtbMVqebHuucG5upTYE8Zp+M/LzhIP2Faz9QGtjrnGuU7pF6d9yh1FjM\nrAhQLIf1/p7u+QCgHBDnnGsE/Azk+BXUOTfWOdfEOdekdNlyx9ejTBrGNiF+849s27qFgwcPMn3K\nRC7q3D1Dm4u6dGfS+MBNxbOmTaFVm/aYGXt278Lv9wOwNX4zWzb/RJWYaicVT15oFNuELT8d6dO0\nKRPo1CVjnzp17s7ED94Bgn1qG+jTR7MXseKr71nx1fdcd+PN3Hz7f8Je3ADUrN+YxG2bSd6xlb//\nPsiS2VNp0SF3N/D9tm8vfx8MfJT3/bqHjV+upEqNmsdYKjQaxzVly08/sjU+MFZTJ39I524Zx6pz\n1+58+H5grGZMnUzrdh0wM7bGb+HQoUMAbN+2lR++/47KBaBoywuxcU35Kd12mTJpAl269cjQpnO3\nHnzwXmC7TPtoMm2D22Xf3r1c3vsSHn70CVq0bBWO8I+qYTa5eVGm3LzoKLmZdX/zY4HY39So15Dk\n7VvYmbCNQ38f5PO504hrd9FxrePzOdNo1TmkdyfkqFFsEzanz8spE+jUNdM+tGt3JgTzcubUyWnj\nNG3OJ6ze8AOrN/zA0Btv5pY77s6L4gZgd+pxL/jIXNxAAT1mn4xQXnTfD2wxs37OuYkWKAkbOOfW\nA/FAHDABuARILcV/A87JYZ0lgZ3Oub/NrANQNd+iz4bP5+Oxp59jYN8e+P1+Lh8wmFp16vLME4/Q\noHEcnbp054qBQ/j3DdfQOq4uEaVK8/LrbwOw4vOljHryUXxFi1KkSBGeHPUipUqVDmX42fL5fAx/\n+jmu7NOdw34/lw8YQq06dRn5xCM0bBRLp649uGLQ1dxyw9W0iq1DRKnSvPK/d8Iddo5O8/m48b4n\neeD/ruCw30+nS/tT9R+1eeelpzi3XkNadOjM9xu+5LF/X82B/XtZsXge7748kv9OW8L2zT/w4qN3\nUsSKcNgdpt+1N2f49VU4+Xw+nnzmeS7r1Y3Dh/30HzSE2nXqMWL4MBo1jqNztx4MuOoa/jl0CE0b\n1qZUqVKMfTPwi5oVy5fxwuiR+Ir6KFKkCE+PfpEyZcuGt0N5xOfz8fSo5+nTsyt+v58BVw2hTt16\nPPHYwzSKbULXbj0YNPgabrhuMLHn1aJUqVL8763Ajz5ee/Vltmz+kaefHM7TTw4HYMr02ZQrXz6c\nXQKO7G8/1lMvAAAftElEQVQGHCM3b80mN7/4fCmjnnwEny+wvxlRQPY3p/l8XH33YzzxrwEcPnyY\nDpdcTuUatZgwZiTV6zakSbtO/PTNOkbdcR2/79/H2iXzmfTf0TwzKfCrxp2J29nzcyJ14lqGuSdH\n+Hw+nnjmOfr37obff5j+AwdTu049nno8kJcXd+3BlYOu5qbrh9CiUR0iSpXi1eAvawuYQn/Mtvy4\n9BX8qdhM51z94PSdwNnAW8AYIJLABhnvnHvUzCoA04AzgDnAv5xzZ5tZUWAuUAYYB/wKNHHO3RRc\nb1lgRnDdq4EWQBfnXHxufnLWoHGc+3jR53nZ9bAr4qFfw6Ran7A33CHki6ZVw3+AyWvlzim6xjl3\nUte5Gsc2cZ8sXZFXIRUIvx/0hzuEPLd0y65wh5Av2tcIfzGb1yqWLJZjXhaWY/bxypcCp7BQgVM4\nqMApPFTgZE8FTuFxKhY4XlV4/uiHiIiISC6pwBERERHPUYEjIiIinqMCR0RERDxHBY6IiIh4jgoc\nERER8RwVOCIiIuI5KnBERETEc1TgiIiIiOeowBERERHPUYEjIiIinqMCR0RERDxHBY6IiIh4jgoc\nERER8RwVOCIiIuI5KnBERETEc1TgiIiIiOeowBERERHPUYEjIiIinqMCR0RERDxHBY6IiIh4ji/c\nAYTTaWacVdxbm2DPgYPhDiHPRRQvFu4Q8sWOX/4MdwgFlgt3AHJMpU/3Zl4m/qq89AqdwRERERHP\nUYEjIiIinqMCR0RERDxHBY6IiIh4jgocERER8RwVOCIiIuI5KnBERETEc1TgiIiIiOeowBERERHP\nUYEjIiIinqMCR0RERDxHBY6IiIh4jgocERER8RwVOCIiIuI5KnBERETEc1TgiIiIiOeowBERERHP\nUYEjIiIinqMCR0RERDxHBY6IiIh4jgocERER8RwVOCdpwbw5NGtUl7jzavHcM09lmZ+SksI1V/Un\n7rxadGzXkm1b4wH4ZOF8OrRqRqumjejQqhlLFi8KceRHt2TRPDqd35ALm9fn1ReeyTI/JSWFW4cO\n4sLm9enTuS07tm0FYNqk8fS4oHnao2bFs9j49fpQh5+tL5Ys4IpOTel3YSxvv/pslvlfrlzGkJ7t\naFO7LItmT0t7/fuNGxjarxMDurRkUPdWLJg1JZRh52jZ4vn06hDLJW0b8sYro7PMX7NiGf27tqFJ\n9VLMnzU1w7zpk97jknaNuKRdI6ZPei9UIYeMF/MS4JMFc2nbtD6tYuvw0rMjs8xPSUnhxmsG0Cq2\nDt07tmb7tngAtm+Lp0ZkSTq1aUqnNk2557Z/hTjyo1v12SKu6dqSIRc3Y/xrL2SZ/9Xq5fyzz4V0\nPi+SJXNnZJn/+4HfuLJDQ14afk8ows2VZYsXcOkFcVzSrhFvHiU3r+zWhqY1SrPg44y5OWPS+/Rs\n35ie7RszY9L7oQrZk0JS4JjZYjNrEqL3am9m54fivfx+P/+5/RYmfDST5Ws2MHnih3y7aWOGNu++\n9QYREaVYs+E7brzp3wx78F4AypQpy/uTprJs1TpeHvsGN143JBQhH5Pf72fYPbfx+vtTmf3ZWmZ+\nNJEfvtuUoc2k98dRIiKChSu+5ur/u5mRjz0AQM++VzBj0QpmLFrBMy/9j0pVYqhbv2E4upGB3+/n\nmWF3Mer1ibw/+wsWzJzMlh++zdCmYlRlHnjqZS7q0TfD66efcQYPjRzDe7OXM/p/k3j+8fv4bf++\nUIafLb/fz4gH7+CltyYzecEq5kyfxE/fZ+xTZFQlHhk1hs49+2V4fd/eXxj73FO8M20R707/hLHP\nPcX+fb+GMvx85cW8hEC/HrjrVt6ZOJ1PvljPtMkf8v23GXNz/DtvUrJkBMvWbmLojbfwxLD70+bF\nxFRn3mermPfZKkY8+3Kow8+W3+/npeF38/irH/DajKUs/ngKW3/8LkOb8pHR3PnEC1zQrXe263jr\nhRGc16RlKMLNFb/fz1MP3cGL4yYxef5K5kyfzOYfsubmsGeOkpvPj+DtqQt5Z9oixj4/okDlZmE7\nludZgWMBBeGMUHsgJAXOmtUrqVa9BjHVqlOsWDF6972M2TOnZ2jz8czpXDFgEAA9L+3DksWLcM7R\noFFjIiOjAKhTtx5//vUnKSkpoQg7R1+tXU3VajWoElONYsWK0a1XXxbOmZmhzYI5s+h92UAAOve4\nlOVLF+Ocy9Bm5kcT6N4rY7EQLhu/WkOlqtWJrhJD0WLF6NitN58t/DhDm8hKVfhH7foUyfQRrlLt\nH1SOqQFAuQqRlCpTlr2/7A5Z7Efz9brVVI6pTqUq1SharBgX9+jD4vmzMrSJqlyVmnXqU6RIxj59\n/ulCWrTpQMmI0pQoWYoWbTqwbPGCUIafr7yYlwDr1qwipnoNqsYE+tWz92XM+zjjGY15s2fQr3+g\nX9169mbpp59kyc2C5LsNa4mqUo3IyoHcbNflUj5fNCdDm4rRVaheqx5WJOvh5ftv1vPrnl3End8+\nRBEf29frAvubI7nZm8XzjpKbmfY3yz9dRPPWR3KzeesOfL54YSjD99Sx/KQ6YWYxZvadmb0NfA0M\nMrPlZrbWzCaa2dnZLNMpcxsz62xmE9O1aW9mM4PPx5jZajP7xsweSdcm3sweCa5ng5nVNrMY4Abg\nNjNbZ2ZtTqZ/x5KUmEh0pcpp01HRlUhKSjxqG5/PR4kSJfllz54MbaZPnULDho0pXrx4foabK8nJ\niURGRadNV4yK5ufkjH36OSmRitGBNj6fj7PPKcGvv2Ts06xpk+l+6WX5H3Au7EpOokLkkT6VqxjF\nrp+Tjns9G9ev4e+DfxNdpVpehndCdiYnUSGyUtp0hcgodmUap6PJvD3KV4xiV/Lxb4+Cyot5CZCU\nlEhk9JF+VYyKJikpIUOb5MREIqMDn4tAv47k5rZt8Vzcthl9unVkxedLQxd4Dnb/nEy5iulzM5I9\nO3P3WTx8+DBjn36Y6+8alk/RnZhdPydSMd0+tHxkNDtzub/Z+XMiFaPS53U0O3/OXV6fDK8ey/Oi\nSjsXeAVoB1wLdHTOxQKrgdvTNzSzssAD2bRZADQ3s7OCTS8Hxgef3++cawI0ANqZWYN0q9wdXM8Y\n4E7nXDzwX+BZ51wj59xnedC/fLVp4zc88uC9jH5xTLhDyTPr1qzkjDPOpGadeuEOJc/s3pnMo3fd\nwP0jXspyRkS8x2t5Wb5CJCs3/MjcJSt5+PGnuWnoYH7bvz/cYZ2UGR+8SbO2F1KuYlS4Q/EKzx3L\n82JPvdU59wXQAqgLLDOzdcBgoGqmttm2cc4dAuYAPczMB3QDUu/0vMzM1gJfAvWCy6dKveNzDRCT\nm2DN7PpgFbl69+5dx9fTTCKjokjYsT1tOjFhR9rp7ezaHDp0iP3791G6TBkAEhJ2cFX/vrzy2ptU\nq17jpGLJKxUrRpGUeORbYXJiAhUy7UAqREaRnBBoc+jQIQ78tp9SpcukzZ81dRLdL814bTmcylWM\n5Od033R3JSdSrkJkrpf//bf93Dn0cq6/7QHqN26aHyEet/IVI/k5aUfa9M9Jibne0WfeHjuTEylX\nMffbo6DzYl4CREZGkZRwpF/JiQlEpjsTB1AxKoqkhMDnItCvQG4WL148LUcbNIqlarXqbP7ph9AF\nfxRlK1RkV3L63EyiTPncfRY3rlvFtPfeYFDHOMaOHMaCaRP43+jH8ivUXCtXIYrkdPvQnUkJlM/l\n/qZ8hSiSE9PndQLlK+RJAVc29bgXfFyfTZtCdSzPjbwocH4P/mvA/GC11cg5V9c5d22mtjm1GQ9c\nBlwArHbO/WZm1YA7gQudcw2AWcDp6daXenHcD/hyE6xzbqxzrolzrknZsuWOu7PpxcY1ZfNPP7I1\nfgsHDx5kyqQJdO7WI0ObLt16MP69dwCY9tFk2rTrgJmxb+9eruh9CQ89+gQtWrY6qTjy0nmN44jf\n/CPbt8Zz8OBBZk2dxIUXd8vQ5sKLuzJlwrsAzJnxES1at8PMgMBp49nTJ9OtV8EpcOqcF8uO+J9I\n3L6Vvw8eZMGsKbS+sEuulv374EHu+dcguvS6ggu69MznSHOvXsM4tm3ZTMK2eP4+eJC5MybT/qKu\nuVr2/HYXsnzJIvbv+5X9+35l+ZJFnN/uwnyOOHS8mJcADWObsOWnH9m2NdCvaVMmcFGX7hnaXNS5\nOxM/CPRr1rQptGrbHjNjz+5d+P1+ALbGb2bL5h+pEhP+S6216jcmYetmknYEcvPT2R/RssPFuVr2\n3pH/5b1FX/LOgjVcf9cwOva8jGtvfzCfIz62eg1j2R7/EwnbU3NzCu1ymZst213AF58dyc0vPltE\ny3YX5EVYu1OPe8HH2GzaFKpjeW7k5bn2L4BWZvYPADM7y8xqHkebT4FYYChHTmmVILDR95lZBSA3\nR6XfgHNOqie55PP5eHrU8/Tt2ZUWsfXp1acvderW44nHHmb2rMDNfwMHX8Mvv+wh7rxajHnxWR5+\n9AkAXnv1ZbZs/pGRTw6nbYs42raIY9fOnaEIO0c+n4+HnxzNNVdcQufWjelySW/OrV2X5556NO1m\n435XDmHvr79wYfP6vPnfF7jz/iPfmlYtX0rFqEoFYueZyufzcfvDT3PbNX3o37k5F3TpRfVz6/Da\nc0+k3Wy88au19Gxdj0VzpvH0Q7cxoEvgVxkLZ3/EulWf8/GU9xncow2De7Th+40bwtkdINCnux8d\nyT+vupTeFzahU7dLqVGzDq+MGs7i+YE+fbN+DRc3r838WVN5/L5b6dOxGQAlI0oz9Jb/MLBHewb2\naM/1t95NyYjS4exOnvJiXkKgX489/RwD+nSnQ/MG9OjVl1p16jLyiUfSbja+YtDV/PrrL7SKrcPY\nV57n3oeHA/DF50u5qHUcndo05f8G92fEqBcpVSr8Y36az8dN94/gvqGXc12PVrS9uCcx59bmrRdH\nsDx4s/F3G77kyg4NWTJ3Bs8Pu5OhPfL11sqTFsjNZ/jXVb3p07EpF3XvRY2adRgz+nE+TZebnVvU\nYf7HU3n8vn/T96LmQCA3r7vlPwy8pAMDL+nA0FvCkpueOZbbydxhH7wRaKZzrn5w+gLgKSD1rrwH\nnHPTzWwxgetqq4/WJrj8S8AQoLxz7o/ga+MI3Em9HdgHTHfOjTOzeKCJc2538Gdrzzjn2gc38iTg\nMHBzTtfuGsc2cYuWrjjh/hdEew4cDHcIeW7X/oLxK5a8dkax08IdQp5rXLXEmuB19hNfhwfz8o+D\n/nCHkOc2JoX/zyXkh7JnFoybyvNSbEzJHPOysB/Lj9qvgvwTwvzmxR2pCpzCQwXOUdbhwbxUgVN4\nnIoFjlfp5yAiIiLiOSpwRERExHNU4IiIiIjnqMARERERz1GBIyIiIp6jAkdEREQ8RwWOiIiIeI4K\nHBEREfEcFTgiIiLiOSpwRERExHNU4IiIiIjnqMARERERz1GBIyIiIp6jAkdEREQ8RwWOiIiIeI4K\nHBEREfEcFTgiIiLiOSpwRERExHNU4IiIiIjnqMARERERz1GBIyIiIp6jAkdEREQ8xxfuAMKpiMEZ\nxU4Ldxh5qlLpM8IdQp4r5vNmHT59U2K4QyiQzLw35l7bzwDUiywZ7hDyxcQNCeEOQfKIt/YiIiIi\nIqjAEREREQ9SgSMiIiKeowJHREREPEcFjoiIiHiOChwRERHxHBU4IiIi4jkqcERERMRzVOCIiIiI\n56jAEREREc9RgSMiIiKeowJHREREPEcFjoiIiHiOChwRERHxHBU4IiIi4jkqcERERMRzVOCIiIiI\n56jAEREREc9RgSMiIiKeowJHREREPEcFjpwSFi+cR4dm59G2SV1eeW5klvkpKSn869qBtG1Sl54X\ntWH7tngAtm+Lp2Z0BF3aNaNLu2bcd8dNIY786DZ+8SmP9b+QRy7vwLx3xmSZv2j86zw+sBNPDu7C\ni7cO4JfkhLR5K2ZP5tErOvDoFR1YMXtyKMMWyeCTBXNp07Q+rWLr8NKz2efmDdcMoFVsHbp3bJ2W\nm6kStm/j3Eql+e+Lo0MU8bFtWvEpjw+4kOH9O7Dg3ay5+cmHr/PkoE48NaQLL/87Y26unD2Z4f07\nMLx/B1YqN09KyAocM4sxs6+zef1RM+t4jGWHmdmdR5l3IK9iFG/y+/08+J9beWvCNBZ8vo7pUybw\n/bebMrT58N1xlIyIYMnqjVx7482MeOSBtHlVY6oz+9OVzP50JU+MeinU4WfrsN/PxNEPc+Mzb3L/\nu3NZs2AGSVt+yNCmUs163PX6NO59azaN2ndh6isjAPh9/15mv/ECd4z9iDvHTmX2Gy/wx/594eiG\nnOL8fj/333Ur706czidfrGfq5A+z5OYH77xJyZIRLFu7iaE33sLjw+7PMH/YA/+hQ8eLQxl2jg77\n/Ux69mH+b+Sb3PP2XNYunEFyfKbcPLced7w2jbvHzaZh+y5MH3MkN+eOe4HbXv2I28dOZe64F/jj\nt/zPTa8en8N+Bsc595BzbkG44xDvWrd2FTHValAlpjrFihWjx6X9mD97RoY282fPoM8VAwHoeklv\nli35BOdcOMLNla2b1lO2UlXKRlfBV7QYcR27s2Hp/Axtasa2pNjpZwAQU68xe3clA7BpxRJqN23N\nWSUiOLNESWo3bc3GFZ+GvA8iX65ZRUz1GlQN5mbP3pcx9+OMuTlv9gz69R8EQLeevVn66ZHcnDNr\nGlWqxFCrdt2Qx340Wzetp2x0VcpGBXKz8YVZc/Pc9LlZtzH7grn57col1GwSzM1zSlKzSWs2hTE3\nC/vxOdQFzmlm9pqZfWNm88zsDDMbZ2Z9Acysq5l9a2ZrzOwFM5uZbtm6ZrbYzDab2S2ZV2xmb5tZ\nr3TT75lZzxD0SQq45KREIqMrpU1HRkWTnJSYpU1UVKCNz+fjnBIl+PWXPUDgMlWX9s25rEdHVi5f\nGrrAc7B3VzKlykemTUeUi2Tvrp+P2n75zAnUbd4OgH27fiYi/bLlK7Ivh2VF8ktyUiJR0ZXTpgO5\nmZCxTWIiUdFHcrNEMDd/P3CAl58fxe13P0BBsm931tzMKb++mDWBOulys1T4ctNzx+dQFzjnAi87\n5+oBe4E+qTPM7HTgVaCLcy4OKJdp2drAxUAz4GEzK5pp/v+AIcF1lQTOB2blQx/kFFK+QiTL1//A\n7MUrePCxp7nl+sH8tn9/uMM6LqvmTmX7txu48Mqh4Q5FJM+Meuoxht54C2edfXa4Qzlhq+dNZft3\nG7igf4HITc8dn0Nd4Gxxzq0LPl8DxKSbVxvY7JzbEpz+INOys5xzKc653cBOoEL6mc65T4Fzzawc\n0B+Y7Jw7lDkAM7vezFab2epdu3edfI+kwKsYGUVSwo606aTEBCpGRmVpk5gYaHPo0CF+27+fUqXL\nULx4cUqVLgPAeY1iqVqtOlt+yng9PRwiylXk151JadN7dyURUa5ClnbfrlrK3Ldf5vqnxlK0WHEA\nSparwN70y+5MpmQ2y4rkt4qRUSQmbE+bDuRmdMY2UVEkJhzJzf3B3Pxy9Soef/g+mjeoyetjXuTF\n0U/z5thXQhp/dkqWzZqb2eXXd6uXMu/tl7nuybH40uXmr/mTm2VTj3vBx/XZtAn78TmvhbrASUn3\n3A/48njZt4GBwNXAG9mtxDk31jnXxDnXpFzZzEWoeFHDxk3YsvlHtm3dwsGDB5nx0UQu6tI9Q5uO\nnbszefy7AHw8fQrnt2mPmbFn9y78fj8A2+I3s+Wnn6gSUy3kfcisSu0G7Noez+7E7Rz6+yBrFszk\nvFYZ7wXc/v03fDjyAa4fMZZzSpVNe71O87ZsWvUZf+zfxx/797Fp1WfUad421F0QoVFsE7b8dCQ3\np02ZQKdMudmpc3cmfvAOALOmTaFV20BufjR7ESu++p4VX33PdTfezM23/4err/9nOLqRQZXaDdi9\nI549wdz8cuFM6mfKzR3ff8OEZx5g6JMZc7N2s7Z8t+oz/vhtH3/8to/vVn1G7WZ5kpu7U497wcfY\nbNqE/fic146nA/ntO6C6mcU45+KBy09gHeOAlUCyc25jHsYmhZjP5+PRp57jqn498Pv9XHblYGrW\nrsuoJx+hQaM4LurSncsHDuG2G6+hbZO6RESU5qXX3wZgxedLGT3iUYoWLYoVKcITo14kolTpMPcI\nTvP56Hf7MF65fTDu8GFadOtHZPWazHr9WarUPo/zWndk6stPkvLn77zxYOCn7aUqRPF/T73GWSUi\n6Dz4JkYODVwS7zLkZs4qERHO7sgpyufzMfzp57iyT3cO+/1cPmAIterUZeQTj9CwUSyduvbgikFX\nc8sNV9Mqtg4RpUrzyv/eCXfYOTrN56PPv4fx3zsHc/jwYZp37UdktZp8/L9nqVLrPOq37sj0MYHc\nfPPhYG6Wj2LoiEBudhp8E6OvD+TmxQUnNwvl8dlC9UsRM4sBZjrn6gen7wTOJnAabKZzbpKZ9QBG\nAr8Dq4BznHMDzGwYcMA590xw2a+B7s65eDM74Jw7O937zAGmOuf+e6yY4uKauGUrVudhLyU/7Nyf\ncuxGhdD0TYnHblTI3Ny6+hrnXJOTWUdsXBP32fJVeRVSgXBaEQt3CHnulwMHwx1Cvpi4IeHYjQqZ\nf7fNOS8L4vE5L4TsDE6w6qufbvqZbJp94pyrbWYGvAysDrYdlmld6deTfuOdSeBGqczXB0VERCQb\nXj0+h/3v4GQy1MzWAd8AJQnctZ0rwT9GtAl40Tmnv1omIiKSdwrd8bkg3YODc+5Z4NkTXHYBUDVv\nIxIREZHCeHwuaGdwRERERE6aChwRERHxHBU4IiIi4jkqcERERMRzVOCIiIiI56jAEREREc9RgSMi\nIiKeowJHREREPEcFjoiIiHiOChwRERHxHBU4IiIi4jkqcERERMRzVOCIiIiI56jAEREREc9RgSMi\nIiKeowJHREREPEcFjoiIiHiOChwRERHxHBU4IiIi4jkqcERERMRzzDkX7hjCxsx2AVtD8FZlgd0h\neJ9Q82K/1KeTU9U5V+5kVhDCvASNd2HhxT5B6Pp10nlZGJ3SBU6omNlq51yTcMeR17zYL/Xp1OLF\nbaM+FR5e7VdBoUtUIiIi4jkqcERERMRzVOCExthwB5BPvNgv9enU4sVtoz4VHl7tV4Gge3BERETE\nc3QGR0RERDxHBU6QmX2eB+tYbGYhuSPezNqb2fmheK8cYogxsytPcNkDeR3PMd6vUI9NcFt/nc3r\nj5pZx2MsO8zM7jzKvJCOw4lQbp5QDIUiN70wLqdybhZ0KnCCnHNZPvhm5ss0bWZWULZZeyCsO1Eg\nBsh2J5p524VCARqf9oRobJxzDznnFoTivcJFuXlCYigguVmAxqY9IRyXUyE3C7qC8KErEFKr5WCV\n/5mZTQc2Bqvz78zsbeBroLKZdTKz5Wa21swmmtnZ2awvSxsz62xmE9O1aW9mM4PPx5jZajP7xswe\nSdcm3sweCa5ng5nVNrMY4AbgNjNbZ2ZtjrOvMWa2ycxeC77fPDM7w8xqmNkcM1sT3Aa1g+3HmVnf\nzNsKGAG0CcZwm5kNMbPpZrYIWBjs88J0sfc8njiPoy/px2dQYR6bYzgtmzFLGxsz62pm3wbH74XU\n+IPqWuDb8mYzuyWbbfK2mfVKN/1efozXiVBuFr7cPMXyEk7R3CzwnHN6BG60PhD8tz3wO1AtOB0D\nHAZaBKfLAkuAs4LTdwMPBZ8vBpocrQ3gA7ale30MMDD4vHTw39OC62kQnI4Hbg4+/yfwevD5MODO\nE+xrDHAIaBScngAMBBYC5wZfaw4sCj4fB/Q9yraame71IcCOdH3xASXSbbcfOXJj+4E8Gre08fHC\n2JzAmI0D+gKnA9s58rn9IHVsgvF8DhQP9n8PUDTTWLYDpgaflwS2AL5w56Vys3DmJqdIXp7quVnQ\nHyG/jFBIrHTObUk3vdU590XweQugLrDMzACKAcszLZ9tG+fcITObA/Qws0lAN+A/wWUuM7PrCSR0\nZHD5r4LzpgT/XQP0zpsussU5ty7demMInL6dGIwZAkl3vOY7534JPjfgCTNrS2BnFw1UAJJPNOij\n2Oqc+8LMuuONsTma7MYsVW1gc7rP7QfA9enmz3LOpQApZraTwDjsSJ3pnPvUzF4xs3JAH2Cyc+5Q\nPvXjZCg3AwpDbp4qeQnKzQJJBU72fs9h2gjsKPrnsHxObcYDNwG/AKudc7+ZWTXgTqCpc+5XMxtH\noOpPlRL810/ejVlKuud+Akm11znXKJu2hwhezrTAtfRiOaw3/bYaAJQD4pxzf5tZPBn7lVdS39Mr\nY3M0mcfsjJNYNrtY3ybwzfMK4Orjji40lJsZFeTcPFXyMv37pb7nqZibBY7uwTl+XwCtzOwfAGZ2\nlpnVPI42nwKxwFACiQtQgsDOYJ+ZVQC65CKO34BzTqonGe0HtphZv2DMZmYNg/Pigbjg80uAormM\noSSwM7gD7QBUzcN4s+PVscmN74DqwXsNAC4/gXWMA/4N4JzbmCdRhZZXx7+w56ZXxyW3lJthogLn\nODnndhG4nv2BmX1F4FRr7dy2cc75gZkEEnJm8LX1wJfAt8D7wLJchDIDuDSPb5gbAFxrZuuBb4DU\nG9leA9oFX2/JkW9mXwF+M1tvZrdls773gCZmtgG4ikD/8o3HxyZHzrk/CdxvMMfM1hDYme87znX8\nDGwC3sz7CPOfx8e/0Oamx8flmJSb4aO/ZCziEWZ2tnPugAVuYngZ+ME59+xxLH8msAGIdc4d1w5Y\nRI5OuRkeOoMj4h1DzWwdgW/4JYFXc7ugBf4g2SbgRe1ARfKccjMMdAZHREREPEdncERERMRzVOCI\niIiI56jAEREREc9RgSMiIiKeowJHREREPEcFjoiIiHjO/wMtSMDcoDrYNgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1cae36d940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm as colm\n",
    "plt.figure(figsize=(8,5))\n",
    "sub = plt.subplot(121)\n",
    "normed = conf_matrix_full/conf_matrix_full.sum()\n",
    "plt.imshow(normed, cmap=colm.Blues, vmax=0.5)\n",
    "plt.title('conf matrix FULL')\n",
    "sub.set_yticks([0,1,2,3])\n",
    "sub.set_yticklabels(['irrelevant', 'neutral','relevant', 'highly'])\n",
    "sub.set_xticks([0,1,2,3])\n",
    "sub.set_xticklabels(['irrelevant', 'neutral','relevant', 'highly'])\n",
    "\n",
    "for i in range(normed.shape[0]):\n",
    "    for j in range(normed.shape[1]):\n",
    "        v = normed.T[i][j]\n",
    "        c='%.2f'%v if v>0.005 else ''\n",
    "        sub.text(i, j, c, va='center', ha='center')\n",
    "\n",
    "\n",
    "\n",
    "sub = plt.subplot(122)\n",
    "normed = conf_matrix_role/conf_matrix_role.sum()\n",
    "plt.imshow(normed, cmap=colm.Blues, vmax=0.5)\n",
    "sub.yaxis.tick_right()\n",
    "sub.set_yticks([0,1,2,3])\n",
    "sub.set_yticklabels(['irrelevant', 'neutral','relevant', 'highly'])\n",
    "sub.set_xticks([0,1,2,3])\n",
    "sub.set_xticklabels(['irrelevant', 'neutral','relevant', 'highly'])\n",
    "\n",
    "for i in range(normed.shape[0]):\n",
    "    for j in range(normed.shape[1]):\n",
    "        v = normed.T[i][j]\n",
    "        c='%.2f'%v if v>0.01 else ''\n",
    "        sub.text(i, j, c, va='center', ha='center')\n",
    "        \n",
    "plt.title('conf matrix ROLE')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affiliate 1145\n",
      "agent 508\n",
      "counterpart 1119\n",
      "guarantor 335\n",
      "insurer 473\n",
      "issuer 1106\n",
      "seller 520\n",
      "servicer 614\n",
      "trustee 2399\n",
      "underwriter 559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x7f1c90d7fac8>], dtype=object)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA68AAAF1CAYAAAAHotyJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1c1581cc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "def score_func(x, norm=True):\n",
    "    kind = 'mulmax'\n",
    "    if kind=='last':\n",
    "        xx = x[-1]\n",
    "    elif kind=='mulmax':\n",
    "        xx = x.argmax(axis=1)*x.max(axis=1)\n",
    "    elif kind=='maxadjust':\n",
    "        xx = np.array([ai-xi[ai:ai+1].sum()+xi[ai+1:].sum() for xi,ai in zip(x,x.argmax(axis=1))])\n",
    "    elif kind == '1234':\n",
    "        xx = np.sum(x * np.array([1, 2, 3, 4]), axis=1)\n",
    "    elif kind == '1246':\n",
    "        xx = np.sum(x * np.array([1, 2, 4, 6]), axis=1)\n",
    "    elif kind=='1245':\n",
    "        xx = np.sum(x * np.array([1, 2, 4, 5]), axis=1)\n",
    "    else:\n",
    "        raise AttributeError('meh.')\n",
    "        \n",
    "    return (xx-xx.min())/(xx-xx.min()).max()\n",
    "\n",
    "distribution_full = []\n",
    "distribution_role = []\n",
    "scores_full = []\n",
    "scores_role = []\n",
    "\n",
    "scored_full = []\n",
    "scored_role = []\n",
    "\n",
    "for role, frm in data.test.groupby('grp'):\n",
    "    print(role, len(frm))\n",
    "    fmpred, fmppred = fm.predict(frm)\n",
    "    mpred, mppred = models[role].predict(frm)\n",
    "    \n",
    "    tmp = frm.copy()\n",
    "    tmp['SCORE'] = score_func(fmppred)\n",
    "    scored_full.append(tmp)\n",
    "    \n",
    "    tmp = frm.copy()\n",
    "    tmp['SCORE'] = score_func(mppred)\n",
    "    scored_role.append(tmp)\n",
    "    \n",
    "    distribution_full += list(fmpred)\n",
    "    distribution_role += list(mpred)\n",
    "    scores_full += list(score_func(fmppred))\n",
    "    scores_role += list(score_func(mppred))\n",
    "    \n",
    "classifier = 'bow'\n",
    "cols = ['SCORE','UNIQUE_ID', 'DOCUMENT_TYPE', 'FILER_NAME', 'FILER_CIK', 'FILING_INTERVAL', 'FILING_DATE', \n",
    "        'MENTIONED_FINANCIAL_ENTITY', 'ROLE', 'THREE_SENTENCES']\n",
    "\n",
    "\n",
    "#pd.concat(scored_full).sort_values(by=['grp', 'SCORE'], \n",
    "#                                   ascending=[True, False])[cols].to_csv('scored_full_'+classifier+'.csv', index=False)\n",
    "#pd.concat(scored_role).sort_values(by=['grp', 'SCORE'], \n",
    "#                                   ascending=[True, False])[cols].to_csv('scored_role_'+classifier+'.csv', index=False)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(16,6), nrows=2, ncols=2)\n",
    "pd.DataFrame({'full_score':scores_full}).hist(ax=ax[0][0])\n",
    "pd.DataFrame({'full_argmax':distribution_full}).hist(ax=ax[0][1])\n",
    "pd.DataFrame({'role_score':scores_role}).hist(ax=ax[1][0])\n",
    "pd.DataFrame({'role_argmax':distribution_role}).hist(ax=ax[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "working_set = data.working.copy()[['INIQUE_ID', 'DOCUMENT_TYPE', 'FILER_NAME', 'FILER_CIK',\n",
    "       'FILING_INTERVAL', 'FILING_DATE', 'MENTIONED_FINANCIAL_ENTITY', 'ROLE',\n",
    "       'THREE_SENTENCES']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affiliate 1145\n",
      "agent 508\n",
      "counterpart 1119\n",
      "guarantor 335\n",
      "insurer 473\n",
      "issuer 1106\n",
      "seller 520\n",
      "servicer 614\n",
      "trustee 2399\n",
      "underwriter 559\n"
     ]
    }
   ],
   "source": [
    "pred_full = []\n",
    "pred_role = []\n",
    "\n",
    "classi = \"emb\"\n",
    "\n",
    "for role, frm in data.test.groupby('grp'):\n",
    "    print(role, len(frm))\n",
    "    fmpred, fmppred = fm.predict(frm)\n",
    "    mpred, mppred = models[role].predict(frm)\n",
    "    \n",
    "    pred_full += [{'index': i,\n",
    "                   'score_full_'+classi: s, \n",
    "                   'pred_full_'+classi: \"{:.5f}|{:.5f}|{:.5f}|{:.5f}\".format(*p)} \n",
    "             for p, s, i in zip(fmppred, score_func(fmppred), frm.index)]\n",
    "    pred_role += [{'index': i,\n",
    "                   'score_role_'+classi: s, \n",
    "                   'pred_role_'+classi: \"{:.5f}|{:.5f}|{:.5f}|{:.5f}\".format(*p)} \n",
    "             for p, s, i in zip(mppred, score_func(mppred), frm.index)]\n",
    "\n",
    "df = pd.DataFrame(pred_full, index =list(pd.DataFrame(pred_full)['index']))\n",
    "working_set['score_full_'+classi] = df['score_full_'+classi]\n",
    "working_set['pred_full_'+classi]  = df['pred_full_'+classi]\n",
    "\n",
    "df = pd.DataFrame(pred_role, index =list(pd.DataFrame(pred_role)['index']))\n",
    "working_set['score_role_'+classi] = df['score_role_'+classi]\n",
    "working_set['pred_role_'+classi]  = df['pred_role_'+classi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "working_set.to_csv('scored_workingset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INIQUE_ID</th>\n",
       "      <th>DOCUMENT_TYPE</th>\n",
       "      <th>FILER_NAME</th>\n",
       "      <th>FILER_CIK</th>\n",
       "      <th>FILING_INTERVAL</th>\n",
       "      <th>FILING_DATE</th>\n",
       "      <th>MENTIONED_FINANCIAL_ENTITY</th>\n",
       "      <th>ROLE</th>\n",
       "      <th>THREE_SENTENCES</th>\n",
       "      <th>score_full_syn</th>\n",
       "      <th>...</th>\n",
       "      <th>score_role_vote</th>\n",
       "      <th>pred_role_vote</th>\n",
       "      <th>score_full_bow</th>\n",
       "      <th>pred_full_bow</th>\n",
       "      <th>score_role_bow</th>\n",
       "      <th>pred_role_bow</th>\n",
       "      <th>score_full_emb</th>\n",
       "      <th>pred_full_emb</th>\n",
       "      <th>score_role_emb</th>\n",
       "      <th>pred_role_emb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1831</td>\n",
       "      <td>10-K</td>\n",
       "      <td>Ally Financial Inc</td>\n",
       "      <td>40729</td>\n",
       "      <td>2015-FY</td>\n",
       "      <td>2/24/2016</td>\n",
       "      <td>Ally Bank</td>\n",
       "      <td>affiliate</td>\n",
       "      <td>Furthermore, there is an “attribution rule” th...</td>\n",
       "      <td>0.369995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354949</td>\n",
       "      <td>0.13684|0.30976|0.37962|0.17378</td>\n",
       "      <td>0.038140</td>\n",
       "      <td>0.09593|0.17808|0.37014|0.35585</td>\n",
       "      <td>0.087432</td>\n",
       "      <td>0.27812|0.26936|0.35539|0.09713</td>\n",
       "      <td>0.056855</td>\n",
       "      <td>0.07855|0.27178|0.36381|0.28585</td>\n",
       "      <td>0.698725</td>\n",
       "      <td>0.13268|0.14744|0.29711|0.42276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1832</td>\n",
       "      <td>10-K</td>\n",
       "      <td>Ally Financial Inc</td>\n",
       "      <td>40729</td>\n",
       "      <td>2015-FY</td>\n",
       "      <td>2/24/2016</td>\n",
       "      <td>Ally</td>\n",
       "      <td>Affiliate</td>\n",
       "      <td>For example, because Ally controls Ally Bank, ...</td>\n",
       "      <td>0.355088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365277</td>\n",
       "      <td>0.13742|0.30340|0.38787|0.17131</td>\n",
       "      <td>0.038140</td>\n",
       "      <td>0.09593|0.17808|0.37014|0.35585</td>\n",
       "      <td>0.087432</td>\n",
       "      <td>0.27812|0.26936|0.35539|0.09713</td>\n",
       "      <td>0.574718</td>\n",
       "      <td>0.08307|0.19761|0.35154|0.36777</td>\n",
       "      <td>0.291061</td>\n",
       "      <td>0.13572|0.14976|0.30190|0.41262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1833</td>\n",
       "      <td>10-K</td>\n",
       "      <td>Ally Financial Inc</td>\n",
       "      <td>40729</td>\n",
       "      <td>2015-FY</td>\n",
       "      <td>2/24/2016</td>\n",
       "      <td>Ally</td>\n",
       "      <td>underwriters</td>\n",
       "      <td>Proceeds from the offering amounted to $2.4 bi...</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.630390</td>\n",
       "      <td>0.01442|0.03046|0.65094|0.30417</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.12561|0.21594|0.36511|0.29334</td>\n",
       "      <td>0.604703</td>\n",
       "      <td>0.01850|0.04103|0.52575|0.41472</td>\n",
       "      <td>0.023048</td>\n",
       "      <td>0.08134|0.21448|0.35735|0.34683</td>\n",
       "      <td>0.570189</td>\n",
       "      <td>0.02373|0.04997|0.52544|0.40086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1834</td>\n",
       "      <td>10-K</td>\n",
       "      <td>Ally Financial Inc</td>\n",
       "      <td>40729</td>\n",
       "      <td>2015-FY</td>\n",
       "      <td>2/24/2016</td>\n",
       "      <td>Ally</td>\n",
       "      <td>Guarantors</td>\n",
       "      <td>The elimination entries set forth in the follo...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.970528</td>\n",
       "      <td>0.00000|0.05355|0.84677|0.09969</td>\n",
       "      <td>0.039948</td>\n",
       "      <td>0.09593|0.17808|0.37014|0.35585</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000|0.04396|0.80675|0.14929</td>\n",
       "      <td>0.032523</td>\n",
       "      <td>0.07766|0.22671|0.35894|0.33669</td>\n",
       "      <td>0.955305</td>\n",
       "      <td>0.00000|0.07988|0.77360|0.14653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1835</td>\n",
       "      <td>10-K</td>\n",
       "      <td>Ally Financial Inc</td>\n",
       "      <td>40729</td>\n",
       "      <td>2015-FY</td>\n",
       "      <td>2/24/2016</td>\n",
       "      <td>Ally</td>\n",
       "      <td>Guarantors</td>\n",
       "      <td>The elimination entries set forth in the follo...</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.998939</td>\n",
       "      <td>0.00000|0.03777|0.86372|0.09851</td>\n",
       "      <td>0.039948</td>\n",
       "      <td>0.09593|0.17808|0.37014|0.35585</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000|0.04396|0.80675|0.14929</td>\n",
       "      <td>0.043275</td>\n",
       "      <td>0.07416|0.26273|0.36122|0.30189</td>\n",
       "      <td>0.867443</td>\n",
       "      <td>0.00000|0.12378|0.72757|0.14865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   INIQUE_ID DOCUMENT_TYPE          FILER_NAME  FILER_CIK FILING_INTERVAL  \\\n",
       "0       1831          10-K  Ally Financial Inc      40729         2015-FY   \n",
       "1       1832          10-K  Ally Financial Inc      40729         2015-FY   \n",
       "2       1833          10-K  Ally Financial Inc      40729         2015-FY   \n",
       "3       1834          10-K  Ally Financial Inc      40729         2015-FY   \n",
       "4       1835          10-K  Ally Financial Inc      40729         2015-FY   \n",
       "\n",
       "  FILING_DATE MENTIONED_FINANCIAL_ENTITY          ROLE  \\\n",
       "0   2/24/2016                  Ally Bank     affiliate   \n",
       "1   2/24/2016                       Ally     Affiliate   \n",
       "2   2/24/2016                       Ally  underwriters   \n",
       "3   2/24/2016                       Ally    Guarantors   \n",
       "4   2/24/2016                       Ally    Guarantors   \n",
       "\n",
       "                                     THREE_SENTENCES  score_full_syn  \\\n",
       "0  Furthermore, there is an “attribution rule” th...        0.369995   \n",
       "1  For example, because Ally controls Ally Bank, ...        0.355088   \n",
       "2  Proceeds from the offering amounted to $2.4 bi...        0.533333   \n",
       "3  The elimination entries set forth in the follo...        0.666667   \n",
       "4  The elimination entries set forth in the follo...        0.666667   \n",
       "\n",
       "                ...                score_role_vote  \\\n",
       "0               ...                       0.354949   \n",
       "1               ...                       0.365277   \n",
       "2               ...                       0.630390   \n",
       "3               ...                       0.970528   \n",
       "4               ...                       0.998939   \n",
       "\n",
       "                    pred_role_vote score_full_bow  \\\n",
       "0  0.13684|0.30976|0.37962|0.17378       0.038140   \n",
       "1  0.13742|0.30340|0.38787|0.17131       0.038140   \n",
       "2  0.01442|0.03046|0.65094|0.30417       0.000000   \n",
       "3  0.00000|0.05355|0.84677|0.09969       0.039948   \n",
       "4  0.00000|0.03777|0.86372|0.09851       0.039948   \n",
       "\n",
       "                     pred_full_bow score_role_bow  \\\n",
       "0  0.09593|0.17808|0.37014|0.35585       0.087432   \n",
       "1  0.09593|0.17808|0.37014|0.35585       0.087432   \n",
       "2  0.12561|0.21594|0.36511|0.29334       0.604703   \n",
       "3  0.09593|0.17808|0.37014|0.35585       1.000000   \n",
       "4  0.09593|0.17808|0.37014|0.35585       1.000000   \n",
       "\n",
       "                     pred_role_bow score_full_emb  \\\n",
       "0  0.27812|0.26936|0.35539|0.09713       0.056855   \n",
       "1  0.27812|0.26936|0.35539|0.09713       0.574718   \n",
       "2  0.01850|0.04103|0.52575|0.41472       0.023048   \n",
       "3  0.00000|0.04396|0.80675|0.14929       0.032523   \n",
       "4  0.00000|0.04396|0.80675|0.14929       0.043275   \n",
       "\n",
       "                     pred_full_emb score_role_emb  \\\n",
       "0  0.07855|0.27178|0.36381|0.28585       0.698725   \n",
       "1  0.08307|0.19761|0.35154|0.36777       0.291061   \n",
       "2  0.08134|0.21448|0.35735|0.34683       0.570189   \n",
       "3  0.07766|0.22671|0.35894|0.33669       0.955305   \n",
       "4  0.07416|0.26273|0.36122|0.30189       0.867443   \n",
       "\n",
       "                     pred_role_emb  \n",
       "0  0.13268|0.14744|0.29711|0.42276  \n",
       "1  0.13572|0.14976|0.30190|0.41262  \n",
       "2  0.02373|0.04997|0.52544|0.40086  \n",
       "3  0.00000|0.07988|0.77360|0.14653  \n",
       "4  0.00000|0.12378|0.72757|0.14865  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>pred_full_syn</th>\n",
       "      <th>score_full_syn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.00000|0.44501|0.55499|0.00000</td>\n",
       "      <td>0.369995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.00000|0.46737|0.53263|0.00000</td>\n",
       "      <td>0.355088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.05000|0.40000|0.05000|0.50000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.00000|0.15000|0.55000|0.30000</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.09358|0.48014|0.27628|0.15000</td>\n",
       "      <td>0.160047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.00000|0.07950|0.50307|0.41743</td>\n",
       "      <td>0.335380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.08745|0.01255|0.10000|0.80000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.08745|0.01255|0.10000|0.80000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.09096|0.15000|0.65904|0.10000</td>\n",
       "      <td>0.439363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.00000|0.10000|0.10000|0.80000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.00000|0.10000|0.10000|0.80000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.03468|0.25529|0.43436|0.27567</td>\n",
       "      <td>0.289575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.20000|0.40000|0.25000|0.15000</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>53</td>\n",
       "      <td>0.00000|0.10000|0.70000|0.20000</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>64</td>\n",
       "      <td>0.00000|0.00000|0.71936|0.28064</td>\n",
       "      <td>0.479575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>66</td>\n",
       "      <td>0.00000|0.55070|0.39930|0.05000</td>\n",
       "      <td>0.183568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>0.10000|0.15000|0.75000|0.00000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>0.05000|0.48391|0.46609|0.00000</td>\n",
       "      <td>0.161303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86</td>\n",
       "      <td>0.00000|0.00000|1.00000|0.00000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>0.00000|0.00000|1.00000|0.00000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>0.00000|0.00000|1.00000|0.00000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>89</td>\n",
       "      <td>0.00000|0.00000|1.00000|0.00000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>105</td>\n",
       "      <td>0.00000|0.00000|0.75000|0.25000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>106</td>\n",
       "      <td>0.10000|0.05000|0.85000|0.00000</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>112</td>\n",
       "      <td>0.00000|0.00000|0.15000|0.85000</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>146</td>\n",
       "      <td>0.30000|0.07448|0.50000|0.12552</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>147</td>\n",
       "      <td>0.30000|0.07448|0.50000|0.12552</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>148</td>\n",
       "      <td>0.00000|0.18761|0.46239|0.35000</td>\n",
       "      <td>0.308260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>149</td>\n",
       "      <td>0.00000|0.26011|0.70379|0.03610</td>\n",
       "      <td>0.469192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>150</td>\n",
       "      <td>0.00000|0.09100|0.66607|0.24293</td>\n",
       "      <td>0.444047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8343</th>\n",
       "      <td>8343</td>\n",
       "      <td>0.00000|0.10000|0.10000|0.80000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8344</th>\n",
       "      <td>8344</td>\n",
       "      <td>0.00000|0.10000|0.10000|0.80000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8351</th>\n",
       "      <td>8351</td>\n",
       "      <td>0.00000|0.75000|0.00000|0.25000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8352</th>\n",
       "      <td>8352</td>\n",
       "      <td>0.00000|0.15000|0.20000|0.65000</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8367</th>\n",
       "      <td>8367</td>\n",
       "      <td>0.00000|0.75000|0.00000|0.25000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8368</th>\n",
       "      <td>8368</td>\n",
       "      <td>0.00000|0.15000|0.20000|0.65000</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8395</th>\n",
       "      <td>8395</td>\n",
       "      <td>0.00000|0.38014|0.21986|0.40000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8418</th>\n",
       "      <td>8418</td>\n",
       "      <td>0.00000|0.50000|0.25000|0.25000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8443</th>\n",
       "      <td>8443</td>\n",
       "      <td>0.00000|0.50000|0.25000|0.25000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8469</th>\n",
       "      <td>8469</td>\n",
       "      <td>0.05000|0.17157|0.32843|0.45000</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8470</th>\n",
       "      <td>8470</td>\n",
       "      <td>0.00000|0.00000|0.00000|1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8471</th>\n",
       "      <td>8471</td>\n",
       "      <td>0.00000|0.02950|0.26608|0.70442</td>\n",
       "      <td>0.704425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8496</th>\n",
       "      <td>8496</td>\n",
       "      <td>0.00000|0.19314|0.50686|0.30000</td>\n",
       "      <td>0.337903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8497</th>\n",
       "      <td>8497</td>\n",
       "      <td>0.05000|0.17157|0.32843|0.45000</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8498</th>\n",
       "      <td>8498</td>\n",
       "      <td>0.00000|0.40000|0.20000|0.40000</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8499</th>\n",
       "      <td>8499</td>\n",
       "      <td>0.00000|0.02950|0.26608|0.70442</td>\n",
       "      <td>0.704425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8519</th>\n",
       "      <td>8519</td>\n",
       "      <td>0.00000|0.15171|0.37262|0.47567</td>\n",
       "      <td>0.475668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8520</th>\n",
       "      <td>8520</td>\n",
       "      <td>0.00000|0.00000|0.05000|0.95000</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8539</th>\n",
       "      <td>8539</td>\n",
       "      <td>0.00000|0.19314|0.50686|0.30000</td>\n",
       "      <td>0.337903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8540</th>\n",
       "      <td>8540</td>\n",
       "      <td>0.00000|0.00000|0.05000|0.95000</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8559</th>\n",
       "      <td>8559</td>\n",
       "      <td>0.00000|0.19314|0.50686|0.30000</td>\n",
       "      <td>0.337903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8560</th>\n",
       "      <td>8560</td>\n",
       "      <td>0.00000|0.05000|0.30000|0.65000</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8609</th>\n",
       "      <td>8609</td>\n",
       "      <td>0.35000|0.00000|0.30000|0.35000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8610</th>\n",
       "      <td>8610</td>\n",
       "      <td>0.35000|0.00000|0.30000|0.35000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8626</th>\n",
       "      <td>8626</td>\n",
       "      <td>0.05000|0.00000|0.30000|0.65000</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8627</th>\n",
       "      <td>8627</td>\n",
       "      <td>0.05000|0.00000|0.30000|0.65000</td>\n",
       "      <td>0.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8672</th>\n",
       "      <td>8672</td>\n",
       "      <td>0.00000|0.13014|0.51986|0.35000</td>\n",
       "      <td>0.346573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8773</th>\n",
       "      <td>8773</td>\n",
       "      <td>0.05000|0.17157|0.32843|0.45000</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8774</th>\n",
       "      <td>8774</td>\n",
       "      <td>0.00000|0.30000|0.35000|0.35000</td>\n",
       "      <td>0.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8775</th>\n",
       "      <td>8775</td>\n",
       "      <td>0.00000|0.02950|0.26608|0.70442</td>\n",
       "      <td>0.704425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8778 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                    pred_full_syn  score_full_syn\n",
       "0         0  0.00000|0.44501|0.55499|0.00000        0.369995\n",
       "1         1  0.00000|0.46737|0.53263|0.00000        0.355088\n",
       "15       15  0.05000|0.40000|0.05000|0.50000        0.500000\n",
       "17       17  0.00000|0.15000|0.55000|0.30000        0.366667\n",
       "21       21  0.09358|0.48014|0.27628|0.15000        0.160047\n",
       "24       24  0.00000|0.07950|0.50307|0.41743        0.335380\n",
       "26       26  0.08745|0.01255|0.10000|0.80000        0.800000\n",
       "27       27  0.08745|0.01255|0.10000|0.80000        0.800000\n",
       "28       28  0.09096|0.15000|0.65904|0.10000        0.439363\n",
       "29       29  0.00000|0.10000|0.10000|0.80000        0.800000\n",
       "30       30  0.00000|0.10000|0.10000|0.80000        0.800000\n",
       "31       31  0.03468|0.25529|0.43436|0.27567        0.289575\n",
       "32       32  0.20000|0.40000|0.25000|0.15000        0.133333\n",
       "53       53  0.00000|0.10000|0.70000|0.20000        0.466667\n",
       "64       64  0.00000|0.00000|0.71936|0.28064        0.479575\n",
       "66       66  0.00000|0.55070|0.39930|0.05000        0.183568\n",
       "77       77  0.10000|0.15000|0.75000|0.00000        0.500000\n",
       "81       81  0.05000|0.48391|0.46609|0.00000        0.161303\n",
       "86       86  0.00000|0.00000|1.00000|0.00000        0.666667\n",
       "87       87  0.00000|0.00000|1.00000|0.00000        0.666667\n",
       "88       88  0.00000|0.00000|1.00000|0.00000        0.666667\n",
       "89       89  0.00000|0.00000|1.00000|0.00000        0.666667\n",
       "105     105  0.00000|0.00000|0.75000|0.25000        0.500000\n",
       "106     106  0.10000|0.05000|0.85000|0.00000        0.566667\n",
       "112     112  0.00000|0.00000|0.15000|0.85000        0.850000\n",
       "146     146  0.30000|0.07448|0.50000|0.12552        0.333333\n",
       "147     147  0.30000|0.07448|0.50000|0.12552        0.333333\n",
       "148     148  0.00000|0.18761|0.46239|0.35000        0.308260\n",
       "149     149  0.00000|0.26011|0.70379|0.03610        0.469192\n",
       "150     150  0.00000|0.09100|0.66607|0.24293        0.444047\n",
       "...     ...                              ...             ...\n",
       "8343   8343  0.00000|0.10000|0.10000|0.80000        0.800000\n",
       "8344   8344  0.00000|0.10000|0.10000|0.80000        0.800000\n",
       "8351   8351  0.00000|0.75000|0.00000|0.25000        0.250000\n",
       "8352   8352  0.00000|0.15000|0.20000|0.65000        0.650000\n",
       "8367   8367  0.00000|0.75000|0.00000|0.25000        0.250000\n",
       "8368   8368  0.00000|0.15000|0.20000|0.65000        0.650000\n",
       "8395   8395  0.00000|0.38014|0.21986|0.40000        0.400000\n",
       "8418   8418  0.00000|0.50000|0.25000|0.25000        0.166667\n",
       "8443   8443  0.00000|0.50000|0.25000|0.25000        0.166667\n",
       "8469   8469  0.05000|0.17157|0.32843|0.45000        0.450000\n",
       "8470   8470  0.00000|0.00000|0.00000|1.00000        1.000000\n",
       "8471   8471  0.00000|0.02950|0.26608|0.70442        0.704425\n",
       "8496   8496  0.00000|0.19314|0.50686|0.30000        0.337903\n",
       "8497   8497  0.05000|0.17157|0.32843|0.45000        0.450000\n",
       "8498   8498  0.00000|0.40000|0.20000|0.40000        0.133333\n",
       "8499   8499  0.00000|0.02950|0.26608|0.70442        0.704425\n",
       "8519   8519  0.00000|0.15171|0.37262|0.47567        0.475668\n",
       "8520   8520  0.00000|0.00000|0.05000|0.95000        0.950000\n",
       "8539   8539  0.00000|0.19314|0.50686|0.30000        0.337903\n",
       "8540   8540  0.00000|0.00000|0.05000|0.95000        0.950000\n",
       "8559   8559  0.00000|0.19314|0.50686|0.30000        0.337903\n",
       "8560   8560  0.00000|0.05000|0.30000|0.65000        0.650000\n",
       "8609   8609  0.35000|0.00000|0.30000|0.35000        0.000000\n",
       "8610   8610  0.35000|0.00000|0.30000|0.35000        0.000000\n",
       "8626   8626  0.05000|0.00000|0.30000|0.65000        0.650000\n",
       "8627   8627  0.05000|0.00000|0.30000|0.65000        0.650000\n",
       "8672   8672  0.00000|0.13014|0.51986|0.35000        0.346573\n",
       "8773   8773  0.05000|0.17157|0.32843|0.45000        0.450000\n",
       "8774   8774  0.00000|0.30000|0.35000|0.35000        0.233333\n",
       "8775   8775  0.00000|0.02950|0.26608|0.70442        0.704425\n",
       "\n",
       "[8778 rows x 3 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(pred_full, index =list(pd.DataFrame(pred_full)['index']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with scoring functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7AAAAF1CAYAAADRBwbsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwEqSJEmSWuH/AOQehma4/r1rAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fde6e94fac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>900.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.522213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.221250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.384295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.489785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.698920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             test\n",
       "count  900.000000\n",
       "mean     0.522213\n",
       "std      0.221250\n",
       "min      0.000000\n",
       "25%      0.384295\n",
       "50%      0.489785\n",
       "75%      0.698920\n",
       "max      1.000000"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "def score_func(x, norm=True):\n",
    "    #return x.argmax(axis=1)\n",
    "    #return np.sum(x * (np.ones_like(x)+ x.argmax(axis=1)), axis=1)\n",
    "    #xx = ((x.argmax(axis=1))*x.argmax(axis=1)*x.max(axis=1))\n",
    "    #xx= np.array([ai-xi[ai:ai+1].sum()+xi[ai+1:].sum() for xi,ai in zip(x,a)])\n",
    "    #xx = x.argmax(axis=1)+x.max(axis=1)\n",
    "    #xx = x.argmax(axis=1)*x.max(axis=1)*x.max(axis=1)\n",
    "    #return xx\n",
    "    kind = 'mulmax'\n",
    "    if kind=='last':\n",
    "        xx = x[-1]\n",
    "    elif kind=='mulmax':\n",
    "        xx = x.argmax(axis=1)*x.max(axis=1)\n",
    "    elif kind=='maxadjust':\n",
    "        xx = np.array([ai-xi[ai:ai+1].sum()+xi[ai+1:].sum() for xi,ai in zip(x,x.argmax(axis=1))])\n",
    "    elif kind == '1234':\n",
    "        xx = np.sum(x * np.array([1, 2, 3, 4]), axis=1)\n",
    "    elif kind == '1246':\n",
    "        xx = np.sum(x * np.array([1, 2, 4, 6]), axis=1)\n",
    "    elif kind=='1245':\n",
    "        xx = np.sum(x * np.array([1, 2, 4, 5]), axis=1)\n",
    "    else:\n",
    "        raise AttributeError('meh.')\n",
    "        \n",
    "    return (xx-xx.min())/(xx-xx.min()).max()\n",
    "\n",
    "#pred, ppred = fm.predict(data.test)\n",
    "#epred, eppred = fm.predict(data.eval)\n",
    "\n",
    "f, ax = plt.subplots(figsize=(16,6), nrows=2, ncols=3)\n",
    "pd.DataFrame({'test_score':score_func(ppred)}).hist(ax=ax[0][0])\n",
    "pd.DataFrame({'test_argmax':pred}).hist(ax=ax[0][1])\n",
    "pd.DataFrame({'eval_true':data.get_target(frm='eval')}).hist(ax=ax[1][2])\n",
    "pd.DataFrame({'eval_score':score_func(eppred)}).hist(ax=ax[1][0])\n",
    "pd.DataFrame({'eval_argmax':epred}).hist(ax=ax[1][1])\n",
    "plt.show()\n",
    "pd.DataFrame({'test':score_func(ppred, norm=False)}).describe()\n",
    "#pd.DataFrame({'eval':score_func(eppred)})\n",
    "#pd.DataFrame({'ppred':score_func(ppred), 'eppred':score_func(eppred)}).describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "218px",
    "left": "1647.38px",
    "right": "20px",
    "top": "114px",
    "width": "230px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
